<html lang="en_US"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.7.1 --> <title>分类算法 | Diana’s Blog</title> <meta name="generator" content="Jekyll v4.2.1" /> <meta property="og:title" content="分类算法" /> <meta name="author" content="Handan YU" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Table of Contents 逻辑回归 Logist 函数 目标函数 最大似然求解Loss Function 运用梯度下降得到参数更新递推公式 朴素贝叶斯 条件概率 极大似然估计 SVM 目标 支持向量 最优化问题 对偶问题 求支撑向量 求分界线 运用梯度下降方法求解SVM 决策树 决策树针对缺失数据的处理办法 信息熵 决策树类型 ID3 C4.5 CART 感知机算法 目标 运用梯度下降进行求解 具体做题步骤 LDA 二分类 多分类" /> <meta property="og:description" content="Table of Contents 逻辑回归 Logist 函数 目标函数 最大似然求解Loss Function 运用梯度下降得到参数更新递推公式 朴素贝叶斯 条件概率 极大似然估计 SVM 目标 支持向量 最优化问题 对偶问题 求支撑向量 求分界线 运用梯度下降方法求解SVM 决策树 决策树针对缺失数据的处理办法 信息熵 决策树类型 ID3 C4.5 CART 感知机算法 目标 运用梯度下降进行求解 具体做题步骤 LDA 二分类 多分类" /> <link rel="canonical" href="http://localhost:4000/2020/04/03/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95.html" /> <meta property="og:url" content="http://localhost:4000/2020/04/03/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95.html" /> <meta property="og:site_name" content="Diana’s Blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2020-04-03T00:00:00+08:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="分类算法" /> <script type="application/ld+json"> {"description":"Table of Contents 逻辑回归 Logist 函数 目标函数 最大似然求解Loss Function 运用梯度下降得到参数更新递推公式 朴素贝叶斯 条件概率 极大似然估计 SVM 目标 支持向量 最优化问题 对偶问题 求支撑向量 求分界线 运用梯度下降方法求解SVM 决策树 决策树针对缺失数据的处理办法 信息熵 决策树类型 ID3 C4.5 CART 感知机算法 目标 运用梯度下降进行求解 具体做题步骤 LDA 二分类 多分类","author":{"@type":"Person","name":"Handan YU"},"@type":"BlogPosting","url":"http://localhost:4000/2020/04/03/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95.html","headline":"分类算法","dateModified":"2020-04-03T00:00:00+08:00","datePublished":"2020-04-03T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/04/03/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95.html"},"@context":"https://schema.org"}</script> <!-- End Jekyll SEO tag --> <link rel="apple-touch-icon" sizes="180x180" href="/assets/img/icons/apple-touch-icon.png?v=qA3OXqyw77"> <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png?v=qA3OXqyw77"> <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png?v=qA3OXqyw77"> <link rel="manifest" href="/assets/img/icons/manifest.json?v=qA3OXqyw77"> <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5"> <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]--> <link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"> <meta name="apple-mobile-web-app-title" content="Diana Blog"> <meta name="application-name" content="Diana Blog"> <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> <meta name="theme-color" content="#ffffff"> <style class="inlineCSS"> h1{color:#313237;margin-top:0;margin-bottom:.5rem}.dark-bg{background-color:#313237}@media (min-width:48em){.post-card{width:48.4375%;margin-right:3.125%}.post-card:last-of-type,.post-card:nth-child(2n+2){margin-right:0}}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figure,main{display:block}figure{margin:1em 40px}a{background-color:transparent;-webkit-text-decoration-skip:objects}img{border-style:none}svg:not(:root){overflow:hidden}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}body{-webkit-overflow-scrolling:touch}*,::after,::before{-webkit-box-sizing:inherit;box-sizing:inherit}.site{display:-webkit-box;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.site__content{-webkit-box-flex:1;-ms-flex:1;flex:1}img{max-width:100%;height:auto;width:auto;vertical-align:middle}figure{margin:0}body{background-color:#fff;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:1rem;line-height:1.5;color:#343851;-webkit-font-smoothing:antialiased;-webkit-text-size-adjust:100%}p{margin-top:0;margin-bottom:1.25rem}h1,h2{color:#313237;margin-top:0;margin-bottom:.5rem}a{color:#277cea;text-decoration:none}.blur{background:#fff;filter:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg"><filter id="filter"><feGaussianBlur stdDeviation="16" /></filter></svg>#filter');-webkit-filter:blur(1rem);filter:blur(1rem)}.container{padding:0 20px}@media (min-width:0){.container{max-width:auto;margin:0 auto}}@media (min-width:36em){.container{max-width:540px;margin:0 auto}}@media (min-width:48em){.container{max-width:720px;margin:0 auto}}@media (min-width:62em){.container{max-width:960px;margin:0 auto}}@media (min-width:75em){.container{max-width:1170px;margin:0 auto}}.header{background-color:#fff;color:#343851;position:absolute;z-index:4;width:100%;top:0;left:0;will-change:transform;-webkit-transform:translateY(0);transform:translateY(0)}.header a{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.header__logo{display:-webkit-box;display:-ms-flexbox;display:flex;height:100%;overflow:hidden;padding:19px 0;margin-right:1.25rem;outline:0;color:#313237}.header__logo .header__logo--container{width:58px}.header__logo .header__logo--container .logo{fill:currentColor}.header__inner{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:3.75em;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.header__links{padding-bottom:.5rem;display:block;position:absolute;top:3.75em;left:0;width:100%;height:auto;visibility:hidden;background:#fff}.header__link{color:#343851;padding:1em 0;border-top:1px solid #ededed}.header__toggle{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:44px;height:100%;background-color:transparent;padding-left:1.25rem}.header__toggle span{display:block;position:relative;margin-top:4px;background-color:#343851;width:100%;height:2px;border-radius:1px}.header__toggle span:first-child{margin-top:0}@media (min-width:62em){.header__toggle{display:none;visibility:hidden}.header__links{position:static;display:-webkit-box;display:-ms-flexbox;display:flex;visibility:visible;width:auto;height:100%}.header__link{position:relative;padding:.938em 0;border:0}.header__link::after{content:"";display:block;position:absolute;left:0;bottom:0;height:3px;width:100%;-webkit-transform:scaleX(0);transform:scaleX(0);background:#277cea}}.post-card{display:block;width:100%;min-height:250px;border-radius:4px;overflow:hidden;background-color:#fff;-webkit-box-shadow:0 1px 3px rgba(0,0,0,.08);box-shadow:0 1px 3px rgba(0,0,0,.08);margin-bottom:5.26316%}@media (min-width:48em){.post-card{width:48.4375%;margin-right:3.125%}.post-card:nth-child(2n+2){margin-right:0}}@media (min-width:75em){.post-card{width:31.25%;margin-right:3.125%}.post-card:nth-child(2n+2){margin-right:3.125%}}.post-card__thumb{margin:0;background:#fff;position:relative;overflow:hidden}.post-card__thumb::after{content:"";display:block;height:0;width:100%;padding-bottom:56.25%}.post-card__thumb>*{position:absolute;top:0;left:0;width:100%;height:100%;display:block}.post-card__inner{padding:1.875rem 1.25rem .625rem;color:#838c8d}.post-card__header{margin-bottom:.75rem}.post-card__header .post-card__meta{font-size:.875rem}.hero{margin:3.75rem auto 0;min-height:16.25rem;width:100%;position:relative;background-color:#dde5ea;background-repeat:no-repeat;background-position:50%;background-size:cover}@media (min-width:62em){.hero{margin:0 auto;height:36em}}.hero::before{position:absolute;display:block;content:"";top:0;left:0;width:100%;height:100%;background:rgba(52,56,81,.8)}.hero__wrap{position:absolute;top:50%;left:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);text-align:center;color:rgba(255,255,255,.8);max-width:40em;z-index:1}.hero__wrap .hero__title{color:#fff}.blog{background-color:#f9f9f9}.post-list{padding-top:2.5em;display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-flex:1;-ms-flex:1 0 auto;flex:1 0 auto}@media (min-width:48em){.hero__wrap .hero__title{font-size:2.625em;line-height:3.125rem}.post-list{padding-top:5em}} </style> <link rel="preload" href="/assets/css/main.css" as="style" onload="this.rel='stylesheet'"> <noscript><link rel="stylesheet" href="/assets/css/main.css"></noscript> <script type="text/javascript"> /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */ (function(w){"use strict";if(!w.loadCSS){w.loadCSS=function(){}} var rp=loadCSS.relpreload={};rp.support=(function(){var ret;try{ret=w.document.createElement("link").relList.supports("preload")}catch(e){ret=!1} return function(){return ret}})();rp.bindMediaToggle=function(link){var finalMedia=link.media||"all";function enableStylesheet(){link.media=finalMedia} if(link.addEventListener){link.addEventListener("load",enableStylesheet)}else if(link.attachEvent){link.attachEvent("onload",enableStylesheet)} setTimeout(function(){link.rel="stylesheet";link.media="only x"});setTimeout(enableStylesheet,3000)};rp.poly=function(){if(rp.support()){return} var links=w.document.getElementsByTagName("link");for(var i=0;i<links.length;i++){var link=links[i];if(link.rel==="preload"&&link.getAttribute("as")==="style"&&!link.getAttribute("data-loadcss")){link.setAttribute("data-loadcss",!0);rp.bindMediaToggle(link)}}};if(!rp.support()){rp.poly();var run=w.setInterval(rp.poly,500);if(w.addEventListener){w.addEventListener("load",function(){rp.poly();w.clearInterval(run)})}else if(w.attachEvent){w.attachEvent("onload",function(){rp.poly();w.clearInterval(run)})}} if(typeof exports!=="undefined"){exports.loadCSS=loadCSS} else{w.loadCSS=loadCSS}}(typeof global!=="undefined"?global:this)) </script> </head> <head> <script> MathJax = { tex: { inlineMath: [['$', '$']], displayMath: [['$$', '$$']], processEnvironments: true, processRefs: true }, options: { skipHtmlTags: ['noscript', 'style', 'textarea', 'pre', 'code'], ignoreHtmlClass: 'tex2jax_ignore', renderActions: { find_script_mathtex: [10, function (doc) { for (const node of document.querySelectorAll('script[type^="math/tex"]')) { const display = !!node.type.match(/; *mode=display/); const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display); const text = document.createTextNode(''); node.parentNode.replaceChild(text, node); math.start = { node: text, delim: '', n: 0 }; math.end = { node: text, delim: '', n: 0 }; doc.math.push(math); } }, ''] } }, svg: { fontCache: 'global' } }; </script> <script id="MathJax-script" async src="https://cdn.staticfile.org/mathjax/3.0.1/es5/tex-svg.js"></script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> </head> <body class="site"> <header class="header" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation"> <div class="container"> <div class="header__inner"> <a class="header__logo" href="/"> <div class="header__logo--container"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="235" height="174" viewBox="0 0 235 174" fill="none"><g opacity="1" transform="translate(0 0) rotate(0 116 74.5)"><text><tspan x="0" y="28" font-size="24" line-height="0" fill="#000000" opacity="1" font-family="SourceHanSansCN-Regular" letter-spacing="0" ></tspan><tspan x="0" y="64" font-size="24" line-height="0" fill="#000000" opacity="1" font-family="SourceHanSansCN-Regular" letter-spacing="0" ></tspan><tspan x="0" y="100" font-size="24" line-height="0" fill="#000000" opacity="1" font-family="SourceHanSansCN-Regular" letter-spacing="0" >Diana</tspan></text></g></svg> </div> </a> <nav class="header__links"> <div class="container header__links-wrapper"> <a class="header__link" href="/" itemprop="url"><span itemprop="name">Home</span></a> <a class="header__link" href="/about" itemprop="url"><span itemprop="name">About</span></a> <a class="header__link" href="/contact" itemprop="url"> <span itemprop="name">Contact</span> </a> </div> </nav> <div class="header__toggle"> <span></span> <span></span> <span></span> </div> </div> </div> </header> <div class="hero lazyload" data-bg="http://localhost:4000/assets/img/posts/分类算法.jpg"> <div class="hero__wrap"> <h1 class="hero__title">分类算法</h1> <p class="hero__meta"> <span> <time>03 Apr 2020</time>&nbsp; </span> <span> 3 mins read &nbsp; </span> </p> </div> </div> </div> <main class="site__content"> <div class="container"> <article class="post-content" itemprop="articleBody"> <h5 id="table-of-contents">Table of Contents</h5> <ul> <li><a href="#逻辑回归">逻辑回归</a> <ul> <li><a href="#Logist 函数">Logist 函数</a></li> <li><a href="#目标函数">目标函数</a></li> <li><a href="#最大似然求解Loss Function">最大似然求解Loss Function</a></li> <li><a href="#运用梯度下降得到参数更新递推公式">运用梯度下降得到参数更新递推公式</a></li> </ul> </li> <li><a href="#朴素贝叶斯">朴素贝叶斯</a> <ul> <li><a href="#条件概率">条件概率</a></li> <li><a href="#极大似然估计">极大似然估计</a></li> </ul> </li> <li><a href="#SVM">SVM</a> <ul> <li><a href="#目标">目标</a></li> <li><a href="#支持向量">支持向量</a></li> <li><a href="#最优化问题">最优化问题</a></li> <li><a href="#对偶问题">对偶问题</a></li> <li><a href="#求支撑向量">求支撑向量</a></li> <li><a href="#求分界线">求分界线</a></li> <li><a href="#运用梯度下降方法求解SVM">运用梯度下降方法求解SVM</a></li> </ul> </li> <li><a href="#决策树">决策树</a> <ul> <li><a href="#决策树针对缺失数据的处理办法">决策树针对缺失数据的处理办法</a></li> <li><a href="#信息熵">信息熵</a></li> <li><a href="#决策树类型">决策树类型</a> <ul> <li><a href="#ID3">ID3</a></li> <li><a href="#C4.5">C4.5</a></li> <li><a href="#CART">CART</a></li> </ul> </li> </ul> </li> <li><a href="#感知机算法">感知机算法</a> <ul> <li><a href="#目标">目标</a></li> <li><a href="#运用梯度下降进行求解">运用梯度下降进行求解</a></li> <li><a href="#具体做题步骤">具体做题步骤</a></li> </ul> </li> <li><a href="#LDA">LDA</a> <ul> <li><a href="#二分类">二分类</a></li> <li><a href="#多分类">多分类</a></li> </ul> </li> </ul> <p><a name="逻辑回归"></a></p> <h2 id="逻辑回归lrp57">逻辑回归(LR)——P57</h2> <p>标准逻辑回归是结果为0，1的二分类算法。目标是求\(P(y_i=1\|x_i,w)\)，若其大于\(\frac{1}{2}\)，则预测分类结果为1，否则为0。</p> <p><a name="Logist 函数"></a></p> <h3 id="logist-函数">Logist 函数</h3> <p>令\(p(x_i) = P(y_i=1\|x_i,w)\)，构建\(p(x_i)\)与\(w^Tx_i\)之间的关系式。</p> <p>首先提出以下猜想</p> <p>\(~~~~~~~~~~~~~p(x_i)=w^Tx_i？~~~~~~~~~~~\log p(x_i) = w^Tx_i\)？</p> <p>由于\(0\leq p(x)\leq 1\), \(w^Tx_i\)是无界的。于是我们说以上等式都是不成立的，因此又有了以下猜想</p> \[\log \frac{p(x_i)}{1-p(x_i)} = w^Tx_i\] <p>可以证明\(\text{odd} = \frac{p(x_i)}{1-p(x_i)}\)的范围是[0,\(\inf\)]，则等式成立。我们称其为logist函数。</p> <p>则可以化简得到</p> \[p(x_i) = \frac{1}{1+e^{-w^Tx_i}}\] <p>可以类比Sigmoid函数发现，\(p(x_i)\)与Sigmoid函数一致。因此我们设\(p(x_i) = g(w^Tx_i)\).</p> <p><a name="目标函数"></a></p> <h3 id="目标函数">目标函数</h3> \[max_w ~~~~\prod_{i=1}^n P(y_i|x_i,w)\] <p>其中</p> \[P(y_i|x_i,w) = P(y_i=1|x_i,w)^{y_i}P(y_i=0|x_i,w)^{1-y_i}\] <p><a name="最大似然求解Loss Function"></a></p> <h3 id="最大似然求解loss-function">最大似然求解Loss Function</h3> <p>令</p> \[~~~~~~~~~ J = \prod_{i=1}^n P(y_i|x_i,w)\] \[~~~~~~~~~~~~~~~~~~~~~~~~= \prod_{i=1}^n g(w^Tx_i)^{y_i}(1-g(w^Tx_i))^{1-y_i}\] \[~~~~~~~~~~~~~~~\log J = \sum_{i=1}^my_i\ln (g(w^Tx_i))+(1-y_i)\ln (1-g(w^Tx_i))\] \[~~~~~~~~~~~~~~~~~~~~ \text{Loss} = - \log J\] <p>目标函数为</p> \[~~~~~~~~~~~~~~~~~~~~ \min_w - \log J(w)\] <p>其中\(g\) 为sigmoid函数</p> <p><a name="运用梯度下降得到参数更新递推公式"></a></p> <h3 id="运用梯度下降得到参数更新递推公式">运用梯度下降得到参数更新递推公式</h3> <p>对Loss函数求导</p> \[\frac{\partial{-\log J}}{\partial{w}}=-\sum_{i=1}^n[y_i\frac{\frac{\partial{g(w^Tx_i)}}{\partial{w}}}{g(w^Tx_i)}+(1-y_i)\frac{\frac{-\partial{ g(w^Tx_i)}}{\partial{w}}}{1-g(w^Tx_i)}]\] <p>由于令\(z_i = w^Tx_i\)，则有</p> \[\frac{\partial{g(w^Tx_i)}}{\partial{w}} = \frac{\partial{g(z_i)}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}\] <p>又因为</p> \[\frac{\partial{g(z_i)}}{\partial{z_i}}=g(z_i)'=g(z_i)(1-g(z_i))\] <p>于是整合得</p> \[\frac{\partial{-\log J}}{\partial{w}} = \sum_{i=1}^n(g{z_i})-y_i)x_i\] <p>根据梯度定义得到</p> \[w:=w-\alpha\sum_{i=1}^n(g{z_i})-y_i)\] <p><a name="朴素贝叶斯"></a></p> <h2 id="朴素贝叶斯p150">朴素贝叶斯——P150</h2> <p>假设各属性变量之间相互独立，条件独立性假设不成立时，朴素贝叶斯分类器仍有可能产生最优贝叶斯分类器。</p> <p><a name="条件概率"></a></p> <h3 id="条件概率">条件概率</h3> \[P(A|B)=\frac{P(AB)}{P(B)}\] <p><a name="极大似然估计"></a></p> <h3 id="极大似然估计">极大似然估计</h3> <h4 id="离散型分布函数">离散型——分布函数</h4> <p>假设样本服从二项分布</p> <p>目标为 \(max_\theta~~~P(X|\theta)\) 对于小概率事件来说：概率=频率</p> <h4 id="连续型密度函数">连续型——密度函数</h4> <p>假设样本服从高斯分布</p> <p>期望=样本均值，方差=样本方差</p> <h5 id="mle证明最小二乘法">MLE证明最小二乘法</h5> <p>最小二乘法的本质是选择一个y拟合效果最好，也就是选择w,b使得$y_i$. 尽可能对，也就是取到$y_i$ 的概率尽可能大，又由于每个点是独立的即使得密度函数尽可能大，即</p> \[max_{w,b}~~~~~~~~~~J(w,b)=\prod_{i=1}^mP(y_i|w,b)\] <p>而由于假设偏差\(\epsilon_i\) ~\(N(0,\sigma^2)\) ,则\(y_i\)~\(N(w^Tx_i+b,\sigma^2)\) ,则</p> \[P(y_|w,b)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i-w^Tx_i-b)^2}{2\sigma^2})\] <p>则对\(J(w,b)\)取2对数，得到</p> \[log J(w,b)=\frac{-1}{\sqrt{2\pi}\sigma}\sum_i(y_i-w^Tx_i-b)^2\] <p>则目标函数相当于</p> \[min_{w,b}~~~~~~~~\sum_i(y_i-w^Tx_i-b)^2\] <h5 id="例子">例子：</h5> <p><img src="/assets/img/post_img/2.png" alt="图片pic1" /></p> <p>由于\(\epsilon_i\) ~\(N(0,1)\),则\(y_i\) ~\(N(exp(wx_i),1)\) ，则</p> \[P(y_i|w,x_i)=\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i-exp(wx_i))^2}{2})\] <p>则极大似然函数J为</p> \[J(w)=\prod_{i=1}^m P(y_i|w,x_i)\\ =(\frac{1}{\sqrt{2\pi}})^m\prod_{i=1}^mexp(-\frac{(y_i-exp(wx_i))^2}{2})\] <p>则求对数似然函数log J</p> \[log J(w)=D\cdot \frac{1}{2}\sum_{=1}^m(y_i-exp(wx_i))^2\] <p>则对对数似然函数求导得</p> \[\frac{\partial logJ}{\partial w}=-\sum_{i=1}^mx_iexp(wx_i)(y_i-exp(wx_i))\] <p>再令对数=0求解w</p> <h3 id="贝叶斯公式">贝叶斯公式</h3> \[P(A|B)=\frac{P(B|A)P(A)}{P(B)}\] <h3 id="朴素贝叶斯方法">朴素贝叶斯方法</h3> \[argmax_k P(Y=k)\prod_{j=1}^n P(X_j=x_j|Y=k)\] <h3 id="假设">假设</h3> <ul> <li>每个特征相互独立</li> <li>每个实例相互独立</li> <li>训练集的分布和测试集的分布一致</li> </ul> <h3 id="平滑法">平滑法</h3> <p>对于某个数据集，由于数据的稀疏性，我们考虑到对于某个特征X在训练集中没有出现，那么将会导致整个分类概率变为0，这将会导致分类变得非常不合理，所以为了解决零概率的问题，避免过拟合。需要通过平滑法来解决</p> <h4 id="epsilon-平滑法">Epsilon 平滑法</h4> <ul> <li>将\(P(x_m\)|\(y)=0\) 替换为\(P(x_m\)|\(y)=\epsilon\) , 其中\(\epsilon\)远小于$$\frac{1}{N}$​$(N是训练集样本数)</li> </ul> <h4 id="laplace-平滑法加法平滑"><strong>Laplace</strong> 平滑法/加法平滑</h4> <ul> <li> <p>假定训练样本很大时，每个分量x的计数加1造成的估计概率变化可以忽略不计，但可以通过减少方差来方便有效的避免零概率问题。</p> </li> <li> <p>在实际的使用中也经常使用加 \(\alpha\)（\(1\geq \alpha\geq 0\)）来代替简单加1。</p> \[P(X_j=x_j|Y=k) = \frac{\alpha + \text{count}(Y=k,X_j=x_j)}{M\alpha + \text{count}(Y=k)}.\] <p>其中M是\(X_j\)取值类别数。</p> </li> <li> <p>当训练集很小的时候，概率变化剧烈；当数据集大的时候，变化不大。</p> </li> <li> <p>减少方差(various)，增大偏差(bias)</p> </li> </ul> <h3 id="算法流程">算法流程</h3> <p><img src="/assets/img/post_img/3.png" alt="image3" /></p> <p><a name="SVM"></a></p> <h2 id="svmp121">SVM——P121</h2> <p><a name="目标"></a></p> <h3 id="目标">目标</h3> <p>寻找最大边缘超平面，即使得支持向量距离超平面距离尽可能大</p> <p><a name="支持向量"></a></p> <h3 id="支持向量">支持向量</h3> <p>样本中距离超平面最近的一些点，SVM的决策边界完全由支持向量决定，因此当将能够被正确分类且远离决策边界的样本点加入到训练数据中，也不会影响改变SVM原来确定的决策边界。</p> <p><a name="最优化问题"></a></p> <h3 id="最优化问题">最优化问题</h3> <p>最大化两间隔边界之间的距离</p> \[max_w~~~~~~~\frac{2}{||w||}\\\Rightarrow min_w~~~\frac{1}{2}||w||\\ s.t. ~~~~~y_i(w^Tx_i+b)\geq 1\] <p><a name="对偶问题"></a></p> <h3 id="对偶问题">对偶问题</h3> \[max_{\lambda_i}~~~~~~~~~ f(\lambda_i)\\=&gt; max_{\lambda_i}~~-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\lambda_i\lambda_jy_iy_j(x_i.x_j)+\sum_{i=1}^n\lambda_i\\ s.t.~~~~~\sum_{i=1}^n\lambda_iy_i=0,~~~\lambda_i\geq0\] <p><a name="求支撑向量"></a></p> <h3 id="求支撑向量">求支撑向量</h3> <p>求\(\frac{\partial f(\lambda_i)}{\partial \lambda_i}=0\) 得到\(\lambda_i\) ，然后验证\(\sum_{i=1}^n \lambda_iy_i=0\) ，如果满足则\(\lambda_i&gt;0\) 对应的样本就是支撑向量；否则分类讨论，即令其中一个\(\lambda_i=0\) ，求解剩余的\(\lambda_j\) 然后比较每次求得的\(f(\lambda_i)\)选择\(f(\lambda_i)\) 最小时候对应的\(\lambda_i\) ，且\(\lambda_i&gt;0\)对应的样本就是支撑向量。</p> <p><a name="求分界线"></a></p> <h3 id="求分界线">求分界线</h3> <h4 id="最优权重">最优权重</h4> \[w^*=\sum_{i=1}^n\lambda_jy_ix_i\] <p>其中\(x_i\)是列向量</p> <h4 id="截距项">截距项</h4> <p>通过将任意支撑向量X(+1)，代入\(\sum_{i=1}^K\sum_{j=1}^nw_ix_{ji}+b=1\) 得到</p> <p>或通过将任意支撑向量X(-1)，代入\(\sum_{i=1}^K\sum_{j=1}^nw_ix_{ji}+b=-1\)得到</p> <h4 id="最优分界线">最优分界线</h4> \[\sum_{i=1}^K\sum_{j=1}^nw_ix_{ji}+b=0\] <p><a name="运用梯度下降方法求解SVM"></a></p> <h3 id="运用梯度下降方法求解svm">运用梯度下降方法求解SVM</h3> <h4 id="线性svm">线性SVM</h4> <h5 id="改进的线性svm目标函数软间隔">改进的线性SVM目标函数——软间隔</h5> \[min~~~~~\frac{1}{2}||w||_2^2+C\#K\] <p>其中C表示错分的惩罚力度，#K表示所分点个数，即\(K=\{i\)|\(y_i(w^Tx_i+b)&lt;1\}\)</p> <p>相当于 \(min~~~~~\frac{1}{2}||w||_2^2+C \sum_{i\in K}(1-y_i(w^Tx_i+b))\)</p> <h5 id="参数更新">参数更新</h5> \[~~~~~~~~~~~~~~w:=w-\alpha(w-C\sum_{i\in K}y_ix_i)\\ b:=b+\alpha C\sum_{i\in K}y_i\] <h4 id="非线性svm">非线性SVM</h4> <h5 id="非线性svm目标函数">非线性SVM目标函数</h5> \[min~~~~~\frac{1}{2}||w||_2^2+C \sum_{i\in K}(1-y_i(w^T\phi(x_i)))\\ K=\{i|y_i(w^T\phi(x_i))&lt;1\},~~~~~~~w=\sum_{i=1}^n\lambda_iy_i\phi(x_i)\] <p>令\(\alpha_i=\lambda_iy_i\)故化简为</p> \[min~~~~~\frac{1}{2}\sum_{i,j}\alpha_i\alpha_j\phi(x_i)^T\phi(x_j)+C \sum_{i\in K}(1-y_i(\sum_j\alpha_j\phi(x_j)^T\phi(x_i)))\\\] <p>再令\(K_{ij}=\phi(x_i)^T\phi(x_j)\)，则最终化简为</p> \[min~~~~~\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jK_{ij}+C \sum_{i\in K}(1-y_i(\sum_j\alpha_jK_{ij}))\\\] <h5 id="参数更新-1">参数更新</h5> \[\alpha_i:=\alpha_i-\eta(\bar{K}\alpha -C\sum_{i\in K}y_i\bar{K_i})\] <h5 id="模型分类结果">模型分类结果</h5> \[\sum_j\alpha_jK_{ij}&lt;0=&gt;y_i=-1\\ \sum_j\alpha_jK_{ij}&gt;0=&gt;y_i=1\] <h4 id="根据线性非线性目标函数的区别以及c高斯核的sigma取值的性质判断目标函数对应的分类结果图">根据线性、非线性目标函数的区别以及C,高斯核的\(\sigma\)取值的性质，判断目标函数对应的分类结果图</h4> <ol> <li> <p>C表示分错点的惩罚程度。当C越大的时候说明对分错点的惩罚程度就越大，也就是对错误点的容忍率越低，分错点就会越少，（也就是强行将分割面插在很近的两个异类点之间）这时候两支撑面之间的间隔就会变小，\(\|w\|\) 会增大。但随着C趋向无穷，也就退回到了原始SVM，没有错误点可以容忍，此时\(\|w\|\) 不会改变，间隔也不会变，支撑向量数量减少（由于在改进SVM中那些容错点（两支撑面之间的点）也是支持向量）。</p> </li> <li> <p>\(\sigma\)越小，数据点越少，越容易造成过拟合</p> </li> </ol> <p><img src="/assets/img/post_img/4-1.png" alt="image-4-1" /> <img src="/assets/img/post_img/4-2.png" alt="image-4-2" /> <img src="/assets/img/post_img/4-3.png" alt="image-4-3" /></p> <p>首先发现1，2的目标函数表示的是线性SVM，而1中的C=0.1,2中的C=1，则说明2中的SVM分错点更少一点，则1对应的图为c，2对应的图为b。</p> <p>其次发现3，4，5的目标函数都是非线性SVM，且3中选择的核函数是多项式，而4，5选择的是高斯核函数，则3对应的决策边界是二次曲线则图应该为b。进而，4中的\(\sigma^2=1\),5中的\(\sigma^2=0.5\)，则5的拟合效果会更好曲线更弯曲且可能会过拟合因此5对应的e图，4对应的是a图</p> <p><a name="决策树"></a></p> <h2 id="决策树p73">决策树——P73</h2> <p><a name="决策树针对缺失数据的处理办法"></a></p> <h3 id="决策树针对缺失数据的处理办法">决策树针对缺失数据的处理办法</h3> <ol> <li>放弃含缺失值的样本，仅使用无缺失值的样本来进行学习</li> <li>根据此属性值已知的其他样本，来估计这个缺失的属性值 <ul> <li>赋给它当前结点所有样本中该属性最常见的值</li> <li>赋给它当前结点同类样本中该属性最常见的值</li> <li>为含缺失值属性的每个可能值赋予一个概率</li> </ul> </li> </ol> <p><a name="信息熵"></a></p> <h3 id="信息熵">信息熵</h3> <h4 id="范围">范围</h4> <h4 id="0log_2n-0-确定事件log_2n-均匀分布">[0,\(\log_2n\)]， 0-&gt;确定事件;\(\log_2n\)-&gt;均匀分布</h4> <h4 id="特征">特征</h4> <p>Ent(x): 当x的取值越多越大</p> <p>Ent(x,a): 当a的前提下，x的取值越多越大</p> <p><a name="决策树类型"></a></p> <h3 id="决策树类型">决策树类型</h3> <p><a name="ID3"></a></p> <h4 id="id3基于信息增益">ID3(基于信息增益)</h4> \[max_a~~~~~~Gain(D,a)=Ent(D)-Ent(D,a)\] <p><a name="C4.5"></a></p> <h4 id="c45基于信息增益率">C4.5(基于信息增益率)</h4> \[max_a~~~Gain\_ratio(D,a)=\frac{Gain(D,a)}{Ent(a)}\] <p><a name="CART"></a></p> <h4 id="cart基于基尼系数">CART(基于基尼系数)</h4> \[min_a~~~~~Gini(D,a)=\sum_{i=1}^mp(a_i)Gini(D_i)\\ Gini(D_i)=1-\sum_{k\in D_i} p(k)^2\] <p><img src="/assets/img/post_img/5.png" alt="image5" /></p> <h5 id="终止分支的条件">终止分支的条件</h5> <ul> <li>Gain &lt;= threshold</li> <li>该节点上所有样本的类别相同</li> <li>所有特征都已经用过了</li> </ul> <h5 id="注意">注意</h5> <ul> <li>信息增益不会&lt;0</li> <li>叶子节点不一定确保只有一个类别</li> </ul> <p><a name="感知机算法"></a></p> <h2 id="感知机算法">感知机算法</h2> <p><a name="目标"></a></p> <h3 id="目标-1">目标</h3> <p>使得错分点个数尽可能的少</p> \[min ~~~~~\#K\] <p>其中\(K=\{i\)|\(y_i(w^Tx_i+b)&lt;0\}\)，相当于使错分类样本到分界面距离之和最小，即</p> \[min ~~~\sum_{i\in K}|w^Tx_i+b|\\ =&gt;min ~~~\sum_{i\in K} -y_i(w^Tx_i+b)\] <p><a name="运用梯度下降进行求解"></a></p> <h3 id="运用梯度下降进行求解">运用梯度下降进行求解</h3> <p>每次随机选择一个错分点\((x_i,y_i)\)进行参数更新</p> \[w:=w+\alpha y_ix_i\\ b:=b+\alpha y_i\] <p><a name="具体做题步骤"></a></p> <h3 id="具体做题步骤">具体做题步骤</h3> <p>随机选取一个样本，若该样本是满足\(y_i(w^Tx_i+b)\leq0\)也就是错分点，就对参数进行更新，直到参数使得所有样本点都不是错分点为止。</p> <h2 id="神经网络dnnp98">神经网络DNN——P98</h2> <h3 id="多层感知机算法前馈神经网络mlp">多层感知机算法（前馈神经网络）MLP</h3> <h3 id="bp算法">BP算法</h3> <p>相当于优化过程，参数更新</p> \[\delta^{(i)}=\delta^{(i+1)} {w^{(i+1)}}\cdot h(X^{(i)}).\\ w^{(i)}:=w^{(i)}-\alpha X^{(i)}\delta^{(i)}\\ b^{(i)}:=b^{(i)}-\alpha \delta^{(i)}\] <p><a name="LDA"></a></p> <h2 id="ldap60">LDA——P60</h2> <p><a name="二分类"></a></p> <h3 id="二分类">二分类</h3> <ol> <li>计算每个类别点集的中心点\(\mu_i\)</li> <li>计算每个类别点集的协方差矩阵\(\Sigma_i\)</li> <li>计算类间散度矩阵\(S_w=\Sigma_1+\Sigma_2\)</li> <li>判断\(S_w\)是否可逆（在python中用np.linalg.det（）求解矩阵A的行列式|A|，如果行列式为0，不可逆，否则可逆。） <ul> <li>若可逆，则\(w^*=S_w^{-1}(\mu_1-\mu_2)\)</li> <li>若不可逆，则\(w^*=(S_w+\lambda I)^{-1}(\mu_1-\mu_2)\)</li> </ul> </li> </ol> <p><a name="多分类"></a></p> <h3 id="多分类">多分类</h3> <ol> <li> <p>计算整个样本点集的中心点\(\mu\)</p> </li> <li> <p>计算每个类别点集的中心点\(\mu_i\)</p> </li> <li> <p>计算每个类别点集的协方差矩阵\(\Sigma_i\)</p> </li> <li> <p>计算全局散度矩阵\(S_t=\sum_{i=1}^m (x_i-\mu)(x_i-\mu)^T\)</p> </li> <li> <p>计算类内散度矩阵\(S_w=\sum_{j=1}^k\Sigma_j\)</p> </li> <li> <p>计算类间散度矩阵\(S_b=\sum_{j=1}^k m_j(\mu_j-\mu)(\mu_j-\mu)^T\)</p> <p>其中\(m_j\)表示第\(j\)类的样本数</p> </li> <li> <p>计算\(S_w^{-1}S_b\)的特征根和特征向量，选取前k个</p> </li> </ol> <h1 id="集成学习">集成学习</h1> <p>集成学习算法存在过拟合、鲁棒性不强等问题。</p> <h2 id="adaboosting">AdaBoosting</h2> <h3 id="算法流程-1">算法流程</h3> <ul> <li> <p>Step1：初始化：最大迭代次数，各样本权重</p> </li> <li> <p>Step2：列出所有可能的弱分类器</p> </li> <li> <p>Step3：计算所有可能的弱分类器的错误率，并选择错误率最小的作为其中一个弱分类器</p> </li> <li> <p>Step4：权重更新</p> \[\alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon})~~~~~~~~其中\epsilon为当前弱分类器的错分率\\ z_i=w_iexp(-\hat{y_i}y_i\alpha)~~~~~~~~~~~~~~~其中\hat{y_i}为预测值\\ w_i^{new}=\frac{z_i}{\sum_{j=1}^mz_j}\] </li> <li> <p>重复Step3,4达到需要的弱分类器个数或强分类器的错误率=0</p> </li> </ul> <h3 id="性质">性质</h3> <ol> <li>所有被错分样本的权重更新比例是相同的，所有被分对的样本的权重更新比例是相同的</li> <li>串行算法</li> <li>AdaBoost模型是弱分类器的线性组合</li> <li>AdaBoost算法的一个解释是该算法实际上是前向分步算法的一个实现，在这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分步算法。</li> </ol> \[\] </article> <div class="post-content controls__inner"> <div class="controls__item prev"> <span>Previous</span> <a href="/2017/10/15/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"> <span> <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11"> <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/> </svg> </span> 线性回归 </a> </div> <div class="controls__item next"> <span>Next</span> <a href="/2020/09/26/%E9%9B%85%E6%80%9D%E7%BB%8F%E9%AA%8C.html"> 雅思经验 <span> <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11"> <path fill="#fillColor" d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z"/> </svg> </span> </a> </div> </div> </div> </main> <footer class="footer"> <div class="container"> <nav class="social"> <a class="social__link" target="_blank" rel="noopener noreferrer" href="https://github.com/HandanYU"> <svg class="social__icon" viewBox="0 0 20 20" width="20px" height="20px"><path d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg> </a> <a class="social__link" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/handanyu"> <svg class="social__icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1468" xmlns:xlink="http://www.w3.org/1999/xlink" width="64" height="64"><defs><style type="text/css"></style></defs><path d="M78 0h868c43.078 0 78 34.922 78 78v868c0 43.078-34.922 78-78 78H78c-43.078 0-78-34.922-78-78V78C0 34.922 34.922 0 78 0z" p-id="1469"></path><path d="M227.328 389.824h114.624v432.399H227.328V389.824z m572.8 24.8c28.32 23.392 42.464 62.224 42.464 116.416v291.184h-115.84V559.216c0-22.752-3.023-40.208-9.056-52.352-11.008-22.24-32-33.328-63.008-33.328-38.08 0-64.192 16.271-78.353 48.8-7.359 17.2-11.008 39.136-11.008 65.824v234.063H452.272V390.608h109.456v63.088c14.496-22.208 28.191-38.208 41.088-48 23.184-17.456 52.544-26.192 88.096-26.192 44.496 0.016 80.897 11.696 109.216 35.12zM355.872 257.216c-0.004 39.345-31.903 71.237-71.248 71.232-39.345-0.005-71.236-31.903-71.232-71.248s31.903-71.237 71.248-71.232c39.344 0.005 71.237 31.903 71.232 71.248z" fill="#FFFFFF" p-id="1470"></path></svg> </a> <!-- --> </nav> <span>&copy; 2022 Diana&#39;s Blog. All rights reserved.</span> </div> </footer> <script async src="/assets/js/bundle.js"></script> </body> </html>
