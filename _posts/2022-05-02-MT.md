---
layout: post
title: Machine Translation
summary: Learn about Machine Translation
featured-img: deep learning
language: chinese
category: NLP
---

![image1](/assets/img/post_img/141.png)

# Machine Translation

is the task of translating text from one **source language** to another **target language**

## 0. Why do we need MT

- Removes language barrier 
- Makes information in any languages accessible to anyone

## 1. Why it is difficult

- 不只是word translation
- structural会改变, e.g., syntax & semantics
- Multiple word translations, idioms 
- Inflections for gender
- 可能会miss information

## 2. Statistical MT

### 2.1 Definition

假设我们想将French sentence $$f$$ 翻译为English sentence $$e$$。则我们的目标是
$$
\arg\max_e P(e\lvert f)
$$
使用Baye's rule进行分解得到
$$
\arg\max_e P(f|e)P(e)
$$

- $$P(e)$$被称为**language model**
  - 用于学习得到fluent English text
  - 通过text statistics in large **monolingual corpora**中学习
- $$P(f\lvert e)$$ 被称为**translation model**
  - 用于将English words & phrases翻译为French
  - 通过word co-occurrences (i.e., English-French **sentence pairs**) in **parallel corpora**中学习
  - parallel corpora: 同一句话（文章）的多种语言表示

因此我们可以这样理解，为了想把French翻译为English，我们从corpus中找到相似的English $$e$$ （**Encode**），然后将$$e$$翻译为French (**Decode**)，判断得到的和真实的$$f$$区别，选择概率最高的，也就是相似程度最大的。

### 2.2 Models

我们如何从parallel corpora中学习$$P(f\lvert e)$$。 注意在parallel corpora中只有sentence pairs，而没有words pairs。因此idea是引入**word alignment**作为latent variable。

#### word alignment

引入latent vatiable:
$$
P(f,a \lvert e)
$$
也就是若得到$$e$$，它在parallel corpora中对应的$$f$$，且遵循$$a$$这种word alignment的概率。可以通过EM算法来求解(e.g., GIZA++)。

##### Complexity of Alignment

- Some words are dropped and have no alignment

  ![image1](/assets/img/post_img/142.png)

- One-to-many alignment

 ![image1](/assets/img/post_img/143.png)

- Many-to-one alignment

![image1](/assets/img/post_img/144.png)

- Many-to-many alignment

![image1](/assets/img/post_img/148.png)

### 2.3 Issues

- 需要很多的feature engineering
- 最先进的systems十分的复杂
  - 很难maintain
  - 需要很多effort去建立new language pairs

## 3. Neural Machine Translation

- 使用a single neural model去直接把source sentence翻译为target sentence

- 同样需要parallel text

### 3.1 Encoder-decoder

- 使用的**encoder-decoder** architecture

  - 首先用一个RNN来encode source sentence

    - $$h_i=RNN_1(h_{i-1}, x_i)$$

  - 再用另一个RNN来decode target sentence

    - 可以被看成一个**conditional language model**
      - language model: predict下一个word利用target sentence *y* 中的previous given words
      - Conditional: 需要依赖于source sentence *x*
      - $$P(y\lvert x)=P(y_1\lvert x)P(y_2\lvert y_1,x)\dots P(y_t\lvert y_1,\dots, y_{t-1}, x)$$
    - $$s_t=RNN_2(s_{t-1}, y_t)$$
    - 其中$$s_1=h_{\lvert x\lvert}$$, $$\lvert x\lvert$$表示sentence的长度

    

![image1](/assets/img/post_img/145.png)

### 3.2 Training Loss

对于decoder部分，依据已知的target sentence, 计算每个word预测得到的negative logprob，并求和

![image1](/assets/img/post_img/146.png)
### 3.3 Decoding in test

![image1](/assets/img/post_img/147.png)

#### Argmax Decoding / Greedy Decoding

在test的时候，由于我们不知道target sentence，因此通过argmax预测next word，并将预测得到的next word作为target sentence中的next word

**Problems**

但这会造成**error propogation/exposure bias**（model is unable to recover from its own error），也就是当其中一个next word预测的有偏差，会导致next next word更加远离真实的。从而不能确保得到optimal probability

![image1](/assets/img/post_img/149.png)

#### Exhaustive Search Decoding

- 为了得到optimal probability，我们需要考虑每一步中可能预测的**所有word**，并计算它们在所有possible sentence中的probability.
- 复杂度为$$O(V^n)$$，其中V=vocab size, n=sentence length
- 成本很高，不大可行

#### Beam Search Decoding

在每一步中只选择考虑**k best words**， k=beam width。

- 当k=1的时候就是greedy decoding
- 当k=V的时候就是exhaustive search decoding

##### **Example:** 

也就是建立k叉树，最终选择probability最高的叶子节点所在路径。

假设我们选择k=2

![image1](/assets/img/post_img/150.png)
![image1](/assets/img/post_img/151.png)

##### When to stop

- 当decoding得到`<end>`token

- 设置early stop的参数
  - hypotheses
  - maximum sentence length can be generated

### 3.3 Issues

- Information of the whole source sentence is represented by a single vector 
- NMT can generate new details not in source sentence  (**bias**)
- Black-box model; difficult to explain when it doesn’t work

### 3.4 Compare with Statistical MT

- NMT是single end-to-end model，而SMT有multiple sub-components
- NMT 的feature engineering会少些
- 会产生new detials不来自于source sentence

## 4. Attention Mechanism

使用单纯的encoder-decoder架构在句子很长的时候无法捕捉到所有的信息 （information bottleneck），在这里我们引入attention机制，对于decoder，在每一步都能attend to words in the source sentence

如图，对于decoder中的第一个time step，通过输入`<start>`的embedding vector，并分别与source sentece中每个word的embedding vector做dot product，然后经过softmax层得到weights，然后将source sentence的word embedding加权求和，并与`<start>`的embedding进行拼接得到prediction embedding (i.e., cow的embedding)。

![image1](/assets/img/post_img/152.png)

同样对于第二个time step，通过输入cow的embedding vector，分别与source sentence中每个word embedding做dot product，经过softmax层得到weights，通过将source sentence的word embedding加权求和，并与cow 的embedding进行拼接得到prediction的word embedding (i.e., eats的embedding)

![image1](/assets/img/post_img/153.png)

### 4.1 Encoder-decoder with attention

- 对于**encoder**有： $$h_i=RNN_1(h_{i-1}, x_i)$$
- 对于**decoder**有：$$s_t=RNN_2(s_{t-1}, y_t)$$
  - 同时加入attention
  - **dot production**: $$e_t=[s_t^T h_1, s_t^T h_2, \dots s_t^Th_{\lvert x\lvert}]$$
  - **softmax**: $$\alpha_t=softmax(e_t)$$
  - **weighted sum**: $$c_t=\sum_{i=1}^{\lvert x\lvert}\alpha_t^i h_i$$
- 最终将$$c_t$$与$$s_t$$进行concat，得到prediction embedding

### 4.2 Attention的变形(Variants)

对于上面提到的**dot production**，可以有其他变形

- 可以采用**Bilinear** ($$e_t^i=s_t^TWh_i$$)
- 也可以采用**Additive** ($$e_t^i=v^T\tanh(W_ss_t+W_hh_i)$$)

## 5. Evaluation

### 5.1 BLEU

通过计算"reference" translation (即人工翻译）与 generated translation正确重合的n-gram
$$
BLEU=BP\times \exp(\frac{1}{N}\sum_n^N\log p_n)\\
p_n = \frac{\# \text{correct n-grams}}{\# \text{predicted n-grams}}\\
BP=\min(1, \frac{\text{output length}}{\text{reference length}})
$$

- 其中BP （Brevity Penalty）用于penalise较短的output

