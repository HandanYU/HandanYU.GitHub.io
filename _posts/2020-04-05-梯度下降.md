---
layout: post
title: 梯度下降
summary: 
featured-img: 梯度下降

---

## 3.3 梯度下降
### 3.3.1 （批量）梯度下降法(BGD)
#### 1. 递推公式推导过程

通过多项式对函数进行逼近的方法得到

$$f(x)=f(x_0)+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\dots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$$

故

$$f(x)≈f(x_0)+f'(x_0)(x-x_0)$$

其中令$\Delta x=x-x_0$，则有

$$f(x)≈f(x_0)+\Delta x\nabla f(x_0)$$

则得到

$$f(x)-f(x_0)=\Delta x\nabla f(x_0)$$

其实$$\Delta x$$和$$\nabla f(x_0)$$都是向量，因此需要$$f(x)-f(x_0)$$最小，只有当$$\Delta x$$和$$\nabla f(x_0)$$两个方向相反，故有

$$\Delta x=-\alpha \nabla f(x_0)~~~~~~~\alpha >0$$

最终得到梯度下降法的递推公式为

$$x:=x-\alpha \nabla f(x)$$

#### 2. 求解最小二乘问题（采用梯度下降算法求解线性回归模型最优解）

- 线性回归模型

$$f_\omega(x)=\omega_0+\omega_1x+\dots+\omega_dx_d$$

其中d表示有d个特征.
- 损失函数

$$J(\widehat{\omega})=\frac{1}{2m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)^2$$

其中m表示有m个样本点.
- 使用梯度下降最小化损失函数，求解最优解

根据梯度下降递推公式有

$$\widehat{\omega}:=\widehat{\omega}-\alpha\frac{\partial{J(\widehat{\omega})}}{\partial{\widehat{\omega}}}$$
即

$$\widehat{\omega}:=\widehat{\omega}-\frac{\alpha}{m}X^T(X\widehat{\omega}-y)=\widehat{\omega}-\frac{\alpha}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$


#### 3. 优点
此时时间复杂度为$O(md)$,迭代次数少
#### 4. 缺点
每次迭代都要用到训练集所有的数据，因此当数据量大的时候迭代速度会很慢
### 3.3.2 随机梯度下降（SGD）
通过观察发现$\frac{1}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$相当于是所有训练集样本点的均值

记$z_i=(X_i\widehat{\omega}-y_i)X_i^T$，每个样本点i选取的概率为$\frac{1}{m}$，则有

$$E_i(z_i)=\frac{1}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$

故随机梯度下降法的递推公式为

$$\widehat{\omega}:=\widehat{\omega}-\alpha(X_j\widehat{\omega}-y_j)X_j^T$$


#### 优点
每次迭代使用随机的一个样本来对参数进行更新，使得训练速度加快，此时的时间复杂度为$O(d)$
#### 缺点
1. 准确度下降，当目标函数为强凸函数的情况下，无法做到线性收敛
2. 可能会收敛到局部最优
3. 迭代次数比BGD多

### 3.3.3 小批量梯度下降法(MBGD)
鉴于BGD,SGD的优缺点，提出每次迭代使用部分样本来对参数进行更新，故MBGD的迭代公式为

$$\widehat{\omega}:=\widehat{\omega}-\frac{\alpha}{\# J}\sum\limits_{j\in J}(X_j\widehat{\omega}-y_j)X_j^T$$


### 3.3.4 Adagrad

#### 递推公式
- step1:计算梯度$\nabla f(\widehat{\omega})$
- step2:累加平方梯度$r_{k+1}=r_k+(\nabla f(\widehat{\omega}_k))^2$
- step3:计算更新参数$$\widehat{\omega}_{k+1}:=\widehat{\omega}_k-\frac{\alpha}{\sqrt{r_{k+1}^2+\epsilon}}\nabla f(\widehat{\omega}_k)$$

#### 适用情况

有些参数已经近乎最优，因此只需要微调了，而另一些可能还需要很大的调整。这种情况可能会在样本较少的情况下出现，比如含有某一特征的样本出现较少，因此被代入优化的次数也较少，这样就导致不同参数的下降不平衡。adagrad就是来处理这类问题的。
#### 优点
不同的分量选取不同的学习率。在平缓的分量下降稍快，在陡峭的分量下降稍慢。
#### 缺点
累计梯度平方导致时间复杂度变大，迭代次数较多时，学习率过早过量减少
### 3.3.5 RMSprop
#### 递推公式
- step1:计算梯度$\nabla f(\widehat{\omega})$
- step2:加权累加平方梯度$r_{k+1}=\rho r_k+(1-\rho)(\nabla f(\widehat{\omega}_k))^2$,其中$\rho$表示学习率的衰减速率一般取0.9
- step3:参数更新$\widehat{\omega}_{k+1}:=\widehat{\omega}_k-\frac{\alpha}{\sqrt{r_{k+1}+\epsilon}}\nabla f(\widehat{\omega}_k)$
#### 优点
解决Adagrad的学习率逐渐消失的问题
### 3.3.6 Momentum
#### 主要思想
借助物理中动量的概念，更新时在一定程度上保持之前更新的方向，同时利用当前的梯度微调最终的方向。
#### 递推公式
- step1:计算梯度$\nabla f(\widehat{\omega})$
- step2:冲量的梯度更新$v_{k+1}=\alpha v_k-\eta\nabla f(\widehat{\omega})$
- step3:参数的更新$x_{k+1}=x_k+v_k$

### 3.4 Logistic回归与梯度上升法
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/E55E53CB00F4446E81CC1A64EEB3752E/4254)
#### 3.4.2 逻辑回归解决多分类问题
##### 一、 OvR
- 思想：

n 种类型的样本进行分类时，分别取一种样本作为一类，将剩余的所有类型的样本看做另一类，这样就形成了 n 个二分类问题，使用逻辑回归算法对 n 个数据集训练出 n 个模型，将待预测的样本传入这 n 个模型中，所得概率最高的那个模型对应的样本类型即认为是该预测样本的类型；
- 时间复杂度：O(n)
- 示意图
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/46794D7A7A51496BB7AD86E76BCEC393/4479)
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/0538F5351AF64FD1A705F6B44201FE32/4481)
##### 二、OvO
- 思想

 n 类样本中，每次挑出 2 种类型，两两结合，一共有 $C_n^2$ 种二分类情况，使用$C_n^2$种模型预测样本类型，有$C_n^2$ 个预测结果，种类最多的那种样本类型，就认为是该样本最终的预测类型；
- 时间复杂度：$O(n^2)$
- 示意图
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/DA7CAE16C2304323B12F9C348548E484/4493)
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/F9B7008076594B5392C5738390C3E876/4495)
##### 三、softmax
- 思想

由于发现OvR的所有概率相加通常会超过1，因此对OvR进行改进，对概率进行归一化
- 推导

令$z_i^j=(w^j)^Tx_i$,其中$w^j$表示第$j$类的权重(是个列向量)，$x_i$表示第$i$个样本(是个列向量)。

将$z_i^j$映射到$y_j(z_i)=e^{z_i^j}/\sum\limits_{c=1}^Ce^{z_i^c}$，其中C表示有C个类，$y_j(z_i)$表示第j类的softmax值，即样本$i$属于$j$类的概率

令$gs(z_i)=[y_1(z_i),y_2(z_i),\dots,y_C(z_i)]^T$
相当于用矩阵表示

$$Ps(w^Tx)=\frac{exp(w^Tx)}{1^Texp(w^Tx)},其中1是全为1的向量,w=[w^1,w^2,\dots,w^C]^T$$

用统计学知识解释，那么在多分类问题中，假设类别标签y∈{1, 2, ..., C}有C个取值，那么给定一个样本x，softmax回归预测x属于类别i的后验概率为：

$$P(y=i|x;w_i)=\frac{exp(w_i^Tx)}{\sum_{c=1}^Cexp(w_c^Tx)}.$$


- 示意图
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/08CA9236CBBE41DEBDF99A64E3A39A23/4514)
- 目标函数

$$L=\sum_kt_k⋅lnP(y=k)，其中目标类的t_k为1，其余类的t_k为0$$

- 权重递推公式

由于对比普通逻辑回归，只是改变了映射函数

logistic的递推公式

$$w:=w+\alpha \sum_{i=1}^C(x_i(y_i-g(w^Tx_i)))
w:=w-\alpha X^T(g(w^TX)-y)$$

则softmax的递推公式

$$w:=w+\alpha \sum_{i=1}^C(x_i(y_i-Ps(w^Tx_i)))
w:=w-\alpha X^T(Ps(w^TX)-y)$$


### 3.5 线性判别法(LDA)
#### 主要思想
需要找一条直线，希望各点投影在该直线上后，希望同一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。量化这两点感官，则需满足异类点的中心距离远，同类点的方差小
#### 模型建立
假设我们有数据集$D=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}$，其中任意样本$x_i$为n为向量，$y_i\in\{0,1\}$，我们定义
- $X_i(i=0,1)$为第$i$类样本的集合，
- $\#X_i(i=0,1)$为第$i$类样本的个数，
- $a_i(i=0,1)$为第$i$类样本的投影中心点（是一个向量）,
- $\mu_i(i=0,1)$为第$i$类样本的中心点
- $S_i(i=0,1)$为第$i$类样本的方差
- $\Sigma_i(i=0,1)$为第$i$类样本的协方差矩阵

根据投影的知识可得

点$(x_{i1},x_{i2})$在直线$w_1x_1+w_2x_2=0$上的投影相当于向量$(x_{i1},x_{i2})$在向量$(w_1,w_2)$上的投影，即为向量$(x_{i1},x_{i2})$与向量$(w_1,w_2)$的点积$(x_i)^Tw$

则有
$$
a_i=\frac{1}{\#X_i}\sum_{j\in X_i}(x_j)^Tw~~~~~~~~~~~

=(\frac{1}{\#X_i}\sum_{j\in X_i}(x_j)^T)w~~

=\mu_iw
~~~~~~~~~~(i=0,1)
$$

$$
S_i=\sum_{j\in X_i}((x_j)^Tw-a_i)^2~~~~~~~~~~~~~~~~~~~~~~

=\sum_{j\in X_i}(((x_j)^T-\mu_i)w)^2~~~~~~~~~~~~~~（1）

=\sum_{j\in X_i}((x_j-\mu_i)^Tw)^2~~~~~~~~~~~~~~~~~（2）

=\sum_{j\in X_i}w^T(x_j-\mu_i)(x_j-\mu_i^T)w~~~（3）

=w^T(\sum_{j\in X_i}(x_j-\mu_i)(x_j-\mu_i)^T)w~~~~~~~~~~

=w^T\Sigma_iw~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~(i=0,1)
$$


- 其中(1)由$a_i$的表达式代入得到
- (2)由于$\mu_i$是数，一个数点转置还是它本身
- (3)根据$(x^Ty)^2=(x^Ty)(x^Ty)=(x^Ty)^T(x^Ty)=y^Txx^Ty$其中$x^Ty$为一个数

根据主要思想，我们建立以下目标函数

由于我们希望$|a_1-a_2|$尽可能大，$S_1+S_2$尽可能小，即可建立目标函数
$$
由于\min~~ |a_1-a_2|=>\min ~~(a_1-a_2)^2

\max\limits_{w}\frac{(a_1-a_2)^2}{S_1+S_2}
$$
下面求解$(a_1-a_2)^2$和$S_1+S_2$
$$
(a_1-a_2)^2=((\mu_1-\mu_2)w)^2=w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw


S_1+S_2=w^T(\Sigma_1+\Sigma_2)w
$$
最终确立目标函数为
$$
\max\limits_w=\frac{w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw}{w^T(\Sigma_1+\Sigma_2)w}
$$
根据拉格朗日乘子法得到最优权重为
$$
w^*=(\Sigma_1+\Sigma_2)^{-1}(\mu_1-\mu_2)
$$
当$\Sigma_1+\Sigma_2$不可逆的时候采用$w^*=(\Sigma_1+\Sigma_2+\lambda I)^{-1}(\mu_1-\mu_2)$










