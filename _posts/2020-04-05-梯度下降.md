---
layout: post
title: 梯度下降和优化器
summary: 本章介绍了梯度下降法的概念，典型的梯度下降法的概念和优缺点，以及几种典型的优化器
featured-img: 梯度下降
---

##### Table of Contents  
- [梯度下降](#梯度下降)  
    - [（批量）梯度下降法(BGD)](#（批量）梯度下降法(BGD))
    - [随机梯度下降（SGD）](#随机梯度下降（SGD）)
    - [小批量梯度下降法(MBGD)](#小批量梯度下降法(MBGD))
- [优化器](#优化器)
    - [Adagrad](#Adagrad)
    - [RMSprop](#RMSprop)
    - [Momentum](#Momentum)
<a name="梯度下降"/>

## 梯度下降

<a name="（批量）梯度下降法(BGD)"/>

### （批量）梯度下降法(BGD)
#### 1. 递推公式推导过程

通过多项式对函数进行逼近的方法得到

$$f(x)=f(x_0)+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\dots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$$

故

$$f(x)≈f(x_0)+f'(x_0)(x-x_0)$$

其中令$$\Delta x=x-x_0$$，则有

$$f(x)≈f(x_0)+\Delta x\nabla f(x_0)$$

则得到

$$f(x)-f(x_0)=\Delta x\nabla f(x_0)$$

其实$$\Delta x$$和$$\nabla f(x_0)$$都是向量，因此需要$$f(x)-f(x_0)$$最小，只有当$$\Delta x$$和$$\nabla f(x_0)$$两个方向相反，故有

$$\Delta x=-\alpha \nabla f(x_0)~~~~~~~\alpha >0$$

最终得到梯度下降法的递推公式为

$$x:=x-\alpha \nabla f(x)$$

#### 2. 求解最小二乘问题（采用梯度下降算法求解线性回归模型最优解）

- 线性回归模型

$$f_\omega(x)=\omega_0+\omega_1x+\dots+\omega_dx_d$$

其中d表示有d个特征.
- 损失函数

$$J(\widehat{\omega})=\frac{1}{2m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)^2$$

其中m表示有m个样本点.
- 使用梯度下降最小化损失函数，求解最优解

根据梯度下降递推公式有

$$\widehat{\omega}:=\widehat{\omega}-\alpha\frac{\partial{J(\widehat{\omega})}}{\partial{\widehat{\omega}}}$$

即

$$\widehat{\omega}:=\widehat{\omega}-\frac{\alpha}{m}X^T(X\widehat{\omega}-y)=\widehat{\omega}-\frac{\alpha}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$


#### 3. 优点
此时时间复杂度为$$O(md)$$,迭代次数少
#### 4. 缺点
每次迭代都要用到训练集所有的数据，因此当数据量大的时候迭代速度会很慢

<a name="随机梯度下降（SGD）"/>

### 随机梯度下降（SGD）
通过观察发现$$\frac{1}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$相当于是所有训练集样本点的均值

记$$z_i=(X_i\widehat{\omega}-y_i)X_i^T$$，每个样本点i选取的概率为$$\frac{1}{m}$$，则有

$$E_i(z_i)=\frac{1}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$

故随机梯度下降法的递推公式为

$$\widehat{\omega}:=\widehat{\omega}-\alpha(X_j\widehat{\omega}-y_j)X_j^T$$

#### 优点
每次迭代使用随机的一个样本来对参数进行更新，使得训练速度加快，此时的时间复杂度为$$O(d)$$

#### 缺点
1. 准确度下降，当目标函数为强凸函数的情况下，无法做到线性收敛
2. 可能会收敛到局部最优
3. 迭代次数比BGD多

<a name="小批量梯度下降法(MBGD)"/>

### 小批量梯度下降法(MBGD)
鉴于BGD,SGD的优缺点，提出每次迭代使用部分样本来对参数进行更新，故MBGD的迭代公式为

$$\widehat{\omega}:=\widehat{\omega}-\frac{\alpha}{\# J}\sum\limits_{j\in J}(X_j\widehat{\omega}-y_j)X_j^T$$

<a name="优化器"/>

## 优化器

<a name="Adagrad"/>

### Adagrad

#### 递推公式
- step1:计算梯度$$\nabla f(\widehat{\omega})$$
- step2:累加平方梯度$$r_{k+1}=r_k+(\nabla f(\widehat{\omega}_k))^2$$
- step3:计算更新参数$$\widehat{\omega}_{k+1}:=\widehat{\omega}_k-\frac{\alpha}{\sqrt{r_{k+1}^2+\epsilon}}\nabla f(\widehat{\omega}_k)$$

#### 适用情况

有些参数已经近乎最优，因此只需要微调了，而另一些可能还需要很大的调整。这种情况可能会在样本较少的情况下出现，比如含有某一特征的样本出现较少，因此被代入优化的次数也较少，这样就导致不同参数的下降不平衡。adagrad就是来处理这类问题的。
#### 优点
不同的分量选取不同的学习率。在平缓的分量下降稍快，在陡峭的分量下降稍慢。
#### 缺点
累计梯度平方导致时间复杂度变大，迭代次数较多时，学习率过早过量减少

<a name="RMSprop"/>

### RMSprop
#### 递推公式
- step1:计算梯度$$\nabla f(\widehat{\omega})$$
- step2:加权累加平方梯度$$r_{k+1}=\rho r_k+(1-\rho)(\nabla f(\widehat{\omega}_k))^2$$,其中$$\rho$$表示学习率的衰减速率一般取0.9
- step3:参数更新$$\widehat{\omega}_{k+1}:=\widehat{\omega}_k-\frac{\alpha}{\sqrt{r_{k+1}+\epsilon}}\nabla f(\widehat{\omega}_k)$$

#### 优点
解决Adagrad的学习率逐渐消失的问题

<a name="Momentum"/>

### Momentum
#### 主要思想
借助物理中动量的概念，更新时在一定程度上保持之前更新的方向，同时利用当前的梯度微调最终的方向。
#### 递推公式
- step1:计算梯度$$\nabla f(\widehat{\omega})$$
- step2:冲量的梯度更新$$v_{k+1}=\alpha v_k-\eta\nabla f(\widehat{\omega})$$
- step3:参数的更新$$x_{k+1}=x_k+v_k$$

