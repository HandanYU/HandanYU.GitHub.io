---
layout: post
title: NLP考点
summary: 本章主要总结了面试中涉及到的NLP相关的一些问题。
featured-img: machine learning
language: chinese 
category: NLP
---
# BERT 相关问题
## BERT的具体网络结构
- Bert由12层transformer的**Encoder**组成[!!没有decoder]
- 输入为一个长度为512的3个词向量(Embedding)之和
    - word embedding (WordPiece)
    - Position embedding
    - Segment embedding
- 每层transformer由一个**multi-head attention**，一个feed forward和两层**layerNorm**构成
## BERT的预训练过程
> 用一句话描述BERT预训练过程：从数据集中抽取A, B两句话，其中B有50%的概率是A的下一句，然后分别输入这两句话的三个embedding之和。接下来进行随机遮掩输入序列中15%的词，并要求Transformer预测这些被遮掩的词，以及B是A的下一句的概率任务。
- NSP (Next sentence prediction)
    - 判断句子B是否是句子A的下文
- MLM (Masked language model)
     - 随机mask k%的tokens
     - 这k%的tokens，其中80%被替换为[mask]，10%被其他token替换，剩余10%不改变
## Multi-head attention的具体结构
> multi-head attention相当于在多个空间维度上寻找embedding
- 首先一个64的hidden向量，被映射成Q, K, V。
- Q, K, V分别通过n次线性变换得到n组Q,K,V，也就是对应的n-head
    - 不妨假设batch_size为32，seqlen为512, 12个head。
    - 也就是Q, K, V的size分别为(32 $$\times$$ 12 $$\times$$ 512 $$\times$$ 64)
- 对每一组$$Q_i, K_i, V_i$$，通过Attention (**scaled dot-product attention**)得到相应的$$Head_i$$
    - scaled dot-product attention
        - 首先通过query和key做乘法得到每对对象之间的匹配度$$QK^T$$
            - $$QK^T$$的size = (32 $$\times$$ 12 $$\times$$  512 $$\times$$ 512)
        - 在进行缩放并做softmax得到attention score: $$softmax(\frac{QK^T}{\sqrt{64}})$$。由于当Q和K相乘的时候使得结果大幅度增长，方差会变成原来的64倍，也有利于减小梯度。
        - 得到attention score后与V相乘，得到加权和作为最终的输出。$$softmax(\frac{QK^T}{\sqrt{64}})V$$
            - $$softmax(\frac{QK^T}{\sqrt{64}})V$$的size为(32 $$\times$$ 12 $$\times$$ 512 $$\times$$ 64)
- 将scaled dot-product attention得到的所有矩阵进行拼接
- 进行一次线性变换，用于降维，使得维度与原始embedding维度一致。
![image-1](/assets/img/post_img/MH.png)
## Position Embedding
### Position Embedding的重要性

- 由于self attention可以实现并行，也就是说词与词之间不存在顺序关系（也就是打乱一句话，这句话中的每个词的词向量依然不会改变）
- 由于Attention 值的计算最终会被加权求和，也就是说无论单词顺序如何，最终计算的 Attention 值都是一样的，进而也就表明了 Attention 丢掉了单词序列顺序信息。
- 因此我们引入Position Embedding，在原始词向量中加入它。

### Position Embedding vs. Position Encoding
- Position Embedding是学习出来的encoding，作为Input embedding的一部分。
- 而Position Encoding是用来获取词向量的位置向量。

## Position Encoding
> 需要注意的一点是：在BERT中Position Encoding是采用一种特殊的自学习方法。以下所讲是Transformer中比较传统的一种方法。

### Position Encoding 的实现
假设$t$表示当前输入句子中想要编码的位置，则该位置的编码向量可以表示为：

$$PE(t,2k) = \sin(t \cdot w_k) $$

$$PE(t, 2k+1) = \cos(t\cdot w_k)$$

其中$$w_k=\frac{1}{10000^{2k/d}}$$，$$d$$表示编码维度（一般为512）。

![image-1](/assets/img/post_img/PE.png)
> 接下来来看看该编码方法是如何体现相互之间的位置关系的：

借助三角函数的【和积化差】公式，

$$\sin(\alpha+\beta)=\sin\alpha\cos\beta+\cos\alpha\sin\beta$$

$$\cos(\alpha+\beta)=\cos\alpha\cos\beta-\sin\alpha\sin\beta$$

则对于$PE(t,i)$，可以展开为:

$$PE(t+\Delta ,2k)=\sin t\cos \Delta+\cos t\sin\Delta\\=PE(t,2k)PE(\Delta,2k+1)+PE(t,2k+1)PE(\Delta,2k)$$

$$PE(t+\Delta, 2k+1)=\cos t\cos \Delta-\sin t\sin\Delta\\=PE(t,2k+1)PE(\Delta, 2k+1)-PE(t,2k)PE(\Delta,2k)$$

于是我们可以看出：对于某个$$t+\Delta$$位置向量的某一个维度$$2k$$或者$$2k+1$$而言，可以表示为位置$$t$$和位置$$\Delta$$的在$$2k$$或者$$2k+1$$维度的位置向量线性组合。也说明“会”是在“明”和“天”之后的。

例如上图，$$t=1=明$$,$$\Delta=2=天$$,$$t+\Delta=3=会$$，也就是说，“会”的某一维的位置向量可以由“明”，“天”某一维的位置向量线性组合得到，因此说“会”的位置向量蕴涵了“明”，“天”的位置信息。

### Position Encoding的重要性

通过Position Encoding使得某个单词的位置信息是其他单词位置信息的线性组合，这种线性组合就意味着位置向量中蕴含了相对位置信息。

## BatchNorm vs. layerNorm

### BatchNorm
- 图像领域用BN比较多的原因是因为每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起规范化。
- BatchNorm针对一个batch里面的数据进行规范化，针对单个神经元进行
- 一般放置在卷积层（conv层）或者全连接层之后（激活层之前）
- padding不会对BN造成影响，因为BN是针对一个batch的，
- 具体步骤
    - 计算每个batch的均值和方差
    - 对每个batch中的神经元，用其所在batch的均值和方差进行归一化
### LayerNorm
- LN针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。
- LN不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。 

- 具体步骤：
    - 对于每个训练样本，计算最后一个维度的均值和方差
        - 例如：已知训练样本是一个$n\times m$的矩阵，则可计算得到$n\times 1$的均值向量和方差向量
    - 进行归一化处理：$$\hat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$$，其中$$x^{(k)}$$表示第k个神经元的输入值
        - $$y^{(k)}=w^{(k)}\hat{x}^{(k)}+b^{(k)}$$
        - $$w^{(k)},b^{(k)}$$的作用：这两个参数归一化是为了让本层网络的输出进行额外的约束，但如果每个网络的输出被限制在这个约束内，就会限制模型的表达能力。
- LN 会是pad的地方值为非零，因此会引起不必要的权重更新



    
## 为什么要用multiply来完成self-attention，而不是用权重和的形式
因为multiply的计算效率更高，并随着$d_k$的增大，multiply的优势越来越明显。

## Transformer中的attention为什么要进行scaled
因为一个比较大的值作为softmax的输入，会使得softmax的梯度很小，导致梯度消失，使得很难真正达到收敛。
也就是，如果计算softmax的元素方差太大，将会导致softmax结果稀疏，进而导致梯度稀疏
> softmax的概率分布以及梯度受数量级的影响
1. 当输入值很大的时候，softmax的输出概率会集中在这个值上
2. 当输入值很大的时候，softmax的梯度

## 为什么scaled的时候用的是$$d_k$$
已知$$E[q_i] = E[k_i]=0$$, $$Var[q_i]=Var[k_i]=1$$，则有：


$$E[qk]=E[\sum_{i=1}^{d_k}q_ik_i]\\=\sum_{i=1}^{d_k} E[q_ik_i]\\=\sum_{i=1}^{d_k}E[q_i]E[k_i]=0$$

$$Var[qk]=Var[\sum_{i=1}^{d_k}q_ik_i]\\=\sum_{i=1}^{d_k}Var[q_ik_i]\\=\sum_{i=1}^{d_k}Var[q_i]Var[k_i]\\=\sum_{i=1}^{d_k}1=d_k$$

因此qk~N(0,$$d_k$$)


## self-attention相比lstm, RNN优点缺点是什么？
### 优点
- 能解决长序列记忆问题，因为self attention 能够保证得到句子中任意一个单词与其他所有单词的关系。
- 可以实现并行，加快计算效率
- （新的词向量）能够得到更多表征信息（句法特征和语义特征）
    - 句法特征：wordA常常出现在wordB后面
    - 语义特征：wordA常常指代了wordB或wordC

### 缺点
- 比较适合短句子，由于self attention计算成本太高，需要进行一一计算
## attention与self-attention的区别
- attention简单的说就是QKV相乘，并没有规定QKV是怎么来的。而对于self-attention来说，QKV是同源的。
- self-attention 是attention的一种
- attention和self-attention 其具体计算过程是一样的，只是计算对象发生了变化而已。
- attention是Attention机制发生在**Target的元素Query和Source中的所有元素之间**。即，source对target的attention。其中只有$$K\approx V\approx X$$
- 而self-attention 是**source 对source的attention**或者**target对target的attention**。也就是$$Q\approx K\approx V\approx X$$
### attention
![image-1](/assets/img/post_img/attention.png)

> attention机制的意义：从大数据中获取重要的信息

> 简短描述attention的过程：将 Query(Q) 和 key-value pairs（把 Values 拆分成了键值对的形式） 映射到输出上，其中 query、每个 key、每个 value 都是向量，输出是 V 中所有 values 的加权，其中权重是由 Query 和每个 key 计算出来的


- 假设一个单词的embedding为X
- 引入查询变量Q（相当于我们人类的眼睛）, 具体数值V ($$V\approx X$$)。
- 通过一个查询变量Q，去V中找到重要信息
    - 为了找到重要信息，我们需要得到一个类似V的数据副本K,也就是$$K\approx V,K=(k_1, k_2, \dots, k_n)$$
    - 接着计算重要度，也就是计算相似度
        - 在这里我们使用**点乘**的方法计算相似值A （也就是S = QK = ($$s_1, s_2,\dots, s_n$$)) 
        - 为了进一步得到重要性/相似度，用一层softmax归一化得到概率向量，$$[a_1,a_2,\dots, a_n] = softmax(s_1,s_2, \dots, s_n)$$，从而就可以找出哪个对Q而言更重要了。
            - 这里要注意的一点是：需要对输入softmax中的值进行归一化处理，因为在数量级较大时， softmax 将几乎全部的概率分布都分配给了最大值对应的标签，由于某一维度的数量级较大，进而会导致 softmax 未来求梯度时会消失。
- 此时我们已经找到信息重要性，接下来通过与原始V做点乘得到各数值的加权值。（相当于对原始信息给予权重）
    - $$V'=[a_1v_1, a_2v_2, \dots, a_nv_n]$$，是单词的新的embedding。与原始embedding的区别在于$$V'$$包含了更多的信息（对于这个单词，周围其他词对它的重要程度）
    - 通过将加权值累计得到最终的注意力值，表示该单词的重要性
> 【使用**点乘**可以进行相似度计算的原因】：向量点乘的几何意义是“向量$$X$$在向量$$Y$$方向上的投影再与向量$$Y$$的乘积”，点乘结果越大，相似度越高。
### self-attention
![image-1](/assets/img/post_img/selfattention.png)
- 与attention的主要区别在于，$$Q,K,V$$来源于同一个$$X$$。也就是通过$X$自己来寻找$X$中的重要信息，因此称为self attention。
- 具体的$$Q,K,V$$是通过$$X$$与$$W_Q,W_K,W_V$$权重矩阵分别右乘得到
- 接下来的具体步骤和attention一样
> self attention的应用：
1. 找关键词
> 【self attention的意义】：能够得到更多context信息，从而得到更准确的词向量表示形式。e.g., 当不进行self attention的时候，对于某个单词来说重定义embedding得到的还是它自己的信息。而self attention通过与周围其他words进行点乘，从而加入了周围其他的信息。


# 主流的Subword算法
## wordpiece的作用

# Transformer
![image-1](/assets/img/post_img/transformer.png)
## 基本架构
- Transformer 也是一个 Seq2Seq 模型（Encoder-Decoder 框架的模型）
![image-1](/assets/img/post_img/ed.png)
- 一般情况下Encoders由6个Encoder组成，Decoders由6个Decoder组成
- 每个Encoder的输出都会和每一层Decoder进行结合

## Transformer的动态流程
- 第一个词的生成
    - 首先将这个句子通过Embedding的形式输入到Encoder，经过多层Encoders得到新的Embedding
    - 然后得到该新的Embedding对应的K,V矩阵，再依次经过Decoder中的Decoder层，得到加权后的Embedding
    - 接着经过Linear层调整维度，并通过Softmax层获取最大概率对应Embedding，并将其作为输出（也就是第一个生成的词）
- 第n个词的生成
    - 将前n-1个生成的词作为Decoder的输入，经过Decoder中Decoder层，再经过Linear+Softmax得到第n个词。
        - 具体的，在经过Decoder过程中，先运用self-attention得到前n-1个生成词的新embedding，然后再与Encoder中得到的K,V做cross multi-attention.
- 当生成词为句子结束符的时候，表明整个句子生成结束。

![image-1](/assets/img/post_img/TM_M.png)
## 为什么Decoder需要做Mask
- 训练阶段：Decoder的输入一直是目标语句(Target)
- 测试阶段：Decoder的输入是已经生成的前n-1个词 （由于此时目标语句是未知的）
- 为了使得训练阶段和测试阶段长度匹配，通过Mask掉测试阶段未生成的词。

## 为什么说K,V来源于Encoder，Q来源于Decoder，且将K,V传递给Decoder
- 由于K,V表示源语句(Source)的另一种表示，因此它们来源于源语句，也就是Encoder
- 又由于Q表示查询语句，我们通过将已经生成的词的信息作为查询语句
- 通过K,V,Q计算（也就是Attention)，运用已经生成的词去源语句中查询下一个重要的可能信息。
- 从而可以减少冗余信息，由于对于传统Encoder-Decoder(e.g., LSTM)，每次都是通过Source语句所有信息去预测下一个词，但其实对于下一个词来说Source语句中很多词信息是冗余的。而Transformer中运用Attention方法，只提取重要信息来进行预测下一个词。