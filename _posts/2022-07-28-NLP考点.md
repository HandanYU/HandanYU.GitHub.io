---
layout: post
title: NLP考点
summary: 本章主要总结了面试中涉及到的NLP相关的一些问题。
featured-img: machine learning
language: chinese 
category: NLP
---
# BERT 相关问题
## BERT的具体网络结构
- Bert由12层transformer的**Encoder**组成[!!没有decoder]
- 输入为一个长度为512的3个词向量(Embedding)之和
    - word embedding (WordPiece)
    - Position embedding
    - Segment embedding
- 每层transformer由一个**multi-head attention**，一个feed forward和两层**layerNorm**构成
## BERT的预训练过程
> 用一句话描述BERT预训练过程：从数据集中抽取A, B两句话，其中B有50%的概率是A的下一句，然后分别输入这两句话的三个embedding之和。接下来进行随机遮掩输入序列中15%的词，并要求Transformer预测这些被遮掩的词，以及B是A的下一句的概率任务。
- NSP (Next sentence prediction)
    - 判断句子B是否是句子A的下文
- MLM (Masked language model)
     - 随机mask k%的tokens
     - 这k%的tokens，其中80%被替换为[mask]，10%被其他token替换，剩余10%不改变
## Multi-head attention的具体结构
- 首先一个64的hidden向量，被映射成Q, K, V。
- Q, K, V分别通过n次线性变换得到n组Q,K,V，也就是对应的n-head
    - 不妨假设batch_size为32，seqlen为512, 12个head。
    - 也就是Q, K, V的size分别为(32 $\times$ 12 $\times$ 512 $\times$ 64)
- 对每一组$Q_i, K_i, V_i$，通过Attention (**scaled dot-product attention**)得到相应的$Head_i$
    - scaled dot-product attention
        - 首先通过query和key做乘法得到每对对象之间的匹配度$QK^T$
            - $QK^T$的size = (32 $\times$ 12 $\times$  512 $\times$ 512)
        - 在进行缩放并做softmax得到attention score: $softmax(\frac{QK^T}{\sqrt{64}})$。由于当Q和K相乘的时候使得结果大幅度增长，方差会变成原来的64倍，也有利于减小梯度。
        - 得到attention score后与V相乘，得到加权和作为最终的输出。$softmax(\frac{QK^T}{\sqrt{64}})V$
            - $softmax(\frac{QK^T}{\sqrt{64}})V$的size为(32 $\times$ 12 $\times$ 512 $\times$ 64)
## Position embedding的方式
## BatchNorm vs. layerNorm

### BatchNorm
- 图像领域用BN比较多的原因是因为每一个卷积核的参数在不同位置的神经元当中是共享的，因此也应该被一起规范化。
- BatchNorm针对一个batch里面的数据进行规范化，针对单个神经元进行
- 一般放置在卷积层（conv层）或者全连接层之后（激活层之前）
- padding不会对BN造成影响，因为BN是针对一个batch的，
- 具体步骤
    - 计算每个batch的均值和方差
    - 对每个batch中的神经元，用其所在batch的均值和方差进行归一化
### LayerNorm
- LN针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。
- LN不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。 

- 具体步骤：
    - 对于每个训练样本，计算最后一个维度的均值和方差
        - 例如：已知训练样本是一个$n\times m$的矩阵，则可计算得到$n\times 1$的均值向量和方差向量
    - 进行归一化处理：$\hat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$，其中$x^{(k)}$表示第k个神经元的输入值
        - $y^{(k)}=w^{(k)}\hat{x}^{(k)}+b^{(k)}$
        - $w^{(k)},b^{(k)}$的作用：这两个参数归一化是为了让本层网络的输出进行额外的约束，但如果每个网络的输出被限制在这个约束内，就会限制模型的表达能力。
- LN 会是pad的地方值为非零，因此会引起不必要的权重更新



    
## 为什么要用multiply来完成self-attention，而不是用权重和的形式
因为multiply的计算效率更高，并随着$d_k$的增大，multiply的优势越来越明显。

## Transformer中的attention为什么要进行scaled
因为一个比较大的值作为softmax的输入，会使得softmax的梯度很小，导致梯度消失，使得很难真正达到收敛。
也就是，如果计算softmax的元素方差太大，将会导致softmax结果稀疏，进而导致梯度稀疏
> softmax的概率分布以及梯度受数量级的影响
1. 当输入值很大的时候，softmax的输出概率会集中在这个值上
2. 当输入值很大的时候，softmax的梯度

## 为什么scaled的时候用的是$d_k$
已知$E[q_i] = E[k_i]=0$, $Var[q_i]=Var[k_i]=1$，则有：


$E[qk]\\=E[\sum_{i=1}^{d_k}q_ik_i]\\=\sum_{i=1}^{d_k} E[q_ik_i]\\=\sum_{i=1}^{d_k}E[q_i]E[k_i]=0$

$Var[qk]\\=Var[\sum_{i=1}^{d_k}q_ik_i]\\=\sum_{i=1}^{d_k}Var[q_ik_i]\\=\sum_{i=1}^{d_k}Var[q_i]Var[k_i]\\=\sum_{i=1}^{d_k}1=d_k$

因此qk~N(0,$d_k$)


## self-attention相比lstm, RNN优点是什么？
- 能够得到更多表征信息
- 可以实现并行，加快计算效率

# 主流的Subword算法
## wordpiece的作用