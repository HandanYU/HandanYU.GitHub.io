---
layout: post
title: Hidden Markov Model (HMM)
summary: 本篇文章主要讲解了HMM在NLP中sequence labelling的应用
featured-img: deep learning
language: chinese
category: NLP
---
# Hidden Markov Model (HMM)

## Motivation

- 将整个句子的tags看作一个序列，作为模型的输出
- 但我们将model的过程设计为一个word-level tagging 的task
- 也就the idea of **sequence labelling**

## Goal

寻找sentence $w$的最佳tag sequence $\hat{t}$
$$
\hat{t} = arg\max_t P(t\lvert w)
$$
由于我们很难直接计算$P(t\lvert w)$，因为对于每个$$P(t_i\lvert w_i)$$,我们不可能假设这个word的tag只和这个word有关这么Naive的假设【因为**w**是个序列，明显这个word的tag和它前面的词和后面的词的tags都有关系】，因此我们尝试运用贝叶斯定律对其进行拆分化简
$$
\hat{t}= arg\max_t\frac{P(w\lvert t)P(t)}{P(w)} \\= arg \max_t P(w\lvert t)P(t)
$$
至此，我们可以做出以下合理假设

- 假设每个word只与其tag有关，与其他word相互独立，则有$$P(w\lvert t)=\prod_{i=1}^nP(w_i\lvert t_i)$$

- 假设每个word的tag只与前一个word的tag有关 ，则有$P(t)=\prod_{i=1}^nP(t_i\lvert t_{i-1})$

$$
\hat{t}=arg\max_t\prod_{i=1}^nP(w_i\lvert t_i)\prod_{i=1}^nP(t_i\lvert t_{i-1})~~~~~~~~~(1)
$$



## Assumptions

### 输出值独立 Output independence

假设输出值只与隐变量 (i.e., tag)有关

### 马尔可夫假设 Markov assumption

假设为一阶的情况：当前state / tag只取决于前一个previous state

## 参数 Parameters

观察目标函数，我们可以看作是有两部分组成，即word与tag关系$$P(w\lvert t)$$以及tag之间关系$$P(t)$$，也就是$$P(w_i\lvert t_i)$$和$$P(t_i\lvert t_{i-1})$$。

### Emission (*Observation/O*) probabilities $P(w_i\lvert t_i$)

我们用Emission Matrix来表示

<img src="/Users/yuhandan/Library/Application Support/typora-user-images/image-20220309211920126.png" alt="image-20220309211920126" style="zoom:50%;" />

- e.g., 单元格**[NNP, Janet]**表示$$P(Janet\lvert NNP)$$

### Transition (*A*) probabilities $P(t_i\lvert t_{i-1})$

我们用Transition Matrix来表示

<img src="/Users/yuhandan/Library/Application Support/typora-user-images/image-20220309211947201.png" alt="image-20220309211947201" style="zoom:50%;" />

- e.g., 单元格**[NNP, MD]**表示$$P(MD\lvert NNP)$$

  

## Training

use MLE

- 对于**Emission probabilities** ，通过counting word frequencies under the specific tag type
  - e.g., $$P(like\lvert \text{VB}) = \frac{count(\text{VB},like)}{count(\text{VB})}$$

- 对于**Transition probabilities**，类似unigram language model
  - e.g., $$P(NN\lvert DT)=\frac{count(DT, NN)}{count(DT)}$$
  - 我们默认用"$$\<s\>$$"来表示第一个word的tag

## Prediction

通过考虑所有的tag combinations，然后选择使得目标值最大的tag sequence/combination

- 有一种错误的想法，对每个word，选择一个tag maximises $$P(w_i\lvert t_i)P(t_i\lvert t_{i-1})$$

  - 这种方法只能做到$$arg\max_{t_i} P(w_i\lvert t_i)P(t_i\lvert t_{i-1})$$ for $$\forall i\in[0, n)$$
  - 这等价于local classifier容易造成error propagation

- 假设sentence length = N, tag size = T，获取所有tag combinations的复杂度是多少$$A_T^N=O(T^N)$$

  

# Viterbi Algorithm

## 原理

为求解HMM，寻找最佳序列，我们可以联想到**动态规划**，这里我们引入**维特比算法（Viterbi Algorithm）**。利用该算法我们可以计算一个sentence的最优 tags 序列组合，但不必穷尽计算所有的可能组合。

我们通过改变上面公式(1)的形式

$$\prod_{i=1}^nP(w_i\lvert t_i)\prod_{i=1}^nP(t_i\lvert t_{i-1})=\prod_{i=1}^n(P(w_i\lvert t_i)P(t_i\lvert t_{i-1}))$$

观察该公式，我们令
$$
s(t_k\lvert w_k) = \max \prod_{i=1}^kP(w_i\lvert t_i)P(t_i\lvert t_{i-1})
$$

$$
s(t_n\lvert w_n) =\max s(t_{n-1}\lvert w_{n-1})\dots \times(P(w_n\lvert t_n)P(t_n\lvert t_{n-1}))
$$

于是根据动态规划的特点，最优解的子集一定是最优解，对于每个word $w_i$的best tag $best_t$，可以通过$$arg \max_{t_{i}} s(t_{i+1}\lvert w_{i+1})$$求得。

于是问题就可以转化为：**根据Transition & Emission Matrix 求 Viterbi  Matrix**

### Viterbi Matrix

- e.g., 单元格**[NNP, Janet]**表示$$s(NNP\lvert Janet)$$

<img src="/Users/yuhandan/Library/Application Support/typora-user-images/image-20220310110204926.png" alt="image-20220310110204926" style="zoom:50%;" />

## 算法流程

- 初始化backpointer matrix，用于记录使得当前state最佳的前一个state的位置
- 获取Transition and Emission Matrix
- 按列填写Viterbi Matrix
  - 对于每个cell $$[t_i,w_i]$$：$$\forall t_{i-1},s.t., s(t_{i-1}\lvert w_{i-1})>0$$ 计算$$ s(t_{i-1}\lvert w_{i-1})\times P(w_i\lvert t_i)\times P(t_{i}\lvert t_{i-1}) $$
  - 并选择最大值作为$$s(t_i\lvert w_i)$$，并记录此时的$$t_{i-1}$$作为$$w_{i-1}$$的best tag
- 开始回溯
  - 对于最后一个word，选择$$arg\max_{t_n} s(t_n\lvert w_n)$$
  - 根据最后一个word的best tag，从backpointer matrix中找到得到该best tag的previous tag
  - 以此类推找到所有的tags

## 例子

假设我们已经从现有corpus中找到Emission Matrix & Transition Matrix，求sentence "**they fish**" 最有可能的Tagging sequence

![image-58](/assets/img/post_img/main58.png)


- 对于第一个单词**they**

$$s(N\lvert they)=P(they\lvert N)\times P(N\lvert <s>)=0.7 \times 0.6 = 0.42$$

$$s(V\lvert they)=P(they\lvert V)\times P(V\lvert <s>)=0.4\times 0.4=0.16$$

- 对于第二个单词**fish**

  - 对于$$s(N\lvert fish)=P(fish\lvert N)\times P(N\lvert they_{tag})\times s(they_{tag}\lvert they)$$

    - If $$they_{tag}=N$$: $$s(N\lvert fish)=P(fish\lvert N)\times P(N\lvert N)\times s(N\lvert they)=0.3\times 0.8\times 0.42=0.1008$$
    - if $$they_{tag}=V$$： $$s(N\lvert fish)=P(fish\lvert N)\times P(N\lvert V)\times s(V\lvert they)=0.3\times 0.7\times 0.16=0.0336$$
    - 显然0.1008 > 0.0336，因此$$s(N\lvert fish)=0.1008$$

  - 对于$$s(V\lvert fish)=P(fish\lvert V)\times P(V\lvert they_{tag})\times s(they_{tag}\lvert they)$$

    - If $$they_{tag}=N$$: $$s(V\lvert fish)=P(fish\lvert V)\times P(V\lvert N)\times s(N\lvert they)=0.6\times 0.2\times 0.42=0.0504$$
    - if $$they_{tag}=V$$： $$s(N\lvert fish)=P(fish\lvert V)\times P(V\lvert V)\times s(V\lvert they)=0.6\times 0.3\times 0.16=0.0288$$
    - 显然0.0504 > 0.0228，因此$$s(V\lvert fish)=0.0504$$

    

- 进行backward找到best tag sequence

  - 由于$$s(N\lvert fish)>s(V\lvert fish)$$，则**fish**在这个sentence中的best tag是**N**，又由于取到这个最优解的$$they_tag=N$$,因此**they**在这个sentence中的best tag是**N**。

- 则sentence **they fish**的最佳tag sequence为**NN**

  

## 时间复杂度

O($T^2N$)

由于对于每个word，需要遍历T种tag，而对于每种tag又需要遍历前一个word的T种tag，因此对于N个word来说需要进行$$T^2N$$次遍历。

## 优点

- 将问题进行decompose，运用independence assumptions

## 实际运用

一般情况下，计算算法流程中的值，采用log形式将连乘转为累加。

# 生成模型 Vs. 判别模型

首先它们都属于概率模型，目标函数都是$$arg\max_Y P(Y\lvert X)$$

## 生成模型 (Generative Model)

不能直接求解出$$arg\max_Y P(Y\lvert X)$$，需要将其转化为$$arg\max_Y P (X\lvert Y)P(Y)$$，即求$$arg\max_Y P(X,Y)$$

- 如NB，HMM。

- 之所以称为**生成**

  - 我们假设所有特征是离散的，则有

    $$P(X\lvert Y)=\frac{Count(X,Y)}{Count(Y)}$$

    $$P(Y)=\frac{Count(Y)}{All}$$

    $$P(X\lvert Y)P(Y)=\frac{Count(X,Y)}{All}$$

  - 也就是通过已有数据，通过观测(X, Y)，生成{$$(X, Y_1),(X,Y_2),\dots,(X,Y_n)$$}等一系列组合情况，并获取它们的出现频率，也就是【观测(X, Y)，生成这一系列组合的生成概率】。因此说是个**生成过程**

  - 另外，我们可以**任意添加Y**的不同值，这样影响的只有生成组合数量以及对应的生成概率。并不会影响其他求解过程。

  

## 判别模型 (Discriminative model)

- models describe $$P(y\lvert X)$$ directly

- supports richer feature set, generally better accuracy when trained over large supervised datasets

直接通过观测值**X**，求出$$\forall y\in Y$$的$$P(y\lvert X)$$

- 如LR
- 之所以是**判别**，它是在已知特征X出现的情况下求解y发生的概率

- 另外，我们可以**任意添加特征**，由于特征是限定条件，我们可以通过添加特征来增强限定条件。

## Application

‣ named entity recognition (lecture 18), shallow parsing, 

‣ In other fields: DNA, protein sequences