# Hidden Markov Model (HMM)

## Motivation

- 将整个句子的tags看作一个序列，作为模型的输出
- 但我们将model的过程设计为一个word-level tagging 的task
- 也就the idea of **sequence labelling**

## Goal

寻找sentence $w$的最佳tag sequence $\hat{t}$
$$
\hat{t} = arg\max_t P(t|w)
$$
由于我们很难直接计算$P(t|w)$，因为对于每个$$P(t_i|w_i)$$,我们不可能假设这个word的tag只和这个word有关这么Naive的假设【因为**w**是个序列，明显这个word的tag和它前面的词和后面的词的tags都有关系】，因此我们尝试运用贝叶斯定律对其进行拆分化简
$$
\hat{t}= arg\max_t\frac{P(w|t)P(t)}{P(w)} \\= arg \max_t P(w|t)P(t)
$$
至此，我们可以做出以下合理假设

- 假设每个word只与其tag有关，与其他word相互独立，则有$$P(w|t)=\prod_{i=1}^nP(w_i|t_i)$$

- 假设每个word的tag只与前一个word的tag有关 ，则有$P(t)=\prod_{i=1}^nP(t_i|t_{i-1})$

$$
\hat{t}=arg\max_t\prod_{i=1}^nP(w_i|t_i)\prod_{i=1}^nP(t_i|t_{i-1})~~~~~~~~~(1)
$$



## Assumptions

### 输出值独立 Output independence

假设输出值只与隐变量 (i.e., tag)有关

### 马尔可夫假设 Markov assumption

假设为一阶的情况：当前state / tag只取决于前一个previous state

## 参数 Parameters

观察目标函数，我们可以看作是有两部分组成，即word与tag关系$P(w|t)$以及tag之间关系$P(t)$，也就是$P(w_i|t_i)$和$P(t_i|t_{i-1})$。

### Emission (*Observation/O*) probabilities $P(w_i|t_i$)

我们用Emission Matrix来表示

<img src="/assets/img/post_img/28.png" style="zoom:50%;" />

- e.g., 单元格**[NNP, Janet]**表示$$P(Janet|NNP)$$

### Transition (*A*) probabilities $P(t_i|t_{i-1})$

我们用Transition Matrix来表示

<img src="/assets/img/post_img/29.png" style="zoom:50%;" />

- e.g., 单元格**[NNP, MD]**表示$$P(MD|NNP)$$

  

## Training

- 对于**Emission probabilities** ，通过counting word frequencies under the specific tag type
  - e.g., $$P(like|\text{VB}) = \frac{count(\text{VB},like)}{count(\text{VB})}$$

- 对于**Transition probabilities**，类似unigram language model
  - e.g., $$P(NN|DT)=\frac{count(DT, NN)}{count(DT)}$$
  - 我们默认用"<s>"来表示第一个word的tag

## Prediction

通过考虑所有的tag combinations，然后选择使得目标值最大的tag sequence/combination

- 有一种错误的想法，对每个word，选择一个tag maximises $$P(w_i|t|i)P(t_i|t_{i-1})$$

  - 这种方法只能做到$$arg\max_{t_i} P(w_i|t_i)P(t_i|t_{i-1})$$ for $$\forall i\in[0, n)$$
  - 这等价于local classifier容易造成error propagation

- 假设sentence length = N, tag size = T，获取所有tag combinations的复杂度是多少$$A_T^N=O(T^N)$$

  
