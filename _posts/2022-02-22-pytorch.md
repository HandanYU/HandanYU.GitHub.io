---
layout: post
title: pytorch
summary: 。
featured-img: 损失函数
language: chinese 
category: deep learning
---

<a name='view'/>

# view
用于改变tensor形状
```python
# 将x从size: (1,8)转为size: (2,4)
## 方法一：固定列数和行数
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(2,4) 
>>> x.size()
output: torch.Size([2, 4])

## 方法二：固定列数，行数自适应
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(-1,4) # -1表示自适应调整该维度大小, 当确定需要将列数改为4的时候，行数根据数据大小和列数4的关系自动计算
x.size()
output: torch.Size([2, 4])

## 方法三：固定行数，列数自适应
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(2,-1)

# 将x从size: (1,8)转为size: (1, 2,4)
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(1,2,4) 
>>> x.size()
output: torch.Size([1, 2, 4])
```

<a name='unsqueeze & squeeze'/>

# unsqueeze & squeeze
## unsqueeze
用于添加维度
```python
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(2,4)
>>> x.size()
output: torch.Size([2, 4])

>>> torch.Size([2, 4])
>>> x = x.unsqueeze(1) # 在第二维上另添加一个维度
>>> x.size()
output: torch.Size([2, 1, 4])

>>> torch.Size([2, 1, 4])
>>> x = x.unsqueeze(0) # 在第一维上另添加一个维度
>>> x.size()
output: torch.Size([1, 2, 4])

>>> x = x.unsqueeze(-1) # 在最后一个维度上另添加一个维度
>>> x.size()
>>> torch.Size([2, 4, 1])
output: torch.Size([2, 4, 1])
```

## squeeze
用于删除维度（当该维度大小为1的时候）
```python
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(1,2,4)
>>> x.size()
output: torch.Size([1,2, 4])

>>> x = x.squeeze(0) # 删除第一维
>>> x.size()
output: torch.Size([2, 4])

>>> x = x.squeeze(-1) # 删除最后一维 
>>> x.size()
output: torch.Size([1, 2, 4]) # 可以看出并没有成功删除最后一维，因为该维度大小为4.
```

<a name='torch.cat'/>

# torch.cat
将多个tensor按行或者按列进行拼接
```python
# dim = 0: 按行拼接
>>> x = torch.tensor([[1,2,3],[4,5,6]])
>>> cat_x_0 = torch.cat([x, x, x], dim = 0) 
>>> cat_x_0
output: tensor([[1, 2, 3],
        [4, 5, 6],
        [1, 2, 3],
        [4, 5, 6],
        [1, 2, 3],

        [4, 5, 6]])

# dim = 1: 按列拼接
>>> cat_x_1 = torch.cat([x, x, x], dim = 1) 
>>> cat_x_1
output: tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],
        [4, 5, 6, 4, 5, 6, 4, 5, 6]])

```

<a name='model.train() & model.eval()'/>

# model.train() & model.eval()
## model.train()
用于训练模型中，启用BN和Dropout。
- 为了**防止过拟合**，每次只选择部分信息进行训练，也就是dropout（丢弃）一些神经元。（e.g., 若假设Dropout层的p=0.4，那么1000个神经元经过Dropout层,约有1000*0.4=400个神经元的值会被设置为0，也就相当于将这400个神经元丢弃了。）
- 为了**加快网络收敛**，并且由于是一个batch一个batch投递给模型进行学习的，因此每个batch的均值和方差都不大一样，需要运用BN来进行计算，且在所有batch结束后，也需要用BN对每个batch上的均值和方差做指数平均，来得到整个样本上的均值和方差的近似值。

## model.eval()
用于模型预测中，不启用BN和Dropout
- 在测试模型，运用训练好的模型进行预测的时候，为了得到最可靠的结果，需要运用的是整个训练好的模型，因此不需要使用Dropout
- 另外运用训练好的模型进行预测或进行测试模型的时候，是使用已经训练好的参数，因此不需要BN来计算样本均值和方差


# nn.ModuleList & nn.Sequential


# running loss & loss
## running loss

当在一个epoch下分了多个batch，此时需要用到running loss表示一个epoch中到目前batch得到的loss均值。

令有m个batch，则对于一个epoch中进行到第j个batch时候的running loss可表示为

$$\text{running loss}_j = \frac{\sum_i^j \text{Loss}_i}{j} = \text{running loss}_{j-1}+\frac{\text{Loss}_j-\text{running loss}_{j-1}}{j}$$

整个epoch得到的loss可以表示为

$$\text{loss} = \frac{\sum_i^m \text{Loss}_i}{m} = \text{running loss}_m$$

## loss

在整个数据集上的loss

<a name='BCELoss() & BCEWithLogitsLoss()'>

# BCELoss() & BCEWithLogitsLoss()
来个等式就一目了然了

BCEWithLogitsLoss() = sigmoid() + BCELoss() 

函数BCELoss(pred, target)要求pred中所有值都是在区间[0,1]内，因此一般需要对预测得到的值进行sigmoid激活操作。而函数BCEWithLogitsLoss(pred, target)则对pred取值范围没有要求。
```python
>>> label = torch.Tensor([1, 1, 0])
>>> pred = torch.Tensor([2, 4, 0])
>>> pred_sig = torch.sigmoid(pred)
>>> loss = nn.BCELoss()
>>> print(loss(pred_sig, label))
output: tensor(0.2794)

# 也可以写成
>>> label = torch.Tensor([1, 1, 0])
>>> pred = torch.Tensor([2, 4, 0])
>>> loss = nn.BCEWithLogitsLoss()
>>> print(loss(pred, label))
output: tensor(0.2794)
```
