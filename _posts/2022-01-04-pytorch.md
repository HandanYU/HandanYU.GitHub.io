---
layout: post
title: PyTorch
featured-img: PyTorch
summary: 运用PyTorch自定义网络层并搭建网络结构
---

# Install PyTorch

# Build self-design Net

## Build self-design Layer

- design $$y = w\times\sqrt{x^2+b}$$

```python
class MyLayer(nn.Module):
    # 初始化：输入输出单元数，权重，偏置
    def __init__(self,in_channel,out_channel,bias=True): # define parameters
        super(MyLayer,self).__init__()#调用父类的构造函数
        self.in_channel=in_channel
        self.out_channel=out_channel
        self.weight=nn.Parameter(torch.Tensor(in_channel,out_channel))#随机产生一个in_channel*out_channel的矩阵权重。又由于权重是要不断训练的，需要将其绑定为一个可以训练的参数于是需要使用Parameter
        if bias:
            self.bias=nn.Parameter(torch.Tensor(in_channel).view(1,-1))#注意⚠️这边的b是自定义层中自带的b，而不是神经网络中的卷积核的偏置，因此维数需要和输入单元数一样
        else:
            self.register_parameter('bias',None)#取消bias这个参数
    def forward(self,x):#前向传播计算
				############################
				# activation function design
				############################
        input_=torch.sqrt(torch.pow(x,2)+self.bias)#相当于卷积层中进行的激活
        output=torch.matmul(input_,self.weight)#矩阵乘法运算 1*in_channel in_channel*out_channel
        return output
```

## Build self-design Net

```python
in_channel1,out_channel1,out_channel2=5,3,1
class MyNet(nn.Module):
    def __init__(self): # define each layer
        super(MyNet,self).__init__()
        self.myLayer1=MyLayer(in_channel1,out_channel1)
    def forward(self,x): # connect each layer
        output=self.myLayer1(x)
        return output
```

# Define Loss Function

```python
loss_fn = torch.nn.CrossEntropyLoss() # multi-classification
loss_fn = torch.nn.HingeEmbeddingLoss() # SVM
loss_fn = torch.nn.MSELoss() # Regression
```

# Define Optimizer

 

```python
learning_rate = 1e-4

optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate)
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
optimizer = torch.optim.Adadelta(model.parameters(), lr = learning_rate)
```

# Train Model

- for each echo
    - step1: predict the labels through **forward**
    - step2: attain error (compare true labels and predicted labels). (loss_fn(y, y_pred))
    - step3: let the gradient as 0. (optimizer.zero_grad())
    - step4: backward the error
    - step5: use optimizer (optimizer.step())

```python
model = MyNet()
x = torch.randn(10,5)  #（10，5）
y = torch.randn(10,3) #（10，3）

for i in range(echos):
    # step1
    y_pred = model(x)
    # step2
    error = loss_fn(y_pred,y)
    # step3
    optimizer.zero_grad()
    # step4
    error.backward()
    # step5
    optimizer.step()
```

# Get Result

```python
output = model(x)
pre = torch.max(output,1)[1]
pred_y = pre.data.numpy() # data.numpy(): transform tensor to array
target_y = y.data.numpy()
```

# BP Net

## Construct BP Net

```python
import torch.nn.functional as fun
class BP(nn.Module):
    def __init__(self,n_feature,n_hidden,n_output):
        super(BP,self).__init__()
        self.hidden = torch.nn.Linear(n_feature,n_hidden)
        self.out = torch.nn.Linear(n_hidden,n_output)
    def forward(self,x):
        o = fun.sigmoid(self.hidden(x))
        output = self.out(o)
        return output
```

## Define optimizer and loss function

```python
model = BP(n_feature,n_hidden,n_output)
learning_rate = 0.02
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
loss_fn = torch.nn.CrossEntropyLoss()
```

## Train model

```python
for i in range(echos):
    output = model(x)
    error = loss_fn(output,y)
    optimizer.zero_grad()
    error.backward()
    optimizer.step()
```

## Get Result

```python
output = model(x)
pre = torch.max(output,1)[1]
pred_y = pre.data.numpy() # data.numpy(): transform tensor to array
target_y = y.data.numpy()
```