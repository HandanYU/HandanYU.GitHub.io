---
layout: post
title: 任意搜索问题
summary: 本章主要总结了面试中涉及到的机器学习相关的一些问题。
featured-img: machine learning
language: chinese 
category: AI
---


# 1. AI 中的自治行为（Autonomous Behavior）

## 1.1 Key Problem

Select the action to do next (**control problem**)

## 1.2 General Problem Solving

动机：寻找one program能够解决all problems的方法

表示形式：write $$X\in\{algorithms\}$$ for all $$Y\in\{'problems'\}$$ s.t. $$X$$ solves $$Y$$

需要解决的问题和突破口：What is '**problem**'? & what doest it mean to '**solve**' it?

![image-36](/assets/img/post_img/36.png)
## 1.3 Approaches

### 1.3.1 Programming-based

control by programmer (hand)

**Pros**：领域知识（domain-knowledge）可以很容易表达。
**Cons**：无法处理程序员没有预见到的情况，因此无法实现general

### 1.3.2 Learning-based

Learn control from **experience** or through **simulation**

- Unsupervised
  - i.e. RL (Reinforcement Learning)
  - Add Panelties & give Reward 
- Supervised
  - i.e. Classification
  - Classify Good or Bad action根据teacher给的info
- 演化 Evolutionary
  - 类似集成学习
  - 从possible controllers中，不断尝试测试它们，选择一个最佳的几个，通过多次迭代变异（mutate）并重组（recombine）它们，从而最终保留下最佳的controller.
- **Pros**: 不需要much knowledge
- **Cons**:实际情况下，很难知道应该学习哪些特征，并且这学习过程很慢

### 1.3.3 Model-based

- problem by hand. Specify model for problem (includes actions, initial situation, goals. Sensors) 
- derive control automatically
- Model-based 的智能行为称为 AI 中的 **规划（Planning）**。
- **Pros**: 
  - Powerful：泛化性强
  - Quick：快速搭建原型 （using prototyping rather than programming code like C++ code）
  - Flexible & Clear：可以修改 / 保留描述。
  - Intelligent & domain-independent：可以自动决定如何有效对一个复杂问题进行求解。
- **Cons**：
  - 需要一个model
  - 计算困难
  - Efficiency loos：由于domain-independent，没有domain-specific knowledge
- Trade-off：**automatic & general** Vs. **manualwork but effective**

# 2. Models

|                          | Classical Planning                                           | Conformant Planning                                          | Planning with MPDs                                           | Planning with POMDPs                                         |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| state space              | finite and discrete                                          | finite and discrete                                          | finite and discrete                                          | finite and discrete                                          |
| initial state            | a **know** initial state $s_0\in S$                          | **possible** initial state**s** $S_0\subseteq S$             | a **know** initial state $s_0\in S$                          | initial belief state $b_0$                                   |
| goal state               | $S_G\subseteq S$                                             | $S_G\subseteq S$                                             | $S_G\subseteq S$                                             | final belief state $b_f$                                     |
| actions                  | $\forall s \in S, A(s)\subseteq A$                           | $\forall s \in S, A(s)\subseteq A$                           | $\forall s \in S, A(s)\subseteq A$                           | $\forall s \in S, A(s)\subseteq A$                           |
| transition               | **Deterministic** transition **function** $F(a,s)\subseteq S$ for $a\in A(s)$ | **non-deterministic** transition **function**$F(a,s)\subseteq S$ for $a\in A(s)$ | **transition probabilities** $P_a(s'|s)$ for $s\in S$ and $a\in A(s)$ | **transition probabilities** $P_a(s'|s)$ for $s\in S$ and $a\in A(s)$ |
| action cost              | **positive** $c(a, s)$                                       | **uniform** $c(a, s)$                                        | **positive** $c(a, s)$                                       |                                                              |
| solution                 | **action sequence** mapping initial state into goal states   | **action sequence** that **any possible initial state and transition** must achieve the goal | **functions/policies** mapping states into actions           | **policies** mapping belief state into actions               |
| how to optimize solution | Minimize **expected cost** to goal                           |                                                              | Minimize **expected cost** to goal                           | Minimize **expected cost **from initial belief $b_0$ to goal |



## 2.1 一致规划 Conformant Planning

**不确定的（uncertainty）但no feedback**

在「」中我们提到了经典规划（状态模型），与其类似有一种规划叫一致规划（Conformant Planning)，

区别在于**经典规划**的初始状态是已知的，然而**一致规划**的初始状态是未知的不确定的，只能够确定possible initial state。【在表示形式上**经典规划**的初始状态是一个元素，而**一致规划**的初始状态是一个**集合**】。

另外一个随之而来的区别就在于**transition function**，**经典规划**的transition function是**确定的（deterministic）**，但**一致规划**的transition function是**不确定的（non-deterministic）**

对于solution，仍然是一个**action sequence**，但**一致规划**得到的solution是要满足每个possible initial state和transition都要能达到goal。

## 2.2 基于Markov Decision Processes的规划 (MPDs)

**可观测的（observable）概率（probabilistic）状态模型**

与之前两种规划的主要区别在于**transition function**，在该模型下根据Markov理论提供的是**transition probabilities**，也就是执行agent在执行$s\stackrel{a}{\longrightarrow}s'$操作的时候概率是$P_a(s'|s)$

## 2.3 部分可观测Markov Decision Processes（POMDPs）

**部分可观测（partially observable）** 的 **概率（probabilities）** 状态模型，

在Markove Decision Process的基础上，我们又引入了**信念状态（belief state）**。也就是说，agent 可能并不知道自己的初始状态或者目标状态，只有关于它们的信念，它通过传感器得到有关周围环境的信息，并在此基础上对信念状态作出调整。

**MPDs**和**POMDPs**存在“维度诅咒” 问题，也就是复杂度会随着number of state成指数级增长。**POMDPs**则更复杂，复杂度随着number of state成双指数增长。

## 2.4 一个例子区别四种规划
![image-37](/assets/img/post_img/37.png)
如图，agent 计划从A到G

- 如果行动是确定性（**action deterministic**）的，并且初始位置已知 （**initial location know**），那么这是一个 **经典规划问题**。
- 如果行动是随机性的（**actions stochastic**），并且位置是可观测（**location observable**）的，那么这是一个 **MDP 问题**。
- 如果行动是随机性的（**actions stochastic**），并且位置是部分可观测的（**location partially observable**），那么这是一个 **POMDP 问题**

# 3. Language

- Models被通过不同的合适的**planning lanuages**进行描述
- states用来解释该语言

## 3.1 Strips

- 最早的规划语言(50年代)，是**Classical Planning**的basic language

  

### 3.1.1 表示形式

**prolem**: $$P=<F, O, I, G>$$

- F （**Fluents**）: 所有atoms (boolean vars)
  - 每个位置可以用boolean来表示，表示agent是否在该位置
- O: 所有operators (actions)
  - **Add**: $$Add(o)\subseteq F$$ 行动执行后结果为真。
  - **Delete**: $$Del(o)\subseteq F$$ 行动执行后结果为假。
  - **Precondition**: $$Pre(o)\subseteq F$$ 为了行动能够执行，必须为真。
- $I$: $I\subseteq F$，initial situation
- G: $G\subseteq F$，goal situation

> 例：假如 Chris 希望从房间左下角移动到房间右上角，我们可以用Strips语言这样描述

我们会添加一个 fluent（“*Chris 站在房间左下角*”），而当 Chris 移动之后，之前的 fluent 会被删除。而要执行这一行动，前提条件是之前的 “*Chris 站在房间左下角*” 这一陈述为真。

### 3.1.2 STRIPS 语义学（Semantics）

从Language到Models， Strips problems决定了state model $$S(P)$$

- States $$s\in S$$ 是变式 F 中 原子的某种集合。且$$S=2^F$$
- initial state $$s_0$$就是$$I$$
- goal states $S_G$ 相当于$$G\subseteq S$$
- 对于action $$a\in A(s)$$相当于$$o\in O$$，s.t. $$Prec(a)\subseteq S$$
- next state $$s'=s-Del(a)+Add(a)$$
- action costs $$c(a,s)===1$$

<u>P的（最优）**解** 是 S(P) 的（最优）**解**</u> 

### 3.1.3 用Strips表示TSP

let *C* be the set of cities, *E* be the set of edges.
$$
P = <F, O, I, G>\\
F = \{at(x), visited(x)|x\in C\}\\
I = \{at(Sydney), visited(Sydney)\}\\
G=\{visited(x)|x\in C\}\\
O = \{move(x,x'): pre=\{at(x)\}, add=\{at(x'),visited(x')\}, del=\{at(x)\}|(x,x')\in E\}
$$

### 3.1.4 用Strips表示Blocks-World

$$
F:=\{on(x, y), onTable(x), clear(x), holding(x), armFree\}\\
\\
O:= \\ 
\{stack(x,y):= \\ 
- prec:= \{holding(x), clear(y)\} \\
- add:= \{clear(x), on(x,y), armFree\} \\
- del:= \{clear(y), holding(x)\}\\
|  x, y \in B \}\\
\cup \{\\
unstack(x,y):= \\
- prec:= \{on(x,y), clear(x), armFree\} \\
- add:= \{holding(x), clear(y)\} \\
- del:= \{clear(x), on(x,y), armFree\} \\
|  x, y \in B \}\\
\}\\
\cup \{\\
putdown(x):= \\
- prec:= \{holding(x) \} \\
- add:= \{clear(x), onTable(x), armFree\} \\
- del:= \{holding(x)\} \\
|  x \in B \}\\
\}\\
\cup \{\\
pickup(x):= \\ 
- prec:= \{onTable(x), clear(x), armFree\} \\
- add:= \{holding(x)\} \\
- del:= \{clear(x), onTable(x), armFree\} \\
|  x \in B \}\\
\}\\
\\
I:=\{on(A, C), onTable(C), onTable(B), clear(A), clear(B), armFree\}\\
\\
G:=\{on(A, B), on(B, C)\}

where, $B$ is the set of blocks
$$

### 3.1.5 用Strips表示$$m\times m$$ Manhattan Grid

Consider a $$m\times m$$ manhattan grid, and a set of coordinates $$V$$ to visit in any order, and a set of inaccessible coordinates (walls) $$W$$. Using Strips, a state-space model can be represented as $$P=<F, O, I, G>$$

- $$F = \{at(x,y), visited(x,y)| x,y \in \{0,1,\dots, m-1\}\}$$

- $$O=\{move(x,y,x',y'):$$

$$~~~~~~~~~~~~~~~~Pre:\{at(x,y)\}$$

$$~~~~~~~~~~~~~~~~Add:\{at(x',y'), visited(x',y')\}$$

$$~~~~~~~~~~~~~~~~Del:\{at(x,y)\}$$

$$~~~~~~~~~~~~~~~~\lvert \text{for each }(x,y), (x',y') \text{ are adjusted}\}$$

- $$I=\{at(0,0), visited(0,0)\}$$

- $$G=\{visited(x,y)\lvert (x,y)\in V\}$$





## 3.2 PDDL

- **规划域定义语言(PDDL)**是基于 Strips 语言的一种扩展。通过从一个**对象（ objects）**的有限集合中将**对象变量（object variables）**实例化
- **不是**一种命题语言

- 行动图式（Action schemas）：通过对象实现参数化。
- 谓词（Predicates）：通过对象完成实例化。

### 3.2.1 PDDL planning task

- 域文件 domain file + 问题文件 problem file
- 域文件 domain file：谓词（Predicates） + Operators/Actions.
  - Actions: precondition, effect and(add), effect not(del)
  - 每个基准域（benchmark domain）都有一个域文件。

- 问题文件： 对象（objects）+ 初始状态（ initial state）+  目标状态（the goal state）.
- 我们可以有一个通用的域文件，然后对于不同问题采用不同的问题文件来编码。

### 3.2.2 PDDL Pros

- specifcation: 简洁的模型描述。
  在前面的砖块世界的例子中，我们仅用非常短的 PDDL 文件就可以定义 domain，即便是很复杂的问题，我们也可以很容易地将其编码为 domain 文件来表示，这将带来诸多好处：因为否则的话，我们将不得不编写每一个单独的 **对象（object）**和每一条单独的 **规则（rule）**，PDDL 语言为我们提供了一种更灵活的选择。
- Computation: 揭示有用的启发式信息（结构）
  另一个问题是计算。语言让我们得以描述问题，使得程序可以理解问题的结构，并且在结构之外加入启发式信息。例如，在吃豆人问题中，我们可以在 PDDL 规划器之外加入启发式搜索来引导搜索树

### 3.2.3 用PDDL表示TSP

- Domain.pddl

```she
(define (domain tsp)
(:requirements :scripts :typing)
(:types node)

;; ? refers to parameters' name
;; - points out the type of parameters

(:predicates
	(at ?pos - node)
	(connected ?start ?end - node)
	(visited ?end - node)

(:action move
	:parameters (?start ?end - node)
	:precondition (and (at ?start)
		(connected ?start ?end)
	:effect (and (at ?end)
		(visited ?end) 
	(not (at ?start)))))
```

- Problem.pddl

```shell
(define (problem tsp)
(:domain tsp)
(:objects Sydney Adelade Brisbane Perth Darwin - node)

;; Define the initial situation
(:init (connected Sydney Brisbane)
	(connected Brisbane Sydney)
	....
	(visited Sydney)
	(at Sydney)

(:goal
	(and (at Sydney)
	(visited Sydney)
	(visited Adelade)
	(visited Brisbane)
	(visited Perth)
	(visited Darwin))))
```



## 3.3 砖块世界The Blocksworld

有一个钩爪和一些砖块，钩爪可以用来拾取和放下砖块，希望通过钩爪将这些砖块堆叠成某个目标状态。
![image-38](/assets/img/post_img/38.png)

### 3.3.1 用Strips表示砖块世界

- 命题（Propositions）

  - on(x, y)：砖块可以堆叠在另一个砖块上方，例如左图中砖块 D 在砖块 C 上方。
  - onTable(x)：砖块可以位于桌面上方，例如左图中砖块 E、A、B、C 都在桌面上方。
  - clear(x)：砖块上方没有物体，例如左图中的砖块 E、A、B、D。
  - holding(x)：表示钩爪正在抓取的砖块。
  - armEmpty()：钩爪是否为空闲状态。

- 初始状态

  - {onTable(E), 

- 目标状态

  - {on(E, C), on(C, E), on(D, B)}

- 动作

  - stack(x, y)：实现一个堆叠状态。
    - Pre(stack(x, y)): {holding(x), clear(y)}
    - Add(stack(x, y)): {on(x, y), armEmpty(), clear(x)}
    - Del(stack(x, y)): {holding(x), clear(y)}

  <u>一般情况下删除列表和前置条件列表相同</u>

### 3.3.2 用PDDL表示砖块世界

- 域文件

```shell
(deﬁne (domain blocksworld)
(:predicates (clear ?x) (holding ?x) (on ?x ?y)
             (on-table ?x) (arm-empty)) 
(:action stack
 :parameters (?x ?y)
 :precondition (and(clear ?y) (holding ?x)) # Pre
 :effect (and (arm-empty) (on ?x ?y)    # :effect 指明action
              (not (clear ?y)) (not (holding ?x))) ## not 表示Del操作
)
...
```

- 问题文件

```shell
(deﬁne (problem bw-abcde)
(:domain blocksworld)
(:objects a b c d e)
(:init (on-table a) (clear a)
       (on-table b) (clear b)
       (on-table e) (clear e)
       (on-table c) (on d c) (clear d)
       (arm-empty))
(:goal (and (on e c) (on c a) (on b d))))

```

## 3.4 Strips Vs. PDDL

- Strips always represents **specific operation**, while PDDL are more abstract
- Any STRIPS planning problem can be modelled using PDDL

# 4. 复杂度

## 4.1 Algorithmic Problems in Planning

- 这两者不相交
- 一般情况下Satisficing Planning会比Optimal Planning 容易
- 解决这类问题的Program被称为：planners, planning systems, or planning tools.

### 4.1.1 Satisficing Planning

- 输出：关于问题P的一个规划 Or 不可解

### 4.1.2 Optimal Planning

- 输出：关于问题P的一个最优规划 Or 不可解

## 4.2 Decision Problems in Planning

- PlanEx 和 PlanLen 是 **PSPACE-完全** 问题。

- 通常，两者具有相同的复杂度。
- 在特定的应用程序中，有界长度规划的存在性问题通常要比规划存在性问题更加困难。
- 这在许多 IPC 基准域中都会发生：PlanLen 是NP-完全问题，而 PlanEx 是P问题。
  - 例如：砖块世界 vs. 物流问题

### 4.2.1 PlanEx

- 给定一个规划任务 P，判断是否存在一个关于 P 的规划。
- 对应于Satisficing Planning

### 4.2.2 PlanLen

- 给定一个规划任务 P 和一个整数 B，判断是否存在一个关于 P 的最大长度为 B 的规划。

- 对应于Optimal Planning

## 4.3  NP & PSPACE

- $$NP\subseteq PSPACE$$

# 5. 计算方法

## 5.1 求解Strips规划问题的方法

### 5.1.1 显式搜索 explicit search

- 直接显式搜索state model $$S(P)$$

- 效率不高

### 5.1.2 分解 decomposition

- 将规划问题分解为一些启发式的子问题，如果我们可以找到这些子问题的解，我们可以将其组合，得到原规划问题的解。
- 但因为规划问题的状态空间通常很大，即使采用很好的启发式算法，搜索空间依然很大。

## 5.2 经典规划(classial Planning)的计算方法

- **通用问题求解器** **General Problem Solver (GPS) and Strips**
  - 均值分析、分解、回归……
- **偏序** **Partial Order (POCL) Planning**
  - 处理任何开放的子目标、解决威胁
- **图规划** **Graphplan**
  - 建立包含一定长度内的所有可能的 **并行** 规划的图；然后通过从目标向后搜索整个图来提取规划
- **可满足性规划** **Planning as Satisfiability, SATPlan**
  - 将规划问题映射为 SAT 问题；使用最前沿的 SAT 求解器。
- **启发式搜索规划** **Heuristic Search Planning** 
  - 从问题 P 中提取的具有启发函数 h 的搜索状态空间 S(P)。
- **模型检查规划** **Model Checking Planning**
  - 使用 “符号” 宽度优先搜索来搜索状态空间 S(P)，其中状态集由 BDD 实现的公式表示。

## 5.3 经典规划中的最新技术

- 经验主义方法
  - 标准 PDDL 语言
  - 提供规划器和基准；比赛
  - 专注于性能和可扩展性
- 不同的构想和想法
  - **启发式搜索** 规划
  - **SAT** 规划
  - **其他**：局部搜索（LPG），蒙特卡洛搜索（Arvand），…


