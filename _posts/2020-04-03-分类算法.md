---
layout: post
title: 分类算法
summary: 本章讲述了机器学习中经典的几种分类算法，包括逻辑回归，朴素贝叶斯，SVM，决策树，感知机算法和LDA。
featured-img: 分类算法
---

##### Table of Contents  
- [逻辑回归](#逻辑回归)  
  - [Logist 函数](#Logist 函数)  
  - [目标函数](#目标函数)  
  - [最大似然求解Loss Function](#最大似然求解Loss Function)  
  - [运用梯度下降得到参数更新递推公式](#运用梯度下降得到参数更新递推公式)  
- [朴素贝叶斯](#朴素贝叶斯)  
  - [条件概率](#条件概率)  
  - [极大似然估计](#极大似然估计)  
- [SVM](#SVM)  
  - [目标](#目标) 
  - [支持向量](#支持向量)  
  - [最优化问题](#最优化问题)
  - [对偶问题](#对偶问题)
  - [求支撑向量](#求支撑向量)
  - [求分界线](#求分界线)
  - [运用梯度下降方法求解SVM](#运用梯度下降方法求解SVM)
- [决策树](#决策树)  
  - [决策树针对缺失数据的处理办法](#决策树针对缺失数据的处理办法)
  - [信息熵](#信息熵)
  - [决策树类型](#决策树类型)
    - [ID3](#ID3)
    - [C4.5](#C4.5)
    - [CART](#CART)
- [感知机算法](#感知机算法)
  - [目标](#目标)
  - [运用梯度下降进行求解](#运用梯度下降进行求解)
  - [具体做题步骤](#具体做题步骤) 
- [LDA](#LDA)
  - [主要思想](#主要思想)
  - [模型建立](#模型建立)
  - [二分类](#二分类)
  - [多分类](#多分类)

<a name="逻辑回归"/>

## 逻辑回归(LR)——P57

标准逻辑回归是结果为0，1的二分类算法。目标是求$$P(y_i=1\|x_i,w)$$，若其大于$$\frac{1}{2}$$，则预测分类结果为1，否则为0。

<a name="Logist 函数"/>

### Logist 函数

令$$p(x_i) = P(y_i=1\|x_i,w)$$，构建$$p(x_i)$$与$$w^Tx_i$$之间的关系式。

首先提出以下猜想

$$~~~~~~~~~~~~~p(x_i)=w^Tx_i？~~~~~~~~~~~\log p(x_i) = w^Tx_i$$？

由于$$0\leq p(x)\leq 1$$, $$w^Tx_i$$是无界的。于是我们说以上等式都是不成立的，因此又有了以下猜想

$$\log \frac{p(x_i)}{1-p(x_i)} = w^Tx_i$$

可以证明$$\text{odd} = \frac{p(x_i)}{1-p(x_i)}$$的范围是[0,$$\inf$$]，则等式成立。我们称其为logist函数。

则可以化简得到

$$p(x_i) = \frac{1}{1+e^{-w^Tx_i}}$$

可以类比Sigmoid函数发现，$$p(x_i)$$与Sigmoid函数一致。因此我们设$$p(x_i) = g(w^Tx_i)$$.

<a name="目标函数"/>

### 目标函数

$$ max_w ~~~~\prod_{i=1}^n P(y_i|x_i,w) $$

其中

$$ P(y_i|x_i,w) = P(y_i=1|x_i,w)^{y_i}P(y_i=0|x_i,w)^{1-y_i} $$

<a name="最大似然求解Loss Function"/>

### 最大似然求解Loss Function

令

$$~~~~~~~~~ J = \prod_{i=1}^n P(y_i|x_i,w) $$

$$ ~~~~~~~~~~~~~~~~~~~~~~~~= \prod_{i=1}^n g(w^Tx_i)^{y_i}(1-g(w^Tx_i))^{1-y_i} $$

$$~~~~~~~~~~~~~~~\log J = \sum_{i=1}^my_i\ln (g(w^Tx_i))+(1-y_i)\ln (1-g(w^Tx_i))$$

$$~~~~~~~~~~~~~~~~~~~~ \text{Loss} = - \log J$$

目标函数为

$$~~~~~~~~~~~~~~~~~~~~ \min_w - \log J(w)$$


其中$$g$$ 为sigmoid函数


<a name="运用梯度下降得到参数更新递推公式"/>

### 运用梯度下降得到参数更新递推公式

对Loss函数求导

$$\frac{\partial{-\log J}}{\partial{w}}=-\sum_{i=1}^n[y_i\frac{\frac{\partial{g(w^Tx_i)}}{\partial{w}}}{g(w^Tx_i)}+(1-y_i)\frac{\frac{-\partial{ g(w^Tx_i)}}{\partial{w}}}{1-g(w^Tx_i)}]$$

由于令$$z_i = w^Tx_i$$，则有

$$\frac{\partial{g(w^Tx_i)}}{\partial{w}} = \frac{\partial{g(z_i)}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}$$

又因为

$$\frac{\partial{g(z_i)}}{\partial{z_i}}=g(z_i)'=g(z_i)(1-g(z_i))$$

于是整合得

$$ \frac{\partial{-\log J}}{\partial{w}} = \sum_{i=1}^n(g{z_i})-y_i)x_i$$

根据梯度定义得到

$$w:=w-\alpha\sum_{i=1}^n(g{z_i})-y_i)$$


<a name="朴素贝叶斯"/>

## 朴素贝叶斯——P150

假设各属性变量之间相互独立，条件独立性假设不成立时，朴素贝叶斯分类器仍有可能产生最优贝叶斯分类器。

<a name="条件概率"/>

### 条件概率

$$
P(A|B)=\frac{P(AB)}{P(B)}
$$

<a name="极大似然估计"/>

### 极大似然估计

#### 离散型——分布函数

假设样本服从二项分布

目标为
$$
max_\theta~~~P(X|\theta)
$$
对于小概率事件来说：概率=频率

#### 连续型——密度函数

假设样本服从高斯分布

期望=样本均值，方差=样本方差

##### MLE证明最小二乘法

最小二乘法的本质是选择一个y拟合效果最好，也就是选择w,b使得$y_i$. 尽可能对，也就是取到$y_i$ 的概率尽可能大，又由于每个点是独立的即使得密度函数尽可能大，即

$$
max_{w,b}~~~~~~~~~~J(w,b)=\prod_{i=1}^mP(y_i|w,b)
$$

而由于假设偏差$$\epsilon_i$$ ~$$N(0,\sigma^2)$$ ,则$$y_i$$~$$N(w^Tx_i+b,\sigma^2)$$ ,则

$$
P(y_|w,b)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i-w^Tx_i-b)^2}{2\sigma^2})
$$

则对$$J(w,b)$$取2对数，得到

$$
log J(w,b)=\frac{-1}{\sqrt{2\pi}\sigma}\sum_i(y_i-w^Tx_i-b)^2
$$

则目标函数相当于

$$
min_{w,b}~~~~~~~~\sum_i(y_i-w^Tx_i-b)^2
$$

##### 例子：

![图片pic1](/assets/img/post_img/2.png)

由于$$\epsilon_i$$ ~$$N(0,1)$$,则$$y_i$$ ~$$N(exp(wx_i),1)$$ ，则

$$
P(y_i|w,x_i)=\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_i-exp(wx_i))^2}{2})
$$

则极大似然函数J为

$$
J(w)=\prod_{i=1}^m P(y_i|w,x_i)\\
=(\frac{1}{\sqrt{2\pi}})^m\prod_{i=1}^mexp(-\frac{(y_i-exp(wx_i))^2}{2})
$$

则求对数似然函数log J

$$
log J(w)=D\cdot  \frac{1}{2}\sum_{=1}^m(y_i-exp(wx_i))^2
$$

则对对数似然函数求导得

$$
\frac{\partial logJ}{\partial w}=-\sum_{i=1}^mx_iexp(wx_i)(y_i-exp(wx_i))
$$

再令对数=0求解w

### 贝叶斯公式

$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$

### 朴素贝叶斯方法

$$
argmax_k P(Y=k)\prod_{j=1}^n P(X_j=x_j|Y=k)
$$

### 假设

- 每个特征相互独立
- 每个实例相互独立
- 训练集的分布和测试集的分布一致

### 平滑法

对于某个数据集，由于数据的稀疏性，我们考虑到对于某个特征X在训练集中没有出现，那么将会导致整个分类概率变为0，这将会导致分类变得非常不合理，所以为了解决零概率的问题，避免过拟合。需要通过平滑法来解决

#### Epsilon 平滑法

- 将$$P(x_m$$\|$$y)=0$$ 替换为$$P(x_m$$\|$$y)=\epsilon$$ , 其中$$\epsilon$$远小于$$\frac{1}{N}$​$(N是训练集样本数)

#### **Laplace** 平滑法/加法平滑

- 假定训练样本很大时，每个分量x的计数加1造成的估计概率变化可以忽略不计，但可以通过减少方差来方便有效的避免零概率问题。

- 在实际的使用中也经常使用加 $$\alpha$$（$$1\geq \alpha\geq 0$$）来代替简单加1。
  
  $$
  P(X_j=x_j|Y=k) = \frac{\alpha + \text{count}(Y=k,X_j=x_j)}{M\alpha + \text{count}(Y=k)}.
  $$

  其中M是$$X_j$$取值类别数。

- 当训练集很小的时候，概率变化剧烈；当数据集大的时候，变化不大。

- 减少方差(various)，增大偏差(bias)

### 算法流程

![image3](/assets/img/post_img/3.png)

<a name="SVM"/>

## SVM——P121

<a name="目标"/>

### 目标

寻找最大边缘超平面，即使得支持向量距离超平面距离尽可能大

<a name="支持向量"/>

### 支持向量

样本中距离超平面最近的一些点，SVM的决策边界完全由支持向量决定，因此当将能够被正确分类且远离决策边界的样本点加入到训练数据中，也不会影响改变SVM原来确定的决策边界。

<a name="最优化问题"/>

### 最优化问题

最大化两间隔边界之间的距离

$$
max_w~~~~~~~\frac{2}{||w||}\\\Rightarrow
min_w~~~\frac{1}{2}||w||\\
s.t. ~~~~~y_i(w^Tx_i+b)\geq 1
$$

<a name="对偶问题"/>

### 对偶问题

$$
max_{\lambda_i}~~~~~~~~~  f(\lambda_i)\\=>
max_{\lambda_i}~~-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\lambda_i\lambda_jy_iy_j(x_i.x_j)+\sum_{i=1}^n\lambda_i\\
s.t.~~~~~\sum_{i=1}^n\lambda_iy_i=0,~~~\lambda_i\geq0
$$

<a name="求支撑向量"/>

### 求支撑向量

求$$\frac{\partial f(\lambda_i)}{\partial \lambda_i}=0$$ 得到$$\lambda_i$$ ，然后验证$$\sum_{i=1}^n \lambda_iy_i=0$$ ，如果满足则$$\lambda_i>0$$ 对应的样本就是支撑向量；否则分类讨论，即令其中一个$$\lambda_i=0$$ ，求解剩余的$$\lambda_j$$ 然后比较每次求得的$$f(\lambda_i)$$选择$$f(\lambda_i)$$ 最小时候对应的$$\lambda_i$$ ，且$$\lambda_i>0$$对应的样本就是支撑向量。

<a name="求分界线"/>

### 求分界线

#### 最优权重

$$
w^*=\sum_{i=1}^n\lambda_jy_ix_i
$$

 其中$$x_i$$是列向量

#### 截距项

通过将任意支撑向量X(+1)，代入$$\sum_{i=1}^K\sum_{j=1}^nw_ix_{ji}+b=1$$ 得到

或通过将任意支撑向量X(-1)，代入$$\sum_{i=1}^K\sum_{j=1}^nw_ix_{ji}+b=-1$$得到

#### 最优分界线

$$
\sum_{i=1}^K\sum_{j=1}^nw_ix_{ji}+b=0
$$

<a name="运用梯度下降方法求解SVM"/>

### 运用梯度下降方法求解SVM

#### 线性SVM

##### 改进的线性SVM目标函数——软间隔

$$
min~~~~~\frac{1}{2}||w||_2^2+C\#K
$$

其中C表示错分的惩罚力度，#K表示所分点个数，即$$K=\{i$$\|$$y_i(w^Tx_i+b)<1\}$$

相当于
$$
min~~~~~\frac{1}{2}||w||_2^2+C \sum_{i\in K}(1-y_i(w^Tx_i+b))
$$

##### 参数更新

$$
~~~~~~~~~~~~~~w:=w-\alpha(w-C\sum_{i\in K}y_ix_i)\\
b:=b+\alpha C\sum_{i\in K}y_i
$$

#### 非线性SVM

##### 非线性SVM目标函数

$$
min~~~~~\frac{1}{2}||w||_2^2+C \sum_{i\in K}(1-y_i(w^T\phi(x_i)))\\
K=\{i|y_i(w^T\phi(x_i))<1\},~~~~~~~w=\sum_{i=1}^n\lambda_iy_i\phi(x_i)
$$

令$$\alpha_i=\lambda_iy_i$$故化简为

$$
min~~~~~\frac{1}{2}\sum_{i,j}\alpha_i\alpha_j\phi(x_i)^T\phi(x_j)+C \sum_{i\in K}(1-y_i(\sum_j\alpha_j\phi(x_j)^T\phi(x_i)))\\
$$

再令$$K_{ij}=\phi(x_i)^T\phi(x_j)$$，则最终化简为

$$
min~~~~~\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jK_{ij}+C \sum_{i\in K}(1-y_i(\sum_j\alpha_jK_{ij}))\\
$$

##### 参数更新

$$
\alpha_i:=\alpha_i-\eta(\bar{K}\alpha -C\sum_{i\in K}y_i\bar{K_i})
$$

##### 模型分类结果

$$
\sum_j\alpha_jK_{ij}<0=>y_i=-1\\
\sum_j\alpha_jK_{ij}>0=>y_i=1
$$

#### 根据线性、非线性目标函数的区别以及C,高斯核的$$\sigma$$取值的性质，判断目标函数对应的分类结果图

1. C表示分错点的惩罚程度。当C越大的时候说明对分错点的惩罚程度就越大，也就是对错误点的容忍率越低，分错点就会越少，（也就是强行将分割面插在很近的两个异类点之间）这时候两支撑面之间的间隔就会变小，$$\|w\|$$ 会增大。但随着C趋向无穷，也就退回到了原始SVM，没有错误点可以容忍，此时$$\|w\|$$ 不会改变，间隔也不会变，支撑向量数量减少（由于在改进SVM中那些容错点（两支撑面之间的点）也是支持向量）。

1. $$\sigma$$越小，数据点越少，越容易造成过拟合 

![image-4-1](/assets/img/post_img/4-1.png)
![image-4-2](/assets/img/post_img/4-2.png)
![image-4-3](/assets/img/post_img/4-3.png)

首先发现1，2的目标函数表示的是线性SVM，而1中的C=0.1,2中的C=1，则说明2中的SVM分错点更少一点，则1对应的图为c，2对应的图为b。

其次发现3，4，5的目标函数都是非线性SVM，且3中选择的核函数是多项式，而4，5选择的是高斯核函数，则3对应的决策边界是二次曲线则图应该为b。进而，4中的$$\sigma^2=1$$,5中的$$\sigma^2=0.5$$，则5的拟合效果会更好曲线更弯曲且可能会过拟合因此5对应的e图，4对应的是a图

<a name="决策树"/>

## 决策树——P73

<a name="决策树针对缺失数据的处理办法"/>

### 决策树针对缺失数据的处理办法

1. 放弃含缺失值的样本，仅使用无缺失值的样本来进行学习
2. 根据此属性值已知的其他样本，来估计这个缺失的属性值
   - 赋给它当前结点所有样本中该属性最常见的值
   - 赋给它当前结点同类样本中该属性最常见的值
   - 为含缺失值属性的每个可能值赋予一个概率

<a name="信息熵"/>

### 信息熵

#### 范围 

####  [0,$$\log_2n$$]， 0->确定事件;$$\log_2n$$->均匀分布

#### 特征

Ent(x): 当x的取值越多越大

Ent(x,a): 当a的前提下，x的取值越多越大

<a name="决策树类型"/>

### 决策树类型

<a name="ID3"/>

#### ID3(基于信息增益)

$$
max_a~~~~~~Gain(D,a)=Ent(D)-Ent(D,a)
$$


<a name="C4.5"/>

#### C4.5(基于信息增益率)

$$
max_a~~~Gain\_ratio(D,a)=\frac{Gain(D,a)}{Ent(a)}
$$

<a name="CART"/>

#### CART(基于基尼系数)

$$
min_a~~~~~Gini(D,a)=\sum_{i=1}^mp(a_i)Gini(D_i)\\
Gini(D_i)=1-\sum_{k\in D_i} p(k)^2
$$

![image5](/assets/img/post_img/5.png)

##### 终止分支的条件

- Gain <= threshold
- 该节点上所有样本的类别相同
- 所有特征都已经用过了

##### 注意

- 信息增益不会<0
- 叶子节点不一定确保只有一个类别

<a name="感知机算法"/>

## 感知机算法

<a name="目标"/>

### 目标

使得错分点个数尽可能的少

$$
min ~~~~~\#K
$$

其中$$K=\{i$$\|$$y_i(w^Tx_i+b)<0\}$$，相当于使错分类样本到分界面距离之和最小，即

$$
min ~~~\sum_{i\in K}|w^Tx_i+b|\\
=>min ~~~\sum_{i\in K} -y_i(w^Tx_i+b)
$$

<a name="运用梯度下降进行求解"/>

### 运用梯度下降进行求解

每次随机选择一个错分点$$(x_i,y_i)$$进行参数更新

$$
w:=w+\alpha y_ix_i\\
b:=b+\alpha y_i
$$

<a name="具体做题步骤"/>

### 具体做题步骤

随机选取一个样本，若该样本是满足$$y_i(w^Tx_i+b)\leq0$$也就是错分点，就对参数进行更新，直到参数使得所有样本点都不是错分点为止。

## 神经网络DNN——P98

### 多层感知机算法（前馈神经网络）MLP

### BP算法

相当于优化过程，参数更新

$$
\delta^{(i)}=\delta^{(i+1)} {w^{(i+1)}}\cdot h(X^{(i)}).\\
w^{(i)}:=w^{(i)}-\alpha X^{(i)}\delta^{(i)}\\
b^{(i)}:=b^{(i)}-\alpha \delta^{(i)}
$$


<a name="LDA"/>

## 线性判别法LDA——P60

<a name="主要思想"/>

### 主要思想
需要找一条直线，希望各点投影在该直线上后，希望同一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。量化这两点感官，则需满足异类点的中心距离远，同类点的方差小.

<a name="模型建立"/>

### 模型建立
假设我们有数据集$$D=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}$$，其中任意样本$$x_i$$为n为向量，$$y_i\in\{0,1\}$$，我们定义
- $$X_i(i=0,1)$$为第$$i$$类样本的集合，
- $$\#X_i(i=0,1)$$为第$$i$$类样本的个数，
- $$a_i(i=0,1)$$为第$$i$$类样本的投影中心点（是一个向量）,
- $$\mu_i(i=0,1)$$为第$$i$$类样本的中心点
- $$S_i(i=0,1)$$为第$$i$$类样本的方差
- $$\Sigma_i(i=0,1)$$为第$$i$$类样本的协方差矩阵

根据投影的知识可得

点$$(x_{i1},x_{i2})$$在直线$$w_1x_1+w_2x_2=0$$上的投影相当于向量$$(x_{i1},x_{i2})$$在向量$$(w_1,w_2)$$上的投影，即为向量$$(x_{i1},x_{i2})$$与向量$$(w_1,w_2)$$的点积$$(x_i)^Tw$$

则有

$$
a_i=\frac{1}{\#X_i}\sum_{j\in X_i}(x_j)^Tw~~~~~~~~~~~

=(\frac{1}{\#X_i}\sum_{j\in X_i}(x_j)^T)w~~

=\mu_iw
~~~~~~~~~~(i=0,1)
$$

$$
S_i=\sum_{j\in X_i}((x_j)^Tw-a_i)^2~~~~~~~~~~~~~~~~~~~~~~

=\sum_{j\in X_i}(((x_j)^T-\mu_i)w)^2~~~~~~~~~~~~~~（1）

=\sum_{j\in X_i}((x_j-\mu_i)^Tw)^2~~~~~~~~~~~~~~~~~（2）

=\sum_{j\in X_i}w^T(x_j-\mu_i)(x_j-\mu_i^T)w~~~（3）

=w^T(\sum_{j\in X_i}(x_j-\mu_i)(x_j-\mu_i)^T)w~~~~~~~~~~

=w^T\Sigma_iw~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~(i=0,1)
$$


- 其中(1)由$$a_i$$的表达式代入得到
- (2)由于$$\mu_i$$是数，一个数点转置还是它本身
- (3)根据$$(x^Ty)^2=(x^Ty)(x^Ty)=(x^Ty)^T(x^Ty)=y^Txx^Ty$$其中$$x^Ty$$为一个数

根据主要思想，我们建立以下目标函数

由于我们希望\|$$a_1-a_2$$\|尽可能大，$$S_1+S_2$$尽可能小，即可建立目标函数

$$
由于\min~~ |a_1-a_2|=>\min ~~(a_1-a_2)^2

\max\limits_{w}\frac{(a_1-a_2)^2}{S_1+S_2}
$$

下面求解$$(a_1-a_2)^2$$和$$S_1+S_2$$

$$
(a_1-a_2)^2=((\mu_1-\mu_2)w)^2=w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw


S_1+S_2=w^T(\Sigma_1+\Sigma_2)w
$$

最终确立目标函数为

$$
\max\limits_w=\frac{w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw}{w^T(\Sigma_1+\Sigma_2)w}
$$

根据拉格朗日乘子法得到最优权重为

$$
w^*=(\Sigma_1+\Sigma_2)^{-1}(\mu_1-\mu_2)
$$

当$$\Sigma_1+\Sigma_2$$不可逆的时候采用$$w^*=(\Sigma_1+\Sigma_2+\lambda I)^{-1}(\mu_1-\mu_2)$$

<a name="二分类"/>

### 二分类

1. 计算每个类别点集的中心点$$\mu_i$$ 
2. 计算每个类别点集的协方差矩阵$$\Sigma_i$$
3. 计算类间散度矩阵$$S_w=\Sigma_1+\Sigma_2$$
4. 判断$$S_w$$是否可逆（在python中用np.linalg.det（）求解矩阵A的行列式\|A\|，如果行列式为0，不可逆，否则可逆。）
   -  若可逆，则$$w^*=S_w^{-1}(\mu_1-\mu_2)$$ 
   -  若不可逆，则$$w^*=(S_w+\lambda I)^{-1}(\mu_1-\mu_2)$$ 

<a name="多分类"/>

### 多分类

1. 计算整个样本点集的中心点$$\mu$$ 

2. 计算每个类别点集的中心点$$\mu_i$$

3. 计算每个类别点集的协方差矩阵$$\Sigma_i$$ 

4. 计算全局散度矩阵$$S_t=\sum_{i=1}^m (x_i-\mu)(x_i-\mu)^T$$ 

5. 计算类内散度矩阵$$S_w=\sum_{j=1}^k\Sigma_j$$

6. 计算类间散度矩阵$$S_b=\sum_{j=1}^k m_j(\mu_j-\mu)(\mu_j-\mu)^T$$

   其中$$m_j$$表示第$$j$$类的样本数

7. 计算$$S_w^{-1}S_b$$的特征根和特征向量，选取前k个



