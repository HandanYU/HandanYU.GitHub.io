---
layout: post
title: 聚类算法
summary: 本章提及了机器学习中典型的聚类算法，以及聚类模型的评价指标
featured-img: machine learning
language: chinese 
category: machine learning
---

## K-Means

### 模型

$$
min_{C_1,C_2,\mu_1,\mu_2}~~~\sum_{X_i\in C_1}||X_i-\mu_1||^2_2+\sum_{X_i\in C_2}||X_i-\mu_2||^2_2
$$

相当于$$X_i$$是服从协方差为单位阵的高斯分布，是一种特殊的GMM。

### 求解

$$
Step1:已知\mu_1^k,\mu_2^k，求解C_1^{k+1},C_2^{k+1}\\
C_1^{k+1},C_2^{k+1}=argmin_{C_1,C_2}~~F(C_1,C_2,\mu_1^k,\mu_2^k)\\
$$

$$
Step2:已知C_1^{k},C_2^{k}，求解\mu_1^{k+1},\mu_2^{k+1}\\
\mu_1^{k+1},\mu_2^{k+1}=argmin_{C_1,C_2}~~F(C_1^k,C_2^k,\mu_1,\mu_2)\\
$$

重复迭代Step1,2

<a name="GMM"/>

## GMM——P206

### 模型

$$
max_{p,\mu_1,\mu_2,\Sigma_1,\Sigma_2}~~~\prod_{i=1}^m[\sum_{k=1}^Kp_k\frac{1}{\sqrt{2\pi}|\Sigma_k|}exp(-\frac{(X_i-\mu_k)^T\Sigma_k(X_i-\mu_k)}{2})]
$$

### 求解

- E-Step: 已知$$\mu$$,$$\Sigma$$,$$p$$，求解$i$样本被聚类到$k$类的概率$$w^{(1)}_k$$

$$
w_k^{(i)}=\frac{p_kN(X_i|\mu_k,\Sigma_k)}{\sum_{j=1}^kp_jN(X_i|\mu_j,\Sigma_j)}
$$

其中$$p_j$$表示$$j$$类被选中的概率

- M-SteP: 更新$$\mu$$,$$\Sigma$$,$$p$$ 

  $$
  p_k=\frac{\sum_{i=1}^mw_k^{(i)}}{m}\\
  \mu_k=\frac{\sum_{i=1}^mw_k^{(i)}x_i}{\sum_{i=1}^mw_k^{(i)}}\\
  \Sigma_k=\frac{\sum_{i=1}^mw_k^{(i)}(x_i-\mu_k)^T(x_i-\mu_k)}{\sum_{i=1}^mw_k^{(i)}}
  $$

<a name="DBSCAN"/>

## DBSCAN——P221

由于K-Means和GMM都只能处理凸函数，因此引入基于密度的聚类

<a name="层次聚类"/>

## 层次聚类

<a name="谱聚类"/>

## 谱聚类

<a name="聚类模型的评价"/>

## 聚类模型的评价

<a name="聚类性能度量内部指标"/>

### 聚类性能度量内部指标

DB指数(DBI)

- 类内距离$$avg(C_i)=\frac{2\sum_{j,k}d(j,k)}{\#C(\#C-1)}$$
- 类间距离$$d_{cen}(\mu_i,\mu_j)$$，其中$$\mu_i$$为第$$i$$类中心点

由于我们希望类内距离越小，类间距离越大，因此DBI越小越好

<a name="聚类性能度量外部指标"/>

### 聚类性能度量外部指标

在已知样本分类结果的前提下

$$
RI=\frac{2(|SS|+|DD|)}{\#C(\#C-1)}
$$

其中\|SS\|表示同一类的还是被聚在一起的个数，\|DD\|表示不同类的还是被聚在不同类的个数,$$\#C$$表示所有样本数量
