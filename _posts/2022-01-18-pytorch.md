---
layout: post
title: pytorch
summary: 主要介绍PyTorch中容易混淆的方法区别
featured-img: deep learning
language: chinese
category: deep learning
---


# view

用于改变 tensor 形状

```python
# 将x从size: (1,8)转为size: (2,4)
## 方法一：固定列数和行数
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(2,4)
>>> x.size()
output: torch.Size([2, 4])

## 方法二：固定列数，行数自适应
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(-1,4) # -1表示自适应调整该维度大小, 当确定需要将列数改为4的时候，行数根据数据大小和列数4的关系自动计算
x.size()
output: torch.Size([2, 4])

## 方法三：固定行数，列数自适应
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(2,-1)

# 将x从size: (1,8)转为size: (1, 2,4)
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(1,2,4)
>>> x.size()
output: torch.Size([1, 2, 4])
```

<a name='unsqueeze & squeeze'/>

# unsqueeze & squeeze

## unsqueeze

用于添加维度

```python
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(2,4)
>>> x.size()
output: torch.Size([2, 4])

>>> torch.Size([2, 4])
>>> x = x.unsqueeze(1) # 在第二维上另添加一个维度
>>> x.size()
output: torch.Size([2, 1, 4])

>>> torch.Size([2, 1, 4])
>>> x = x.unsqueeze(0) # 在第一维上另添加一个维度
>>> x.size()
output: torch.Size([1, 2, 4])

>>> x = x.unsqueeze(-1) # 在最后一个维度上另添加一个维度
>>> x.size()
>>> torch.Size([2, 4, 1])
output: torch.Size([2, 4, 1])
```

## squeeze

用于删除维度（当该维度大小为 1 的时候）

```python
>>> x = torch.Tensor(list(range(0,8)))
>>> x = x.view(1,2,4)
>>> x.size()
output: torch.Size([1,2, 4])

>>> x = x.squeeze(0) # 删除第一维
>>> x.size()
output: torch.Size([2, 4])

>>> x = x.squeeze(-1) # 删除最后一维
>>> x.size()
output: torch.Size([1, 2, 4]) # 可以看出并没有成功删除最后一维，因为该维度大小为4.
```

## 运用

- 在做卷积之前需要保证输入的数据是 4 维的，也就是[batch_size, in_channel, feature_size, embedding_dim]。
- 在做 lstm 之前需要保证输入的数据是 3 维的，也就是[feature_size, batch_size， embedding_dim]。注意其中参数'batch_first'的定义：默认为 False，也就是输入的格式为[feature_size, batch_size， embedding_dim]，但往往我们的输入值是形如[ batch_size，feature_size， embedding_dim]，因此需要将 batch_first 设置为 False

<a name='torch.cat'/>

# torch.cat

将多个 tensor 按行或者按列进行拼接

```python
# dim = 0: 按行拼接
>>> x = torch.tensor([[1,2,3],[4,5,6]])
>>> cat_x_0 = torch.cat([x, x, x], dim = 0)
>>> cat_x_0
output: tensor([[1, 2, 3],
        [4, 5, 6],
        [1, 2, 3],
        [4, 5, 6],
        [1, 2, 3],

        [4, 5, 6]])

# dim = 1: 按列拼接
>>> cat_x_1 = torch.cat([x, x, x], dim = 1)
>>> cat_x_1
output: tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],
        [4, 5, 6, 4, 5, 6, 4, 5, 6]])

```

<a name='model.train() & model.eval()'/>

# model.train() & model.eval()

## model.train()

用于训练模型中，启用 BN 和 Dropout。

- 为了**防止过拟合**，每次只选择部分信息进行训练，也就是 dropout（丢弃）一些神经元。（e.g., 若假设 Dropout 层的 p=0.4，那么 1000 个神经元经过 Dropout 层,约有 1000\*0.4=400 个神经元的值会被设置为 0，也就相当于将这 400 个神经元丢弃了。）
- 为了**加快网络收敛**，并且由于是一个 batch 一个 batch 投递给模型进行学习的，因此每个 batch 的均值和方差都不大一样，需要运用 BN 来进行计算，且在所有 batch 结束后，也需要用 BN 对每个 batch 上的均值和方差做指数平均，来得到整个样本上的均值和方差的近似值。
- 为了**防止梯度消失**，在训练过程中，每经过一层中间激活层，得到的输出分布都会有所变化，因此为了修正这些分布，采用 BN 来减少这种内部协变量(ICS)。

## model.eval()

用于模型预测中，不启用 BN 和 Dropout

- 在测试模型，运用训练好的模型进行预测的时候，为了得到最可靠的结果，需要运用的是整个训练好的模型，因此不需要使用 Dropout
- 另外运用训练好的模型进行预测或进行测试模型的时候，是使用已经训练好的参数，因此不需要 BN 来计算样本均值和方差

# nn.ModuleList & nn.Sequential

# running loss & loss

## running loss

当在一个 epoch 下分了多个 batch，此时需要用到 running loss 表示一个 epoch 中到目前 batch 得到的 loss 均值。

令有 m 个 batch，则对于一个 epoch 中进行到第 j 个 batch 时候的 running loss 可表示为

$$\text{running loss}_j = \frac{\sum_i^j \text{Loss}_i}{j} = \text{running loss}_{j-1}+\frac{\text{Loss}_j-\text{running loss}_{j-1}}{j}$$

整个 epoch 得到的 loss 可以表示为

$$\text{loss} = \frac{\sum_i^m \text{Loss}_i}{m} = \text{running loss}_m$$

## loss

在整个数据集上的 loss

<a name='BCELoss() & BCEWithLogitsLoss()'>

# BCELoss() & BCEWithLogitsLoss()

来个等式就一目了然了

BCEWithLogitsLoss() = sigmoid() + BCELoss()

函数 BCELoss(pred, target)要求 pred 中所有值都是在区间[0,1]内，因此一般需要对预测得到的值进行 sigmoid 激活操作。而函数 BCEWithLogitsLoss(pred, target)则对 pred 取值范围没有要求。

```python
>>> label = torch.Tensor([1, 1, 0])
>>> pred = torch.Tensor([2, 4, 0])
>>> pred_sig = torch.sigmoid(pred)
>>> loss = nn.BCELoss()
>>> print(loss(pred_sig, label))
output: tensor(0.2794)

# 也可以写成
>>> label = torch.Tensor([1, 1, 0])
>>> pred = torch.Tensor([2, 4, 0])
>>> loss = nn.BCEWithLogitsLoss()
>>> print(loss(pred, label))
output: tensor(0.2794)
```

<a name='nn.CrossEntropyLoss() & nn.LogSoftmax() & nn.NLLLoss()'/>

# nn.CrossEntropyLoss() & nn.LogSoftmax() & nn.NLLLoss()

nn.CrossEntropyLoss() = nn.LogSoftmax() + nn.NLLLoss()

- nn.NLLLoss()
  一般需要与 nn.LogSoftmax()连用，也就是在搭建的网络中最终需要加一层 LogSoftmax()

把 LogSoftmax()的输出与真实 Label 对应的那个值拿出来，再去掉负号，再求均值。

<a name='torch.Tensor() & torch.tensor()'/>

# torch.Tensor() & torch.tensor()

- torch.Tensor()

  - 默认返回的 Tensor 类里面的数据是 float 类型
  - 神经网络层的输入数据必须用 torch.Tensor()进行创建

- torch.tensor()
  - 返回的 Tensor 类里面的数据是根据实际数据类型进行推断
  - 若为了存储下标，则必须用 torch.tensor()来包裹整型数据。

一般我们可以通过 torch.Tensor().long(), torch.Tensor().float()的形式来创建指定数据类型的 tensor.

<a name='model.load_state_dict'/>

# model.load_state_dict

用于加载已训练好的模型

```python
model.load_state_dict(torch.load(model_path, strict=False))  # 注意'strict=False'必须有，否则会报错'Missing key(s) in state_dict'
```

# 保存训练好的模型及其参数
```python
torch.save({
  'config': config,
  'model': model.state_dict(),
  'optimizer': optimizer.state_dict(),
  'scheduler': scheduler.state_dict(),
  'iteration': it,
}, checkpoint_path)
```
# 加载部分预训练模型并冻结某些层
## 加载预训练模型信息
```python
model_state_dict = torch.load('./XX.pt')
model_dict = model_state_dict['model']
model = ... # 根据训练的初始值实例化model
model.load_state_dict(model_state_dict['model'])
```
## 删除部分不需要的层
在运用预训练得到模型之前需要对其进行一些删除，修改操作
```python
model_dict.pop(<layer_name>)
```
## 提取预训练得到的部分层权重信息
```python
model_dict = {k: v for k, v in model_dict.items() if k in my_model_dict} # 提取出与自定义模型层相同层的参数，也就是保留对我们有用的参数，剔除无用的。
```
## 参数更新
```python
my_model_dict.update(model_dict)
```

## 加载我们正在需要的模型
```python
model = ... # 根据训练的初始值实例化model
model.load_state_dict(model_dict)
```
## 冻结部分不需要进行更新的层
```python
```