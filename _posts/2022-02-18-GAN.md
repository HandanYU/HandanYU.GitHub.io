---
layout: post
title: GAN模型原理和应用
summary: 。
featured-img: deep learning
language: chinese 
category: deep learning
---
#### Table of Contents
- [policy gradient](#policy gradient)
- [Standard GAN](#Standard GAN)
- [ORGAN](#ORGAN)

<a name='policy gradient'/>

# policy gradient

首先通俗易懂的解释policy gradient，就是我们希望我们训练出来的model所做的policy能够得到尽可能高的reward。

在监督学习中我们通常使用交叉熵作为损失函数，也就是

$$L = -\sum_i y_i \log p(x_i)$$

其中$$y_i$$表示第i个样本对应的label，$$p(x_i)$$表示第i个样本输出概率

对于强化学习，我们用reward来代替label，则损失函数可以表示为

$$L=-\sum_{\tau}R(\tau)\log \pi(\tau)$$

其中$$\pi(\tau)$$表示采取$$\tau$$策略发生的概率，$$R(\tau)$$表示采取$$\tau$$策略后得到的reward值。

运用**PyTorch**可以这样实现
```python
input = torch.Tensor([[1,4,5],[3,5,7]])
last_out_layer = nn.LogSoftmax(dim=1)  # 计算\log\pi
output = last_out_layer(input)

# 计算R(\tau)\log\pi
R = torch.Tensor([3,6,-1])
policy_output = R * output

# 计算R(\tau)\log\pi(\tau)，也就是采取对应target的策略发生的损失
target = torch.tensor([2,1]) # 注意必须用torch.tensor()创建
loss = nn.NLLLoss()
L = loss(policy_output, target)
```

对于生成一些离散数据（如文本生成），在generator中有argmax操作，该操作是不可导的，因此在反向传播的时候，梯度更新就会停止在该操作处，从而导致generator无法更新。通过选择policy gradient，在反向传播的过程中，就可以避免对不可求导操作进行求导了。

<a name='Standard GAN'/>

# Standard GAN

## 基本架构
主要由Generator和Discriminator两部分组成
- Generator
    - 目标是尽可能多的生成与真实数据十分相近的数据，以假乱真让Discriminator区分不了。
    - Generator一般由Linear层, Sigmoid层组成
- Discriminator
    - 目标是尽可能准确的判断喂给它的数据是来自原始真实数据集还是由Generator生成的。
    - Discriminator一般有多个CNN组成


## Loss

我们假设Generator生成的数据的分布为$$P_G(x;\theta)$$，于是我们顺势想要求解其似然函数

$$L=\prod_{i=1}^m P_G(x_i;\theta)$$

始终牢记Generator的目标（生成尽可能与真实图片相近的数据），因此我们的目标是最大化似然函数。

### Generator的目标函数

$$\argmax_{\theta}\prod_{i=1}^m P_G(x_i;\theta)$$

- 取对数

$$\argmax_{\theta}\log\prod_{i=1}^m P_G(x_i;\theta)$$

$$\argmax_{\theta}\sum_{i=1}^m P_G(x_i;\theta)$$

- 由于这m个样本数据是来自真实分布，因此可以约等于真实分布中的所有x在生成分布中的log似然的期望值

$$\argmax_{\theta}E_{x \sim P_{data}}[\log P_G(x;\theta)]$$

- 引入KL散度定义

$$KL(P\||Q) = - E_P[\log Q]$$

则又可以转化为$$\argmax_{\theta}-KL(P_{data}(x)\||P_G(x;\theta)$$





损失函数也有两部分组成，一部分是Discrminator对真实数据判断的loss，另一部分是对生成数据判断的loss。也就是真实数据来自





## 目标函数

$$\arg \min_G \max_D V(G,D)=E_{x\sim P_{data}}[\log D(x)]+E_{x \sim  P_G}[\log(1-D(x))]$$

固定G，最大化$$P_G$$和$$P_{data}$$之间差异。固定D，让这最大值最小化，也就是使得两个分布之间差异最小。
- fix G, learning D

    $$\max_D E_{x\sim P_{data}}[\log D(x)]+E_{x \sim  P_G}[\log(1-D(x))]$$
- fix D, learning G
    $$\min_G  E_{x \sim  P_G}[\log(1-D(x))]$$

#### [参考文章](https://www.cnblogs.com/bonelee/p/9166084.html)
<a name='ORGAN'/>

# Objective-reinforced GAN (ORGAN)

## 与GAN的区别
- GAN中每次迭代进行k次discrminator的参数updating但只进行一次generator的参数updating。而ORGAN每次迭代discrminator的参数和generator的参数都进行了多次updating.
- ORGAN加入了奖励机制。

