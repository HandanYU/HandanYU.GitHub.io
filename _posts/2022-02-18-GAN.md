---
layout: post
title: GAN模型原理和应用
summary: GAN模型原理和应用。
featured-img: deep learning
language: chinese
category: deep learning
---

#### Table of Contents

- [policy gradient](#policy gradient)
- [Standard GAN](#Standard GAN)
- [ORGAN](#ORGAN)

<a name='policy gradient'/>

# policy gradient

首先通俗易懂的解释 policy gradient，就是我们希望我们训练出来的 model 所做的 policy 能够得到尽可能高的 reward。

在监督学习中我们通常使用交叉熵作为损失函数，也就是

$$L = -\sum_i y_i \log p(x_i)$$

其中$$y_i$$表示第 i 个样本对应的 label，$$p(x_i)$$表示第 i 个样本输出概率

对于强化学习，我们用 reward 来代替 label，则损失函数可以表示为

$$L=-\sum_{\tau}R(\tau)\log \pi(\tau)$$

其中$$\pi(\tau)$$表示采取$$\tau$$策略发生的概率，$$R(\tau)$$表示采取$$\tau$$策略后得到的 reward 值。

运用**PyTorch**可以这样实现

```python
input = torch.Tensor([[1,4,5],[3,5,7]])
last_out_layer = nn.LogSoftmax(dim=1)  # 计算\log\pi
output = last_out_layer(input)

# 计算R(\tau)\log\pi
R = torch.Tensor([3,6,-1])
policy_output = R * output

# 计算R(\tau)\log\pi(\tau)，也就是采取对应target的策略发生的损失
target = torch.tensor([2,1]) # 注意必须用torch.tensor()创建
loss = nn.NLLLoss()
L = loss(policy_output, target)
```

对于生成一些离散数据（如文本生成），在 generator 中有 argmax 操作，该操作是不可导的，因此在反向传播的时候，梯度更新就会停止在该操作处，从而导致 generator 无法更新。通过选择 policy gradient，在反向传播的过程中，就可以避免对不可求导操作进行求导了。

<a name='Standard GAN'/>

# Standard GAN

## 基本架构

主要由 Generator 和 Discriminator 两部分组成

- Generator
  - 目标是尽可能多的生成与真实数据十分相近的数据，以假乱真让 Discriminator 区分不了。
  - Generator 一般由 Linear 层, Sigmoid 层组成
- Discriminator
  - 目标是尽可能准确的判断喂给它的数据是来自原始真实数据集还是由 Generator 生成的。
  - Discriminator 一般有多个 CNN 组成

## Loss

我们假设 Generator 生成的数据的分布为$$P_G(x;\theta)$$，于是我们顺势想要求解其似然函数

$$L=\prod_{i=1}^m P_G(x_i;\theta)$$

始终牢记 Generator 的目标（生成尽可能与真实图片相近的数据），因此我们的目标是最大化似然函数。

### Generator 的目标函数

$$\argmax_{\theta}\prod_{i=1}^m P_G(x_i;\theta)$$

- 取对数

$$\argmax_{\theta}\log\prod_{i=1}^m P_G(x_i;\theta)$$

$$\argmax_{\theta}\sum_{i=1}^m P_G(x_i;\theta)$$

- 由于这 m 个样本数据是来自真实分布，因此可以约等于真实分布中的所有 x 在生成分布中的 log 似然的期望值

$$\argmax_{\theta}E_{x \sim P_{data}}[\log P_G(x;\theta)]$$

- 引入 KL 散度定义

$$KL(P\||Q) = - E_P[\log Q]$$

则又可以转化为$$\argmax_{\theta}-KL(P_{data}(x)\||P_G(x;\theta)$$

损失函数也有两部分组成，一部分是 Discrminator 对真实数据判断的 loss，另一部分是对生成数据判断的 loss。也就是真实数据来自

## 目标函数

$$\arg \min_G \max_D V(G,D)=E_{x\sim P_{data}}[\log D(x)]+E_{x \sim  P_G}[\log(1-D(x))]$$

固定 G，最大化$$P_G$$和$$P_{data}$$之间差异。固定 D，让这最大值最小化，也就是使得两个分布之间差异最小。

- fix G, learning D

  $$\max_D E_{x\sim P_{data}}[\log D(x)]+E_{x \sim  P_G}[\log(1-D(x))]$$

- fix D, learning G
  $$\min_G  E_{x \sim  P_G}[\log(1-D(x))]$$

#### [参考文章](https://www.cnblogs.com/bonelee/p/9166084.html)

<a name='ORGAN'/>

# Objective-reinforced GAN (ORGAN)

## 与 GAN 的区别

- GAN 中每次迭代进行 k 次 discrminator 的参数 updating 但只进行一次 generator 的参数 updating。而 ORGAN 每次迭代 discrminator 的参数和 generator 的参数都进行了多次 updating.
- ORGAN 加入了奖励机制。

<a name='latentGAN'/>

# latentGAN

latentGAN = autoencoder + GAN

## 与其他 GAN 的区别

generator 和 discriminator 的输入不再是 SMILES strings, 而是用 autoencoder 训练得到的 SMILES heteroencoder (来自 latent space)
