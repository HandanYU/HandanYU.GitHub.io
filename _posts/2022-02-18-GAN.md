---
layout: post
title: GAN模型原理和应用
summary: 。
featured-img: 损失函数
language: chinese 
category: deep learning
---
#### Table of Contents
- [policy gradient](#policy gradient)
- [Standard GAN](#Standard GAN)
- [ORGAN](#ORGAN)

<a name='policy gradient'/>

# policy gradient

首先通俗易懂的解释policy gradient，就是我们希望我们训练出来的model所做的policy能够得到尽可能高的reward。

在监督学习中我们通常使用交叉熵作为损失函数，也就是

$$L = -\sum_i y_i \log p(x_i)$$

其中$$y_i$$表示第i个样本对应的label，$$p(x_i)$$表示第i个样本输出概率

对于强化学习，我们用reward来代替label，则损失函数可以表示为

$$L=-\sum_{\tao}R(\tao)\log \pi(\tao)$$

其中$$\pi(\tao)$$表示采取$$\tao$$策略发生的概率，$$R(\tao)$$表示采取$$\tao$$策略后得到的reward值。

运用**PyTorch**可以这样实现
```python
input = torch.Tensor([[1,4,5],[3,5,7]])
last_out_layer = nn.LogSoftmax(dim=1)  # 计算\log\pi
output = last_out_layer(input)

# 计算R(\tao)\log\pi
R = torch.Tensor([3,6,-1])
policy_output = R * output

# 计算R(\tao)\log\pi(\tao)，也就是采取对应target的策略发生的损失
target = torch.tensor([2,1]) # 注意必须用torch.tensor()创建
loss = nn.NLLLoss()
L = loss(policy_output, target)
```



<a name='Standard GAN'/>

# Standard GAN

<a name='ORGAN'/>

# Objective-reinforced GAN (ORGAN)

## 与GAN的区别
- GAN中每次迭代进行k次discrminator的参数updating但只进行一次generator的参数updating。而ORGAN每次迭代discrminator的参数和generator的参数都进行了多次updating.
- ORGAN加入了奖励机制。