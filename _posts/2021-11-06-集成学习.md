---
layout: post
title: 集成学习
summary: 本章讲述了机器学习中的集成算法。集成算法主要分为Boost, Bag两种。
featured-img: 集成学习
language: chinese 
---

# 集成学习

集成学习算法存在过拟合、鲁棒性不强等问题。

## AdaBoosting

### 算法流程

- Step1：初始化：最大迭代次数，各样本权重

- Step2：列出所有可能的弱分类器

- Step3：计算所有可能的弱分类器的错误率，并选择错误率最小的作为其中一个弱分类器

- Step4：权重更新

  $$
  \alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon})~~~~~~~~其中\epsilon为当前弱分类器的错分率\\
  z_i=w_iexp(-\hat{y_i}y_i\alpha)~~~~~~~~~~~~~~~其中\hat{y_i}为预测值\\
  w_i^{new}=\frac{z_i}{\sum_{j=1}^mz_j}
  $$

- 重复Step3,4达到需要的弱分类器个数或强分类器的错误率=0

### 性质

1. 所有被错分样本的权重更新比例是相同的，所有被分对的样本的权重更新比例是相同的
2. 串行算法
3. AdaBoost模型是弱分类器的线性组合
4. AdaBoost算法的一个解释是该算法实际上是前向分步算法的一个实现，在这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分步算法。


$$

$$

