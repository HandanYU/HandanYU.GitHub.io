---
layout: post
title: Deep Learning读书笔记
summary: 本章介绍了梯度下降法的概念，典型的梯度下降法的概念和优缺点，以及几种典型的优化器
featured-img: 梯度下降
---

# optimization
The goal of Machine Learning problem is to minimize the expected generalization error.
## The difference between Pure/Exact Optimization and Optimization for training ML models

### empirical risk minimization
1. do we know the true distribution of data ?
    - Yes: Pure/Exact Optimization
    - No: Machine Learning problem

So convert ML problem back into an Optimization problem using **empirical distribution** instead of true ditribution. Then minimize the empirical risk. (This process is called as **empirical risk minimization**)

#### disadvantages of empirical risk minimization

- prone to overfitting
- some of loss functions do not have derivitives (e.g., 0-1 loss function) -> introduce **surrogate function** (e.g. log function) to deal with this problem
- prone to halt at a local minimum -> introduce **early stopping** to address this problem

### surrogate function & early stopping
2. does it halt at a local minimum ?
    - Yes: Pure/Exact Optimization (considered to have converged when the gradient becomes very small.)
    - No: Machine Learning problem (Training often halts while the surrogate loss function still has large derivatives)

**early stopping criterion** is basedon the true underlying loss function

### Batch and Minibatch
3. Can objective function be computed by seperating into several subsets ?
    - Yes: Machine Learning problem (objective function usually decomposes as a sumover the training examples)
    - No: Pure/Exact Optimization

Minibatch is faster to make objective function convergence. Since in each epoch, Minibatch does many more times derivation while batch can only does once. Additionally,  computation time per update does not grow with thenumber of training examples.Also, batch algorithm spends too much time on computing derivitives.

#### The motivation of minibatch (with random sampling)
- too slow to compute exact gradient using batch
- a small number of samples is redundancy in the training set.

#### batch/deterministic gradient methods

Optimization algorithms that use the entire training set

#### stochastic/online methods

Optimization algorithms that use only a single example at a time

#### minibatch/minibatch stochastic methods

using more than one but fewer than all the training examples

#### why do we need to shuffle the samples before selecting minibatch

for some dataset where the order of the datasetholds some signiﬁcance. e.g., medical data with a long list of blood sample test results. If we do not do shuffle before, in each batch, the data represent primarily one patient out of themany patients in the dataset.

#### different types of algorithms use diﬀerent kinds of information from the mini-batch in various ways
- methods that compute updates based only on the gradient are usually relatively robust and can handle smaller batch sizes, like 100.
- second-order methods, which use Hessian Matrix H and compute updates such as $$H^{-1}g$$, require much larger batch sizes, like 100,000.


## Challenges in Neural Network Optimization
### Ill-Conditioning

### Local Minima
#### convex & nonconvex
- if the objective function is convex, we can find the local minima instead of finding the global minima. That is, any local minimum isguaranteed to be a global minimum.
- Neural network's objective function is nonconvex. Although all of activation functions are convex, because of hidden layers, the objective function cannot be guaranteed as convex. 
#### Local Minima is problematric ?
- Local minima can be problematic if they have high cost in comparison to the global minimum
- all these local minima arising from nonidentiﬁability are equivalent toeach other in cost function value, in this case, the local minima cannot be problematic.

#### how to determine that whether it is due to Local Minima
plot the norm of the gradient over time to rule out the local minima
- If the norm of the gradient does not shrink to insigniﬁcant size, the problem is neither local minima nor any other kind of critical point

#### why second-order methods have not succeeded in replacing gradient descent forneural network training.
due to the proliferation of saddle points in high-dimensional spaces 

### Cliffs and Exploding Gradients

#### why there are so many cliffs structure in the cost function of deep NN, specially in RNN ?
result from the multiplication of several large weights together.

As for RNN, because such models involve amultiplication of many factors, with one factor for each time step. Long temporalsequences thus incur an extreme amount of multiplication

#### what is the risk of cliff structures? / what is exploding gradient problem

Because of large weights, when the parameter gradient is very large, a gradient descent parameter update could throw the parameters very far into a region where the objective function is larger, that is the parameters will be far away from the optimal values.

#### How to avoid the risk of cliffs?

Because the gradient descent parameter update is related to both parameter gradient and learning rate, when the parameter gradient is too large, we typically use learning rates that decay slowly enough that consecutive steps have approximately the same learning rate. 

**clipping the gradient**
- element-wise clipping: 
    clip the parameter gradient from a minibatch element-wise before the parameter update
    - the direction of the update is not aligned with the true gradient or the minibatch gradient, but it is still a descent direction.
- gradient norm clipping: 
    clip the norm of the gradient before the parameter update
    - advantages: guaranteeing that each step is still in the gradient direction
    - property(differences between traditional minibatch gradient descent): the true gradient direction is not equal to the average over all minibatch gradients.
### Long-Term Dependencies
#### vanishing and exploding gradient problem
do not happen in feedforward networks.

- the approaches to deal with exploding gradient problem
    - clipping the gradient
    - weithts regularization
### Inexact Gradients
Since in practices, we usually have only a noisy or even biased estimate of these quantities, nearly every deep learning algorithm relies on sampling-based estimates (e.g.,using a minibatch of training examples to compute the gradient. ) 

### Future research will need to develop further understanding of the factors thatinﬂuence the length of the learning trajectory and better characterize the outcomeof the process. 