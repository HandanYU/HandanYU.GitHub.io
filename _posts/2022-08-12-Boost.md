---
layout: post
title: Boosting
summary: 本章讲述了机器学习中的集成算法。集成算法主要分为Boost, Bag两种。
featured-img: machine learning
language: chinese 
category: machine learning
---

# 基本概念
## 前向差分算法
**基本思想**:每次只学习一个基函数及系数,逐步逼近最优解
## 残差和负梯度的关系
- 残差是负梯度在损失函数是**平方误差**时候的特殊情况

- 对于梯度提升模型我们的目标是寻找一个$$f(x)$$使得损失函数$$L(y,f(x))$$最小。由于是解决**最小化问题**，因此采用**梯度下降**方法，也就是关注**负梯度**

- 损失函数的负梯度可以表示为

$$
-\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)}
$$

- 当损失函数为**平方误差**的时候，也就是

$$
L(y, f_{m-1}(x)) = \frac{1}{2}(y-f_{m-1}(x))^2
$$

则带入上式，得到此时损失函数的负梯度为

$$
-\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)} = y - f_{m-1}(x)
$$

此时我们可以发现$$y - f_{m-1}(x)$$就是当前模型的拟合残差。因此我们通常用以下式子计算残差：

$$
r_{m-1} = -\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)} 
$$

# 梯度提升决策树（Gradient Boosting Decision Tree, GBDT）

- GBDT属于集成学习中的Boosting算法，即是一个串行的算法，通过逐步拟合逼近真实值。
- 其基分类器是回归树(CART)。
- 可以减少bias（误差）却不能减少variance（偏差），因为每次基本都是全样本参与训练，不能消除偶然性的影响，但每次都逐步逼近真实值，可以减少误差。
- 目标：通过寻找新的模型使得残差不断减小。
- 每一棵树学的是之前所有树的残差，这个残差累加后能得到真实值
- 用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。
## 基本思路

GBDT的基本想法是让新的基模型去拟合前面模型的偏差，从而不断将加法模型的偏差降低。

## 基学习器——回归树（CART）

$$
T(x;\Theta) = \sum_{j=1}^J c_jI(x\in R_j)
$$

## GBDT模型

$$
\hat{y_i} = \sum_{k=1}^Kf_k(x_i)
$$

## 模型优化
采用Additive Training & 前向差分
模型优化过程可以有如下：

$$
\hat{y}^{(0)} = f_0(x) = 0\\
\hat{y}^{(1)} = f_1(x) = T_0(x;\Theta_1) = f_0(x) + T(x;\Theta_1)\\
\hat{y}^{(2)}=f_2(x) = T_1(x;\Theta_1) + T(x;\Theta_2) = f_1(x) + T_2(x;\Theta_2)\\
\cdot\\
\cdot\\
\cdot\\
\hat{y}^{(m)}=f_m(x) = \sum_{j=1}^mT(x;\Theta_m) = f_{m-1}(x) + T_m(x;\Theta_m)
$$

## 损失函数

$$
L(y_i,\hat{y_i})
$$

- 对于回归问题:
    - 常用损失函数有：MAE、MSE、RMSE
    - 当为均方差MAE/平方误差Square Loss的时候： $$L(y_i,\hat{y_i})=(y_i-\hat{y_i})^2$$
- 对于分类问题:
    - 二分类

$$
L(y_i,\hat{y_i})=\log (1+e^{-2y_i\hat{y_i}}),\quad y_i\in\{-1,1\}
$$

- 多分类

$$
L(y_i,\hat{y_i})=-\sum_{k=1}^Ky_k\log \frac{e^{\hat{y_k}}}{\sum_{k=1}^Ke^{\hat{y_k}}}
$$

## 目标函数

我们的目标是【**经验风险最小**】，即

$$
\min_{\Theta_m} \sum_{i=1}^n L(y_i,\hat{y_i}^{(m)})
$$

## 学习过程/求解过程
由于目标是进行最小化，因此通过**梯度下降**的方法来求极值（也就是选择**损失函数的负梯度方向**能够快速找到最优解）

换句话说，也就是通过**损失函数的负梯度方向**去拟合损失值，从而进一步去拟合CART树。用公式表示就是

$$
T_m(x_i;\Theta_m) \approx f_m(x_i) - f_{m-1}(x_i) \\=-\frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}
$$

> 另一种理解方法：采用泰勒一阶展开$$f(x+\Delta x) \approx f(x) + f'(x)\Delta x$$

$$
L(y_i,\hat{y_i}^{(m)}) = L(y_i,\hat{y_i}^{(m-1)}+T_m(x_i,\Theta_m))\\
\approx L(y_i,\hat{y_i}^{(m-1)})+\frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}T_m(x_i,\Theta_m)
\\\Rightarrow L(y_i,\hat{y_i}^{(m)})-L(y_i,\hat{y_i}^{(m-1)})\approx \frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}T_m(x_i,\Theta_m)
$$

由于我们的目的是$$L(y_i,\hat{y_i}^{(m)}) < L(y_i,\hat{y_i}^{(m-1)})$$

则当

$$
T_m(x_i,\Theta_m) = -\frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}
$$

的时候一定成立。

## 算法流程步骤
已知有样本数据$${(x_i,y_i)}$$
- Step 1: 取初始弱学习器:$$f_0(x)$$
- Step 2: 拟合第1颗回归树
    - 将【上一轮的损失函数负梯度】作为本轮的损失值近似值，(当损失函数是均方误差的时候也就是计算残差：$$r_{i1}=y_i-f_0(x_i)$$)

    $$r_{i1}=-\frac{\partial L(y_i, f_0(x))}{\partial  f_0(x) }$$

    - 将$$r_{i1}$$看作是$$f_1(x_i)$$的真实值。
    - 寻找回归树的最佳划分节点
        - 遍历每个特征的每个可能取值
        - 分别计算方差，找到使总方差最小的那个划分节点即为最佳划分节点
    - 对划分后的叶子结点分别赋一个参数，来拟合残差：一般采用该节点$$j: r_{i1}\quad i\in I_j$$均值

- Step 3: 拟合函数更新(模型更新)：$$f_1(x) = f_0(x) + T_0(x;\Theta_0)$$
- Step 4: 不断进行Step2,3。

## 实际求解回归问题
- 1.初始化残差，构成弱学习器1。（预测特征所对应得特征值求平均值）
- 2.计算残差（实际值 - 弱学习器1）。
- 3.寻找回归树的最佳划分点（阈值）。遍历每个特征的每个特征值作为阈值，通过阈值将数据二分，分别计算方差，找到使方差最小的特征值为最佳二分阈值
- 4.将二分后的残差值更新为实际值，计算实际值平均值 作为残差。构成弱学习器2。
- 5.合并强学习器。（弱学习器1 + 弱学习器2）
- 6.满足条件迭代停止。


# XGBoost

- XGBoost属于GBDT属于集成学习中的Boosting算法

## 相较于GBDT的改进
- GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到了二阶。保留了更多有关目标函数的信息，对提升效果有帮助。
- GBDT是给新的基模型寻找新的拟合标签（前面加法模型的负梯度），而xgboost是给新的基模型寻找新的目标函数（目标函数关于新的基模型的二阶泰勒展开）。
- xgboost加入了和叶子权重的L2正则化项，因而有利于模型获得更低的方差。
- xgboost增加了自动处理缺失值特征的策略。通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。
- xgboost支持并行计算

## XGBoost模型

$$\hat{y_i} = \sum_{k=1}^Kf_k(x_i)$$

## 目标函数
由**损失项**加**正则项**组成
$$J=\sum_{i=1}^nL(y_i,\hat{y_i})+\sum_{k=1}^K\Omega(f_k)$$
- 其中$$\Omega(f_k)=\gamma T+\sum_{j=1}^T w_j^2$$，$$T$$为树的叶子节点个数，$$w_j$$表示第$$j$$个叶子节点的权重

## 目标函数的求解
> 如何进行Boost的？

- 采用【前向差分】：

$$
J^{(t)}=\sum_{i=1}^nL(y_i,\hat{y_i}^{(t)})+\sum_{k=1}^K\Omega(f_k)\\
=\sum_{i=1}^nL(y_i,\hat{y_i}^{(t-1)}+f_t(x_i))+\sum_{k=1}^{t-1}\Omega(f_k)+\Omega(f_t)
$$

- 采用【泰勒二阶展开】：

$$
J^{(t)}=\sum_{i=1}^nL(y_i,\hat{y_i}^{(t-1)}+f_t(x_i))+\sum_{k=1}^{t-1}\Omega(f_k)+\Omega(f_t)\\
=\sum_{i=1}^n[L(y_i,\hat{y_i}^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\sum_{k=1}^{t-1}\Omega(f_k)+\Omega(f_t)
$$

- 去除已知量（常量）$$L(y_i,\hat{y_i}^{(t-1)}), \sum_{k=1}^{t-1}\Omega(f_k)$$

$$
J^{(t)}=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)
$$

- 令$$I_j=\{i\lvert q(x_i)=j\}$$ 表示叶子节点$$j$$中的样本，并将$$\Omega(f_t) = \gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw^2_j$$代入

$$
J^{(t)}=\sum_{j=1}^T[\sum_{i\in I_j}g_iw_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\gamma T
$$

- 令$$G_j=\sum_{i\in I_j}g_i,  H_j= \sum_{i\in I_j}h_i$$

$$
J^{(t)}(w)=\sum_{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2]+\gamma T
$$

- 求极值：
    - 目标函数对$$w$$求导，

    $$\frac{\partial J(w_j)}{\partial w_j}=G_j+(H_j+\lambda)w_j$$


    - 令$$\frac{\partial J(w_j)}{\partial w_j}=0$$得

    $$
    w_j^*=-\frac{G_j}{H_j+\lambda}
    $$
    - 代回$$J^{(t)}(w)$$得到

    $$
    J^{(t)}(w)=-\frac{1}{2}\sum_{j=1}^T\frac{G^2_j}{H_j+\lambda}+\gamma T
    $$

# References

[【 GBDT算法】 机器学习实例推导计算+公式详细过程 （入门必备）](https://blog.csdn.net/weixin_41194171/article/details/85042720)

[GBDT理论推导总结](https://zhuanlan.zhihu.com/p/456801055)