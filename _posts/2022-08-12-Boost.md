---
layout: post
title: Boosting
summary: 本章讲述了机器学习中的集成算法。集成算法主要分为Boost, Bag两种。
featured-img: machine learning
language: chinese 
category: machine learning
---

# Boost基本概念
在集成学习的Boosting提升算法中，有两大家族：第一是AdaBoost提升学习方法，另一种是GBDT梯度提升树。
## 前向分布算法
**基本思想**:每次只学习一个基函数及系数,逐步逼近最优解
## 残差和负梯度的关系
- 残差是负梯度在损失函数是**平方误差**时候的特殊情况

- 对于梯度提升模型我们的目标是寻找一个$$f(x)$$使得损失函数$$L(y,f(x))$$最小。由于是解决**最小化问题**，因此采用**梯度下降**方法，也就是关注**负梯度**

- 损失函数的负梯度可以表示为

$$
-\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)}
$$

- 当损失函数为**平方误差**的时候，也就是

$$
L(y, f_{m-1}(x)) = \frac{1}{2}(y-f_{m-1}(x))^2
$$

则带入上式，得到此时损失函数的负梯度为

$$
-\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)} = y - f_{m-1}(x)
$$

此时我们可以发现$$y - f_{m-1}(x)$$就是当前模型的拟合残差。因此我们通常用以下式子计算残差：

$$
r_{m-1} = -\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)} 
$$

# 梯度提升决策树（Gradient Boosting Decision Tree, GBDT）
- 基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪学习，每次迭代都学习一棵CART树来拟合之前t-1棵树的预测结果与训练样本真实值的残差。
- GBDT属于集成学习中的Boosting算法，即是一个串行的算法，通过逐步拟合逼近真实值。
- 其基分类器是回归树(CART)。
- 可以减少bias（误差）却不能减少variance（偏差），因为每次基本都是全样本参与训练，不能消除偶然性的影响，但每次都逐步逼近真实值，可以减少误差。
- 目标：通过寻找新的模型使得残差不断减小。
- 每一棵树学的是之前所有树的残差，这个残差累加后能得到真实值
- 用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。

## 基本思路

GBDT的基本想法是让新的基模型去拟合前面模型的偏差，从而不断将加法模型的偏差降低。

## 核心思想
利用【损失函数的负梯度方向】在当前模型的值作为残差的近似值，进而拟合一棵CART回归树

## 基学习器——回归树（CART）

$$
T(x;\Theta) = \sum_{j=1}^J c_jI(x\in R_j)
$$

## GBDT模型

$$
\hat{y_i} = \sum_{k=1}^Kf_k(x_i)
$$

## 模型优化
采用Additive Training & 前向分布
模型优化过程可以有如下：

$$
\hat{y}^{(0)} = f_0(x) = 0\\
\hat{y}^{(1)} = f_1(x) = T_0(x;\Theta_1) = f_0(x) + T(x;\Theta_1)\\
\hat{y}^{(2)}=f_2(x) = T_1(x;\Theta_1) + T(x;\Theta_2) = f_1(x) + T_2(x;\Theta_2)\\
\cdot\\
\cdot\\
\cdot\\
\hat{y}^{(m)}=f_m(x) = \sum_{j=1}^mT(x;\Theta_m) = f_{m-1}(x) + T_m(x;\Theta_m)
$$

## 损失函数

$$
L(y_i,\hat{y_i})
$$

- 对于回归问题:
    - 常用损失函数有：MAE、MSE、RMSE
    - 当为均方差MAE/平方误差Square Loss的时候： $$L(y_i,\hat{y_i})=(y_i-\hat{y_i})^2$$
- 对于分类问题:
    - 二分类

$$
L(y_i,\hat{y_i})=\log (1+e^{-2y_i\hat{y_i}}),\quad y_i\in\{-1,1\}
$$

- 多分类

$$
L(y_i,\hat{y_i})=-\sum_{k=1}^Ky_k\log \frac{e^{\hat{y_k}}}{\sum_{i=1}^Ke^{\hat{y_i}}}
$$

## 目标函数

我们的目标是【**经验风险最小**】，即

$$
\min_{\Theta_m} \sum_{i=1}^n L(y_i,\hat{y_i}^{(m)})
$$

## 学习过程/求解过程
由于目标是进行最小化，因此通过**梯度下降**的方法来求极值（也就是选择**损失函数的负梯度方向**能够快速找到最优解）

换句话说，也就是通过**损失函数的负梯度方向**去拟合损失值，从而进一步去拟合CART树。用公式表示就是

$$
T_m(x_i;\Theta_m) \approx f_m(x_i) - f_{m-1}(x_i) \\=-\frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}
$$

> 另一种理解方法：采用泰勒一阶展开$$f(x+\Delta x) \approx f(x) + f'(x)\Delta x$$

$$
L(y_i,\hat{y_i}^{(m)}) = L(y_i,\hat{y_i}^{(m-1)}+T_m(x_i,\Theta_m))\\
\approx L(y_i,\hat{y_i}^{(m-1)})+\frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}T_m(x_i,\Theta_m)
\\\Rightarrow L(y_i,\hat{y_i}^{(m)})-L(y_i,\hat{y_i}^{(m-1)})\approx \frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}T_m(x_i,\Theta_m)
$$

由于我们的目的是$$L(y_i,\hat{y_i}^{(m)}) < L(y_i,\hat{y_i}^{(m-1)})$$

则当

$$
T_m(x_i,\Theta_m) = -\frac{\partial L(y_i,\hat{y_i}^{(m-1)})}{\partial \hat{y_i}^{(m-1)}}
$$

的时候一定成立。

## 算法流程步骤
已知有样本数据$${(x_i,y_i)}$$
- Step 1: 取初始弱学习器:$$f_0(x)$$
- Step 2: 拟合第1颗回归树
    - 将【上一轮的损失函数负梯度】作为本轮的损失值近似值，(当损失函数是均方误差的时候也就是计算残差：$$r_{i1}=y_i-f_0(x_i)$$)

    $$r_{i1}=-\frac{\partial L(y_i, f_0(x))}{\partial  f_0(x) }$$

    - 将$$r_{i1}$$看作是$$f_1(x_i)$$的真实值。
    - 寻找回归树的最佳划分节点
        - 遍历每个特征的每个可能取值
        - 分别计算方差，找到使总方差最小的那个划分节点即为最佳划分节点
    - 对划分后的叶子结点分别赋一个参数，来拟合残差：一般采用该节点$$j: r_{i1}\quad i\in I_j$$均值

- Step 3: 拟合函数更新(模型更新)：$$f_1(x) = f_0(x) + T_0(x;\Theta_0)$$
- Step 4: 不断进行Step2,3。

## 实际求解回归问题
- 1.初始化残差，构成弱学习器1。（预测特征所对应得特征值求平均值）
- 2.计算残差（实际值 - 弱学习器1）。
- 3.寻找回归树的最佳划分点（阈值）。遍历每个特征的每个特征值作为阈值，通过阈值将数据二分，分别计算方差，找到使方差最小的特征值为最佳二分阈值
- 4.将二分后的残差值更新为实际值，计算实际值平均值 作为残差。构成弱学习器2。
- 5.合并强学习器。（弱学习器1 + 弱学习器2）
- 6.满足条件迭代停止。

## GBDT防止过拟合的方法
- shrinkage：学习率/步长,为了给后面的训练留出更多的学习空间。

## 优点
GBDT可以自动筛选特征

# XGBoost (xgb)

- XGBoost属于GBDT属于集成学习中的Boosting算法
- 是对GBDT的一种改进算法

## 相较于GBDT的改进
- 【基分类器】：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的逻辑回归(分类问题)或者线性回归(回归问题)
- 【导数信息】：GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到了二阶。保留了更多有关目标函数的信息，对提升效果有帮助。
- 【目标】：GBDT是给新的基模型寻找新的拟合标签（前面加法模型的负梯度），而xgboost是给新的基模型寻找新的目标函数（目标函数关于新的基模型的二阶泰勒展开）。
- 【正则项】：xgboost加入了和叶子权重的L2正则化项，因而有利于模型获得更低的方差。
- 【缺失值处理】：xgboost增加了自动处理缺失值特征的策略。通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。
- 【并行化】：xgboost在进行分裂特征选择的时候支持并行计算「**特征维度上的并行**」
    - 在训练之前，每个特征按照特征值对样本进行预排序，并存储为block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分裂点时，可以利用多线程对每个block并行计算。
- 【列抽样】：XGBoost支持列抽样，与随机森林类似，用于防止过拟合。

## XGBoost模型

$$\hat{y_i} = \sum_{k=1}^Kf_k(x_i)$$

## 目标函数

由**损失项**加**正则项**组成

$$J=\sum_{i=1}^nL(y_i,\hat{y_i})+\sum_{k=1}^K\Omega(f_k)$$
- 其中$$\Omega(f_k)=\gamma T+\sum_{j=1}^T w_j^2$$，$$T$$为树的叶子节点个数，$$w_j$$表示第$$j$$个叶子节点的权重

## 目标函数的求解
> 如何进行Boost的？

- 采用【前向分布算法】：

$$
J^{(t)}=\sum_{i=1}^nL(y_i,\hat{y_i}^{(t)})+\sum_{k=1}^K\Omega(f_k)\\
=\sum_{i=1}^nL(y_i,\hat{y_i}^{(t-1)}+f_t(x_i))+\sum_{k=1}^{t-1}\Omega(f_k)+\Omega(f_t)
$$

- 采用【泰勒二阶展开】：

$$
J^{(t)}
=\sum_{i=1}^n[L(y_i,\hat{y_i}^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\sum_{k=1}^{t-1}\Omega(f_k)+\Omega(f_t)
$$

- 去除已知量（常量）$$L(y_i,\hat{y_i}^{(t-1)}), \sum_{k=1}^{t-1}\Omega(f_k)$$

$$
J^{(t)}=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)
$$

- 令$$I_j=\{i\lvert q(x_i)=j\}$$ 表示叶子节点$$j$$中的样本，并将$$\Omega(f_t) = \gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw^2_j$$代入

$$
J^{(t)}=\sum_{j=1}^T[\sum_{i\in I_j}g_iw_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\gamma T
$$

- 令$$G_j=\sum_{i\in I_j}g_i （叶子节点$$j$$的所有样本的损失函数一阶导数和）， H_j= \sum_{i\in I_j}h_i$$（叶子节点$$j$$的所有样本的损失函数二阶导数和）。

$$
J^{(t)}(w)=\sum_{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2]+\gamma T
$$

> XGBoost中叶子结点的权重如何计算出来?——
**目标函数对w求偏导并令其等于0求得**

- 求极值：
    - 目标函数对$$w$$求导，

    $$\frac{\partial J(w_j)}{\partial w_j}=G_j+(H_j+\lambda)w_j$$


    - 令$$\frac{\partial J(w_j)}{\partial w_j}=0$$得

    $$
    w_j^*=-\frac{G_j}{H_j+\lambda}
    $$

    - 代回$$J^{(t)}(w)$$得到

    $$
    J^{(t)}(w)=-\frac{1}{2}\sum_{j=1}^T\frac{G^2_j}{H_j+\lambda}+\gamma T
    $$

## 最佳分裂特征选择

### 分裂指标：Gain

$$Gain=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma$$

- 其中$$\frac{1}{2}[\frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]$$是指不考虑其他因素，通过分裂得到的增益，但实际上每次引入新叶子结点，都会带来复杂度的代价，因此需要再减一个$$\gamma$$

### 如何选择分裂特征
xgboost采用【特征并行】的方法进行计算选择要分裂的特征，即用多个线程。
- 对每个特征单独寻找最优分割点 （这里会运用到预排序算法）
- 再对根据这些最优分割点分别计算分裂后产生的增益，选择增益最大的那个特征作为分裂的特征，并对应使用其最优分割点

### 某特征分裂点位的选择

#### 全局扫描法
- 全局扫描法将所有样本该特征的取值按从小到大排列，将所有可能的分裂位置都试一遍，找到其中增益最大的那个分裂点
- 其计算复杂度和叶子节点上的样本特征不同的取值个数成正比。

#### 候选分位点法
- 候选分位点法是一种近似算法，仅选择常数个（如256个）候选分裂位置，然后从候选分裂位置中找出最优的那个。

### 分裂策略

【level-wise分类策略】：对每一层所有结点做无差别分裂，尽管部分结点的增益比较小，依然会进行分裂，带来了没必要的开销。

![image-1](/assets/img/post_img/level_w.png)

## 剪枝策略
- 【Gain的阈值】：XGboost采用预剪枝策略，也就是先尝试分裂，如果发现分裂后的Gain小于某个阈值就剪枝。
- 【正则项】：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度
- 【最小样本权重和】：当引入一次分裂后，重新计算新生成的左右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂
- XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。
## XGBoost中的一颗树停止生长条件
- 【超参数：树的最大深度】 树达到最大深度
- 当新引入一次分裂所带来的增益Gain<0时，放弃当前的分裂
- 【超参数：最小样本权重和】当引入一次分裂后，重新计算新生成的左右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂

## XGBoost防止过拟合的方法

- 目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化
- 列抽样：训练的时候只用一部分特征(不考虑剩余的Block块即可)
- 子采样：每轮计算可以不适用全部样本，使得算法更加保守
- shrinkage：学习率/步长,为了给后面的训练留出更多的学习空间。

## XGBoost如何处理不平衡数据
- 通过上采样，下采样
- 在意AUC，采用AUC评估模型的性能


## XGBoost如何评价特征的重要性
- weight 该特征在所有树中被用作分割样本的特征的总次数。
- gain :该特征在其出现过的所有树中产生的平均增益
- cover：该特征在其出现过的所有树中的平均覆盖范围。

# LightGBM (lgb)
- 基本原理与XGBoost一样，只是在框架上做了一优化（重点在模型的训练速度的优化）

## 相较于XGBoost的改进点
- 能适用于类别型数据，对于XGBoost它需要自己采用one-hot等方法进行处理
- 通过使用leaf-wise分裂策略代替XGBoost的level-wise分裂策略，通过只选择分裂增益最大的结点进行分裂，避免了某些结点增益较小带来的开销。
    - 在当前所有叶子结点中选择分类增益最大的结点进行分裂，并进行最大深度限制，避免过拟合。
![image-1](/assets/img/post_img/leaf_w.png)
- 另外LightGBM通过使用**基于直方图的决策树算法**，只保存特征离散化之后的值，代替XGBoost使用exact算法中使用的预排序算法（预排序算法既要保存原始特征的值，也要保存这个值所处的顺序索引），减少了内存的使用，并加速的模型的训练速度。
- 支持高效并行：除了能特征并行还能**数据并行**。
    - 数据并行：即让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。其使用分散规约（Reduce scatter）把直方图合并的任务分摊到不同的机器上，降低了通信量，并利用直方图做差，减少了通信量，即减少了通信时间。
    ![image-1](/assets/img/post_img/parrall.png)
    ![image-1](/assets/img/post_img/parrall1.png)

## lgb速度优化算法
### 基于直方图的排序算法
将连续浮点型特征数据离散化成$$k$$个整数，然后初始化一个宽度为$$k$$的直方图。接着遍历数据，将离散化后的值作为索引，累积统计量在直方图对应处。然后根据直方图的离散值，遍历寻找最优的分割点。此时只要再遍历$$k$$次即可。
- 优点：
    - 减少内存：因为直方图不需要额外存储预排序的结果，而且可以只保存特征离散化后的值。
    - 减少时间复杂度：由于将每个特征都离散化成了$$k$$，因此对于每个特征每次计算分割点增益的时候只需要遍历$$k$$次即可，使得复杂度从原来的$$O(\# data * \# feature)$$降低为$$O(k * \# feature)$$
    - 直方图能做差加速： 一个叶子的直方图可以由它的父亲结点的直方图与它兄弟的直方图做差得到

### 基于梯度的单边采样算法 （Gradient-based One-Side Sampling, GOSS）

#### 主要思想
【通过减少样本维度，加快速度】

梯度大的样本点在信息增益的计算上扮演着主要的作用，也就是说这些梯度大的样本点会贡献更多的信息增益，因此为了保持信息增益评估的精度，当我们对样本进行下采样的时候保留这些梯度大的样本点，而对于梯度小的样本点按比例进行随机采样即可

#### 算法流程
- 先根据模型得到预测值preds
- 根据preds计算出loss，并更进一步计算样本梯度
- 初始化样本权重均为1
- 根据样本梯度的绝对值，降序排序得到sorted
- 得到top N ($$a\times \# data$$)个作为大梯度样本 **topSet**
- 从剩余的样本里随机挑选rand N= ($$b\times \# data$$) 个作为小梯度样本，得到**randSet**；
- 将**topSet**和**randSet**进行合并，得到**usedSet**，
- 对于小梯度样本的权重乘上权重系数因子$$\frac{1-a}{b}$$，从而得到新的权重$$w'$$
- 从而得到了一个新的弱分类器：数据大小为$$(a+b)\times \# data$$的**usedSet**，对应的权重$$w'$$，梯度

> 为什么对梯度小的进行随机采样呢？

由于梯度小的样本产生的训练误差也很小，也就是说对于梯度小的样本来说是很好训练的。

> 为什么不丢弃所有梯度小的样本呢？

为了维持样本分布，防止伤害模型准确性。这里尽管会对梯度小的样本进行采样，但是会对采样的样本梯度乘以一个【常数因子】，将样本分布尽可能拉回来。
- 例如： 有100w条训练数据，设置梯度大的样本的保留10%，而梯度小的样本保留20%。
    - 则先对每条样本数据计算得到的梯度进行从大到小排序
    - 接着保留梯度处于前10%的样本数据
    - 对于剩下的90w条数据中，随机挑选20w(90w*20%)条样本数据保留
    - 注意：在实际操作中，我们会引入常数因子，对保留下来的小梯度样本的梯度进行放大
        - 如此题中a = 10%, b=20%,则常数因子 = $$\frac{1-a}{b}$$
        

### 互斥特征捆绑算法（Exclusive Feature Bundling, EFB）
#### 主要思想
【通过减少特征维度，加快速度】

将互斥的特征（很少同时去非0值）进行捆绑形成一个新的特征。

#### 实现方法
采用【构图（build graph）】思想。将特征作为节点，不互斥的特征之间进行连边，然后从图中找出所有的捆绑特征集合。

- EFB的特征捆绑的贪心策略流程：

（1）将特征作为图的顶点，对于不互斥的特征进行相连（存在同时不为0的样本），特征同时不为0的样本个数作为边的权重；

（2）根据顶点的度对特征进行降序排序，度越大表明特征与其他特征的冲突越大（越不太可能与其他特征进行捆绑）；

（3）设置最大冲突阈值K，外层循环先对每一个上述排序好的特征，遍历已有的特征捆绑簇，如果发现该特征加入到该特征簇中的冲突数不会超过最大阈值K，则将该特征加入到该簇中。否则新建一个特征簇，将该特征加入到新建的簇中。
# References

[【 GBDT算法】 机器学习实例推导计算+公式详细过程 （入门必备）](https://blog.csdn.net/weixin_41194171/article/details/85042720)

[GBDT理论推导总结](https://zhuanlan.zhihu.com/p/456801055)

[30分钟看懂XGBoost的基本原理 
](https://www.sohu.com/a/327017474_99979179)

[LightGBM官方文档](https://lightgbm.readthedocs.io/en/latest/Features.html)

[机器学习之LightGBM算法](https://www.cnblogs.com/hugechuanqi/p/10584602.html)

[详解LightGBM两大利器：基于梯度的单边采样（GOSS）和互斥特征捆绑（EFB）](https://zhuanlan.zhihu.com/p/366234433)