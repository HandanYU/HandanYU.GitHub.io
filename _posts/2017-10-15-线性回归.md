---
layout: post
title: 线性回归
summary: 本章从标准线性回归开始，根据其局限性提出局部加权线性回归，岭回归和LASSO回归
featured-img: 线性回归

---

##### Table of Contents  
- [标准线性回归](#标准线性回归)  
  - [最小一乘法](#最小一乘法)  
  - [普通最小二乘法](#普通最小二乘法)  
- [局部加权线性回归](#局部加权线性回归)  
  - [加权最小二乘法](#加权最小二乘法)  
- [岭回归](#岭回归)  
- [LASSO回归](#LASSO回归)  


<a name="标准线性回归"/>

## 标准线性回归
本质是已知m$$(x_i,y_i)$$个需要拟合一条型如$$y=\omega x+b$$的直线$l$。则可根据满足$l$尽可能接近$$(x_i,y_i)$$，根据点到直线距离公式$$\frac{|\omega x_i-y_i+b|}{\sqrt{\omega^2+1}}$$建立目标函数

$$\min\limits_{\omega,b}\sum_{i=1}^m\frac{|\omega x_i-y_i+b|}{\sqrt{\omega^2+1}}\Rightarrow最小一乘  \min\limits_{\omega,b}\sum_{i=1}^m|\omega x_i-y_i+b|\Rightarrow最小二乘\min\limits_{\omega,b}\sum_{i=1}^m(\omega x_i-y_i+b)^2$$

<a name="最小一乘法"/>

### 最小一乘法

$$\min\limits_{\omega,b}\sum_{i=1}^m|\omega x_i-y_i+b|$$

$$\widehat{W}$$的最优解为$$\widehat{W}\in\{w\|wx_i-y_i+b=0\}$$

<a name="普通最小二乘法"/>
### 普通最小二乘法(OLS)
- 目标函数为

$$\min\limits_{\omega,b}\sum_{i=1}^m(\omega x_i-y_i+b)^2
=\min\limits_{\widehat{W}}(X\widehat{W}-y)^T(X\widehat{W}-y)$$

定义$$F(\widehat{W})=(X\widehat{W}-y)^T(X\widehat{W}-y)$$
- 损失函数为

  $$||X\widehat{W}-y||^2 $$
  
其中$$\widehat{W}=(\omega,b)^T,X=(x_1,x_2,\dots,x_m,1),y=(y_1,y_2,\dots,y_m)$$

#### 求解
- 通过对$$F(\widehat{W})$$求关于$$\widehat{W}$$的一阶偏导为0，此时求得的$$\widehat{W}$$为驻点，可以得到$$\widehat{W}$$可能最优解为$$\widehat{W}^*=(X^TX)^{-1}X^Ty$$
- 为了验证上面求得的$$\widehat{W}^*$$是否是真正的最优解，也就是要验证该驻点是否是极小值点，因此需要对$$F(\widehat{W})$$求关于$$\widehat{W}$$的二阶偏导，如果结果为正定的，则说明该点为极小值点。

#### 线性回归基本表达式

$$\widehat{W}^*=(X^TX)^{-1}X^Ty$$

- 陷阱一：$$X^TX$$不可逆,也就是$$X$$不是列满秩或存在列与列直线相关共线的情况。为解决$$X^TX$$不可逆，我们可以通过改进最小二乘法中大损失函数来解决，也就是正则化处理。常用的改进方法有以下两种分别为岭回归和LASSO回归(Least absolution and selection operator)

- 陷阱二：当特征维数过高$$(X\in R^{m\times d})$$ ,其中𝑚 为样本个数,𝑑为特征维,求$$(X^TX)^{-1}$$的时间复杂度为$$O(d^3)$$太复杂了，因此采用梯度下降法进行求解

<a name="局部加权线性回归"/>

## 局部加权线性回归（LWLR）
由于线性回归会出现欠拟合现象，因此引入局部加权线性回归

<a name="加权最小二乘法"/>

### 加权最小二乘法
- 目标函数为

$$\min\limits_{\omega,b}\sum_{i=1}^mw_i(\omega x_i-y_i+b)^2$$

定义$J(\widehat{W})=\sum_{i=1}^mw_i(\omega x_i-y_i+b)^2$

- 损失函数为
$$w||X\widehat{W}-y||^2$$

​	其中$$w$$是以$$w_i$$为对角线上的值的对角矩阵，即权重矩阵

#### 求解
对$J(\widehat{W})$求关于$\widehat{W}$的一阶偏导为0，得到$\widehat{W}=(XwX)^{-1}X^Twy$，其中$w_i=exp(-\frac{(x_i-x)^2}{2\sigma^2})$我们成其为$Gaussian$核，，可以看出$x$越接近$x_i$，则$w_i$就会越大
#### 对$\sigma$的选取
- $$\sigma$$越小，逼近效果越好，越容易产生过拟合
- $$\sigma$越大，逼近效果越差，越容易产生欠拟合
- 则根据模型复杂度-误差图得到如下$\sigma$取值方法
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/291DCB2C0F164E848099222C1FA29B77/3585)

<a name="岭回归"/>

## 岭回归
- 主要思想：修改损失函数,添加惩罚项（L2正则化项  ），在最优解中$$X^TX$$处添加单位向量,把不可逆变成可逆。这里通过引入$$\lambda$$来限制了所有$$\omega$$之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫做缩减(shrinkage)
  
  $$X^TX->X^TX+\lambda I$$

- 目标函数为

$$\min\limits_{\widehat{W}}=||X\widehat{W}-y||^2+\lambda||\widehat{W}||^2$$

其中$$\lambda\|\widehat{W}\|^2$$为正则化项

- 损失函数为

$$||X\widehat{W}-y||^2+\lambda||\widehat{W}||^2$$

- 最优解

$$\widehat{w}_{\lambda}^*=(X^TX+\lambda I)^{-1}X^Ty$$

#### $$\lambda$$的选取
- 当$$\lambda$$越大，容易导致$$\|X\widehat{W}-y\|^2$$不太起作用，故容易产生欠拟合
- 当$$\lambda$$越小，故容易产生过拟合
- 首先抽一部分数据用于测试，剩余的作为训练集用于训练参数$$\omega$$。训练完毕后在测试集上测试预测性能。通过选取不同的λ来重复上述测试过程，最终得到一个使预测误差最小的$$\lambda$$

<a name="LASSO回归"/>

## LASSO回归
- 主要思想：进行特征选取。修改损失函数，添加L1惩罚项。【选特征，类似于AIC信息准则法】，选取X的子列矩阵$$X'$$使得$$(X')^T(X')$$可逆
- 目标函数为

$$\min\limits_{\widehat{W}}=\frac{1}{2}||X\widehat{W}-y||^2+\lambda|\widehat{W}|$$

其中$$P_{\lambda}(|\widehat{W}|)$$为惩罚项
- 损失函数为

$$J(\widehat{W})=\frac{1}{2}||X\widehat{W}-y||^2+\lambda|\widehat{W}|$$



#### 求解
可通过降维到一维$$\frac{1}{2}(\widehat{W}-y)^2+\lambda|\widehat{W}|$$进行求解

故将目标函数化简为

$$\min\limits_{\widehat{W}\in R} \frac{1}{2}(\widehat{W}-y)^2+\lambda|\widehat{W}|$$

根据$$\widehat{W}$$与0的关系分为以下两种情况
- $$\widehat{W}\geq 0$$

$$J(\widehat{W})=\frac{1}{2}(\widehat{W}-y)^2+\lambda\widehat{W}$$

通过对$$J(\widehat{W})$$求关于$$\widehat{W}$$一次偏导为0得到

$$\widehat{W}=y-\lambda$$


- $$\widehat{W}\leq 0$$

$$J(\widehat{W})=\frac{1}{2}(\widehat{W}-y)^2-\lambda\widehat{W}$$

通过对$J(\widehat{W})$求关于$\widehat{W}$一次偏导为0得到

$$\widehat{W}=y+\lambda$$
 
故通过以上两种情况可以得到$y$与$\widehat{W}$的关系
![image](https://note.youdao.com/yws/public/resource/cf6ffdff1ac9efd3d7ea23f51cdba86f/xmlnote/WEBRESOURCE2bccde0c54885603bed3fa37d3b80f34/3672)
