---
layout: post
title: 梯度下降和优化器
summary: 本章介绍了梯度下降法的概念，典型的梯度下降法的概念和优缺点，以及几种典型的优化器
featured-img: 梯度下降
---

##### Table of Contents  
- [梯度下降](#梯度下降)  
    - [（批量）梯度下降法(BGD)](#（批量）梯度下降法(BGD))
    - [随机梯度下降（SGD）](#随机梯度下降（SGD）)
    - [小批量梯度下降法(MBGD)](#小批量梯度下降法(MBGD))
- [优化器](#优化器)
    - [最基本的优化算法](#最基本的优化算法)
        - [Momentum](#Momentum)
    - [自适应参数的优化算法](#自适应参数的优化算法)
        - [Adagrad](#Adagrad)
        - [RMSprop](#RMSprop)
        - [Adam](#Adam)
- [优化算法的常用tricks](#优化算法的常用tricks)
<a name="梯度下降"/>

## 梯度下降

<a name="（批量）梯度下降法(BGD)"/>

### （批量）梯度下降法(BGD)
#### 1. 递推公式推导过程

通过多项式对函数进行逼近的方法得到

$$f(x)=f(x_0)+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\dots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$$

故

$$f(x)≈f(x_0)+f'(x_0)(x-x_0)$$

其中令$$\Delta x=x-x_0$$，则有

$$f(x)≈f(x_0)+\Delta x\nabla f(x_0)$$

则得到

$$f(x)-f(x_0)=\Delta x\nabla f(x_0)$$

其实$$\Delta x$$和$$\nabla f(x_0)$$都是向量，因此需要$$f(x)-f(x_0)$$最小，只有当$$\Delta x$$和$$\nabla f(x_0)$$两个方向相反，故有

$$\Delta x=-\alpha \nabla f(x_0)~~~~~~~\alpha >0$$

最终得到梯度下降法的递推公式为

$$x:=x-\alpha \nabla f(x)$$

#### 2. 求解最小二乘问题（采用梯度下降算法求解线性回归模型最优解）

- 线性回归模型

$$f_\omega(x)=\omega_0+\omega_1x+\dots+\omega_dx_d$$

其中d表示有d个特征.
- 损失函数

$$J(\widehat{\omega})=\frac{1}{2m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)^2$$

其中m表示有m个样本点.
- 使用梯度下降最小化损失函数，求解最优解

根据梯度下降递推公式有

$$\widehat{\omega}:=\widehat{\omega}-\alpha\frac{\partial{J(\widehat{\omega})}}{\partial{\widehat{\omega}}}$$

即

$$\widehat{\omega}:=\widehat{\omega}-\frac{\alpha}{m}X^T(X\widehat{\omega}-y)=\widehat{\omega}-\frac{\alpha}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$


#### 3. 优点
此时时间复杂度为$$O(md)$$,迭代次数少
#### 4. 缺点
每次迭代都要用到训练集所有的数据，因此当数据量大的时候迭代速度会很慢

<a name="随机梯度下降（SGD）"/>

### 随机梯度下降（SGD）
通过观察发现$$\frac{1}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$相当于是所有训练集样本点的均值

记$$z_i=(X_i\widehat{\omega}-y_i)X_i^T$$，每个样本点i选取的概率为$$\frac{1}{m}$$，则有

$$E_i(z_i)=\frac{1}{m}\sum\limits_{i=1}^m(X_i\widehat{\omega}-y_i)X_i^T$$

故随机梯度下降法的递推公式为

$$\widehat{\omega}:=\widehat{\omega}-\alpha(X_j\widehat{\omega}-y_j)X_j^T$$

#### 优点
每次迭代使用随机的一个样本来对参数进行更新，使得训练速度加快，此时的时间复杂度为$$O(d)$$

#### 缺点
1. 准确度下降，当目标函数为强凸函数的情况下，无法做到线性收敛
2. 可能会收敛到局部最优
3. 迭代次数比BGD多

<a name="小批量梯度下降法(MBGD)"/>

### 小批量梯度下降法(MBGD)
鉴于BGD,SGD的优缺点，提出每次迭代使用部分样本来对参数进行更新，故MBGD的迭代公式为

$$\widehat{\omega}:=\widehat{\omega}-\frac{\alpha}{\# J}\sum\limits_{j\in J}(X_j\widehat{\omega}-y_j)X_j^T$$

<a name="优化器"/>

## 优化器

<a name='最基本的优化算法'/>

### 最基本的优化算法

<a name="Momentum"/>

#### Momentum
##### 主要思想
借助物理中动量的概念，更新时在一定程度上保持之前更新的方向，同时利用当前的梯度微调最终的方向。

$$m_t = \beta m_{t-1}+ (1-\beta)g_t$$

其中$$\beta$$为超参数，$$m_{t-1}$$表示之前积累的动量，$$g_t$$表示当前梯度。当$$\beta$$越大，表明当前动量主要由以前积累动量的方向决定；当$$\beta$$越小，表示当前动量主要由当前梯度决定。

##### 主要解决的问题

- 引入了动量v，以指数衰减的形式累计历史梯度，以此来解决「Hessian矩阵病态问题」 
    - Hessian矩阵病态问题
        1. 矩阵病态问题
        
        求解方程组的时候，如果对矩阵数据有较小的改动，得到的结果出现很大的变化，这种问题成为矩阵病态问题。常常出现在该矩阵的向量之间存在相似性。
        2. Hessian矩阵
        
        一个多元函数的**二阶偏导数**构成的方阵，描述了函数的局部曲率
- 解决收敛过程产生的震荡问题
    - SGD出现震荡的原因是，它每次迭代**只记录前一次的梯度和当前梯度**。当当前梯度方向与前一次的梯度方向恰好相反，那么就会在该点附近出现大幅度震荡徘徊。
    - 而Momentum**积累了之前所有的动量梯度**，因此即便出现当前梯度方向与之前积累的梯度方向相反，只会减弱当前梯度。并且当当前梯度方向与之前积累的梯度方向相同的时候，该梯度趋势会被增强。从而也加快了收敛速度。
- 加速收敛


##### 递推公式
- step1:计算梯度$$\nabla f(\widehat{\omega})$$
- step2:冲量的梯度更新$$v_{k+1}=\alpha v_k-\eta\nabla f(\widehat{\omega})$$
- step3:参数的更新$$x_{k+1}=x_k+v_k$$


<a name='自适应参数的优化算法'/>

### 自适应参数的优化算法

该类优化算法主要特点为：每个参数有不同的学习率，在整个学习过程中自动适应这些学习率。

<a name="Adagrad"/>

#### Adagrad
引入二阶动量。
##### 递推公式
- step1:计算梯度$$\nabla f(\widehat{\omega})$$
- step2:累加平方梯度$$r_{k+1}=r_k+(\nabla f(\widehat{\omega}_k))^2$$
- step3:计算更新参数$$\widehat{\omega}_{k+1}:=\widehat{\omega}_k-\frac{\alpha}{\sqrt{r_{k+1}^2+\epsilon}}\nabla f(\widehat{\omega}_k)$$

##### 适用情况

有些参数已经近乎最优，因此只需要微调了，而另一些可能还需要很大的调整。这种情况可能会在样本较少的情况下出现，比如含有某一特征的样本出现较少，因此被代入优化的次数也较少，这样就导致不同参数的下降不平衡。adagrad就是来处理这类问题的。
##### 优点
不同的分量选取不同的学习率。在平缓的分量下降稍快，在陡峭的分量下降稍慢。
##### 缺点
累计梯度平方导致时间复杂度变大，迭代次数较多时，学习率过早过量减少

<a name="RMSprop"/>

#### RMSprop
##### 递推公式
- step1:计算梯度$$\nabla f(\widehat{\omega})$$
- step2:加权累加平方梯度$$r_{k+1}=\rho r_k+(1-\rho)(\nabla f(\widehat{\omega}_k))^2$$,其中$$\rho$$表示学习率的衰减速率一般取0.9
- step3:参数更新$$\widehat{\omega}_{k+1}:=\widehat{\omega}_k-\frac{\alpha}{\sqrt{r_{k+1}+\epsilon}}\nabla f(\widehat{\omega}_k)$$

##### 优点
将“历史梯度平方和”转化为“指数衰减的移动平均”，从而解决Adagrad的学习率逐渐消失的问题

#### Adam
结合Momentum和RMSProp算法。

##### 优点
收敛速度快，参数自适应，不需要调参。适用于稀疏矩阵

##### 缺点
- 容易错过全局最优解
- 有时候无法收敛

<a name='优化算法的常用tricks'/>

# 优化算法的常用tricks（参考[博客](https://blog.csdn.net/qq_32907491/article/details/107442503))

- 首先，各大算法孰优孰劣并无定论。如果是刚入门，优先考虑 SGD+Nesterov Momentum或者Adam.（Standford 231n : The two recommended updates to use are either SGD+Nesterov Momentum or Adam）
- 选择你熟悉的算法——这样你可以更加熟练地利用你的经验进行调参。
- 充分了解你的数据——如果模型是非常稀疏的，那么优先考虑自适应学习率的算法。
- 根据你的需求来选择——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。
- 先用小数据集进行实验。有论文研究指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。（The mathematics of stochastic gradient descent are amazingly independent of the training set size. In particular, the asymptotic SGD convergence rates are independent from the sample size. [2]）因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。
- 考虑不同算法的组合。先用Adam进行快速下降，而后再换到SGD进行充分的调优。切换策略可以参考本文介绍的方法。
- 数据集一定要充分的打散（shuffle）。这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。
- 训练过程中持续监控训练数据和验证数据上的目标函数值以及精度或者AUC等指标的变化情况。对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。
- 制定一个合适的学习率衰减策略。可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。
