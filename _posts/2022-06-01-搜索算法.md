---
layout: post
title: 搜索算法
summary: 本章主要总结了面试中涉及到的机器学习相关的一些问题。
featured-img: machine learning
language: chinese 
category: AI
---

#  状态搜索 （State-based Search)

# 1. Satisficing Planning vs. Optimal Planning

- 两者几乎是对立面，没有重合部分
- 对于很多问题对于satisficing planner很简单，但要对于optimal planner来说很难

## 1.1 Satisficing Planning

所有能够被搜索到的结果都OK，（即使我们更希望更lowest cost）

## 1.2 Optimal Planning

必须是最短路径(lowest cost)

# 2. 基本状态模型(Basic State Model/Classical Planning)

## 2.1 目标

写出一个能够解决所有经典搜索问题的程序

## 2.2 参数定义

- 状态模型(State Model)：S(P)
- 有限(finite)离散型(discrete)的状态空间(state space)：S
- 已知的初始状态(initial state)：$s_0$, $s_0  \in  S$
  - 只可能是一个
- 目标状态(goal states)：$S_G$, $S_G \subseteq S$
  -  [==注意==：由于目标状态可以不止一个，因此一般用集合形式来表示]
- 某个状态s下可执行操作(actions applicable in s)：$A(s) \subseteq  A, s\in S$
- 确定性转移函数(deterministic transition function)：$s'=f(a, s)$ for $a\in A(s)$
- 正的行动代价(positive action costs)：c(a, s)
- 结果(solution)：一个可执行序列，将$s_0$映射到$S_G$

#### 2.2.1 用state model表示$$m\times m$$ Manhattan Grid

Consider a $$m\times m$$ manhattan grid, and a set of coordinates $$V$$ to visit in any order, and a set of inaccessible coordinates (walls) $$W$$. Using Strips, a state-space model can be represented as $$P=<S,s_0,S_G,A,T,C>$$ 

*    $S = \{ \langle x , y, V' \rangle \mid x, y \in \{0,\dots,m-1\} \, \land \, V' \subseteq V \}$

*    $s_0 = \langle (0,0),V \setminus \{(0,0)\} \rangle$ 

*    $S_G = \{ \langle (x,y),\{\} \rangle \mid x,y \in \{0,\dots,m-1\} \}$ 

*    $A(\langle x , y, V' \rangle) = \{ \left( dx , dy \right) \mid $

     *   $ dx, dy \in \{-1, 0, 1\} $
     *   $\land \ |dx| + |dy| = 1 $
     *   $\land \ x + dx, y + dy \in \{0,\dots,m-1\}$
     *   $(x+dx,y+dy) \notin W \ \} $

*    $T(\langle x , y, V' \rangle,\left(dx, dy\right)) = \langle x+dx , y+dy, V' \setminus \{(x+dx, y+dy)\} \rangle $

*    $c(a, s) = 1$

## 2.3 状态模型的求解(Path-finding in graphs)

搜索算法：利用「经典状态模型」与「有向图」之间的关系

### 2.3.1 有向图与状态模型的对等关系

- 有向图的结点node -> 状态模型中的状态s
- 有向图的边edge (s, s') -> 状态模型中具有相同cost的转移(transition)

# 3. 搜索算法的分类

## 3.1 盲目式搜索 (Blind Search) Vs 启发式搜索(Heuristic Search)

### 3.1.1 盲目式搜索 (Blind Search)

- 只使用基本的原始信息
- 优点：简单直接，不需要额外信息
- 缺点：计算量大，效率低
- **e.g.:**
- DFS: Stack : Last-in-First-out
- BFS: Queue: First-in-First-out
- Dijkstra(Uniform Cost Search): Priority Queue: First-in-First-out, also being sorted so that can get the smallest one each time
- ID(Iterative Deepening)

### 3.1.2 启发式(信息)搜索 (Heuristic/Informed Search)

- 额外使用启发式方程
- 当在**satisficing planning**中，有远超于Blind Search的出色表现
- 当在**optimal planning**中，效果和Blind Search差不多
- e.g.:
  - 系统搜索 (Systemetric Search)
    -  Greedy best-first search (Satisficing Planning)
    -  Weighted A* (Satisficing Planning)
    -  A* (Optimal Planning)
    -  IDA*
  - 局部搜索 (Local Search)
    - Hill Climbing
    - Enforced hill-climbing (Satisficing Planning)
    - Genetic algorithms

#### 启发式方程 (Heuristic Function)

#### 定义

- 令$h$表示启发式方程
- $h(s)$ 表示状态s的启发值(heuristic value / h-value)，评估从状态s到最近一个目标状态的距离值
- $h^*$表示optimal/perfect heuristic （可以理解为real following cost)
- $h^*(s)$表示从状态s到目标状态的optimal/perfect heuristic

- 一个好的启发式方程

  - 衡量启发的信息和计算开销 Heuristic performance is always a balance between how well it directs the search (informedness) and how long it takes to compute (computation overhead)

#### 性质

如果某个启发式方程满足以下一个条件，则称其为对应的启发式方程。

- 安全性 (Safe)
  - $h^*(s)=\infin$ if $\forall s\in S, h(s)=\infin$。即从状态s到任意目标状态不可达，则$h^*(s)=\infin$
  - AKA：If a solution exists from state s, then $h(s) < \infin$
  - not safe:  then it can assign $h(s)=\infin$ to all nodes $n$ that lead to a solution
- 目标察觉性 (Goal-aware)
  -  $\forall s\in S_G, h(s)=0$
- 可采纳性 (Admissible)
  - $\forall s \in S, h(s) \leq h^*(s)$. 因为$h(s)$表示的到**最近**一个目标状态的距离值
- 一致性 (Consistent)
  -  for all transition $s \stackrel{a}{\longrightarrow}s'$, $h(s)\leq h(s')+c(a)$
- 主导性 (Dominate)
  - 假设有两个heuristic functions **h1,h2**，$$\forall s\in S, h_1(s) > h_2(s)$$，则说$$h_1$$ dominates $$h_2$$


**如果 h 具有一致性和目标察觉性，那么 h 具有可采纳性。如果 h 具有可采纳性，那么 h 具有目标察觉性和安全性。除此以外，不存在任何其他关系。**

- Consistent + Goal-aware = Admissible
- Admissible ->  Goal-aware & Safe

#### Greedy Best-First Search

```python
# Step1: 初始化openList, closedList，分别存放待扩展结点和已扩展遍历的结点。
openList := a new priority queue ordered by ascending h(state(node))
openList.insert(start())

closedList := an empty set

# 判断openList是否为空，如果为空，则返回搜索失败，即目标状态不可达。否则从openList中取出优先级最高的
while not openList.empty():
  	node = open.pop() # 取出优先级最高的
    # 检验是否是已经被扩展过的结点 check duplicates
    if state(node) not in closedList:
      	closedList.add(node)
    # 判读是否是goal state
    if is-target(state(node)):
      	return 'success'
    # 扩展结点：遍历当前结点的后继结点
    for a, s_next in succ(state(node)):
      	# 检验是否是已经被扩展过的结点 check duplicates
      	if (s_next not in closedList) and (s_next not in openList):
      			node_next = (node, a, s_next)
        if h(state(node_next)) < inifite: # 预判从node_next出发，是否可达目标结点
          	openList.insert(node_next)
 return 'unsolvable'
      
    
```

##### evaluation function

h

##### Properties

- 可以简单的这样理解：对于一个棋盘格中，它总是尝试向离目标节点更近的方向探索，怎样才算离目标节点更近呢？在只能上下左右四方向移动的前提下，我们通过计算当前节点到目标节点的距离来进行判断。

- Completed. 当检测duplicates且启发式方程是safe的

- 不能保证是optimal的，但搜索速度很快

- 当启发式方程发生严格单调变化的时候也不会发生变化（Does not vary with monotonic changes）

- with duplicate detection

- 数据结构：优先队列(e.g., min heap)，**根据$h(state(node))$升序排列**

  - 当假设$\forall s\in S, h(s)=0$，由于此时所有结点的$h$都一致了没有大小顺序可言，因此对于openList的顺序，我们有以下几种假设

    - FIFO: 也就是假设按照“先进先出”原则，则Greedy Best-First Search会退化为BFS

    - LIFO：也就是假设按照“后进先出”原则，则Greedy Best-First Search会退化为DFS

    - 根据 $g$​的大小，则Greedy Best-First Search会退化为Uniform-cost search((Dijkstra)

      

#### A*

```python
openList := a new priority queue ordered by ascending g(state(node)) +  h(state(node))
openList.insert(start())

closedList := an empty set
best_g_mapping := a dict mapping states to g-values

# 判断openList是否为空，如果为空，则返回搜索失败，即目标状态不可达。否则从openList中取出优先级最高的
while not openList.empty():
  	node = openList.pop()
    # 判断node是否已在closedList
    if state(node) not in closedList:
       # 判断 node 是否已在openList，如果不在如果在openList需要考虑是否re-open
        if state(node) not in openList or g(state(node)) < best_g_dict(state(node)):
            closedList.add(state(node))
            best_g_dict(state(node)) = g(state(node))
         if is-target(state(node)):
          	return 'success'
         for a, s_next in succ(state(node)):
          	node_next = (node, a, s_next)
            if h(state(node_next)) < infty:
              	openList.insert(node_next)
        
return 'unsolvable'
```

##### 相关术语

- Evaluation function: $f(s) = g(s) + h(s)$
  - g(s): 从起点到s经过的代价 previous/historical costs
  - h(s)：从s到最近一个目标状态的代价 feature costs
  - f-value越小，在优先队列中的优先权越大
  - openList按照f-value升序排列
  - if h(s) === 0, -> Uniform-cost search  （e.g. Dijsktra)
- 生成结点 (Generated Nodes)：Nodes inserted into **openList** ，即由当前结点扩展生成的后继结点
- 扩展结点 (Expanded Nodes)：Nodes pop from **openList**，即从openList中pop出来的优先级最高的结点（用于生成Generated Nodes的结点)
- 重扩展结点 (Re-expanded Nodes)/重开放结点 (Re-open Nodes)：扩展那些$s\in$closedList的结点

#####  Evaluation function

$f(s) = g(s) + h(s)$

##### 性质

- 对于具备安全性(safe)的h： Completed。【因为当h不具备安全性，也就是我们可以假设$\forall s \in S, h(s)=\infin$。这样就导致我们在一开始就终止了search，从而无法找到solution】

- 对于具备可采纳性(admissible)的h：Optimal。【因为当h不具备可采纳性，也就是我们可以假设$\exist s\in S, h(s) > h^*(s)$，这样就会使得我们错过在optimal path上的node，而选择了其他的path. 】

- 对于具备可采纳性(admissible)和一致性(Consistent)的h： A*不需要进行re-open能Optimal

- 当h是admissible但inconsistent的时候, A*不一定optimal

#### WA*

```python
openList := a new priority queue ordered by ascending g(state(node)) + W * h(state(node))
openList.insert(start())

closedList := an empty set
best_g_dict := {}

while not openList.empty():
  	node = openList.pop()
    if state(node) not in closedList or g(start(node)) < best_g_dict(state(node)):
      	closedList.add(state(node))
        best_g_dict(state(node)) = g(state(node))
        if is-target(state(node)):
          	return 'success'
        for a, s_next in succ(state(node)):
          	node_next = (node, a, s_next)
            if h(state(node_next)) < infty:
              	openList.insert(node_next)
return 'unsolvable'
```

##### Evaluation  function 

g + w*h

- Parameter: the weight **W** 
  - W = 0: 一致代价搜索(Dijkstra)
  - W=1:  WA* -> A*
  - W=$\infin$：WA* -> Greedy Best-First Search 【因为此时g的值对evaluation function来说影响不大，因此就只留下h，退回到了GBFS】

##### 性质

- 当**W > 1**， WA*是有界次优的(bounded suboptimal)。因为当h是可采纳性的(admissible)，于是返回结果不会超过最优解的W倍
