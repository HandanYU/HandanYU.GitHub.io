---
layout: post
title: Topic Model
summary: Learn about Topic Model
featured-img: deep learning
language: chinese
category: NLP
---
![image1](/assets/img/post_img/156.png)
# Topic Model

Topic models learn common, overlapping themes in a document collection 

## 0. Assumption

- 每个文档包含多个主题；
- 每个主题包含多个单词

## 1. Latent Semantic Analysis (LSA)

![image1](/assets/img/post_img/157.png)

### 1.1 

### 1.2 Issue

- Positive and negative values in the U and $V^T$
- Difficult to interpret

## 2. Probabilistic LSA (PLSA)

$$P(w,d) = P(w\lvert d)P(d)=P(d)\sum_tP(w\lvert t)P(t\lvert d)$$

- $$P(w,d)$$: Joint probability of words 
- $$P(w\lvert t)$$: word distribution for a topic
- $$P(t\lvert d)$$: topic distribution for a document
- $$t$$: the number of topics in a document

### 2.1 Pros

- No more negative values! 
- PLSA can learn topics and topic assignment for documents in the train corpus 

### 2.2 Cons

- But it is unable to infer topic distribution on **new documents** 
- PLSA needs to be re-trained for new documents

## 3. Latent Dirichlet Allocation (LDA)

- Introduces a prior to the **document-topic** and **topic-word** distribution

- **Fully generative**: trained LDA model can infer topics on **unseen** documents!
- LDA is a **Bayesian** version of **PLSA**

### 3.1 Core idea

- assume each document contains **a mix of topics**
- the topic structure is **hidden (latent)**
- LDA infers the topic structure given the observed words and documents 
- LDA produces **soft clusters** of documents (based on topic overlap), rather than hard clusters
- Given a trained LDA model, it can **infer topics on new documents** (not part of train data)

![image1](/assets/img/post_img/158.png)

### 3.2 Training Model

#### 3.2.1 Input

- A collection of documents 
- Bag-of-words 
- Good preprocessing practice
  - Remove stopwords 
  - Remove low and high frequency word types 
  - Lemmatisation

#### 3.2.2 Output

##### Topics 

 distribution over words in each topic

![image1](/assets/img/post_img/159.png)

##### Topic assignment

distribution over topics in each document

![image1](/assets/img/post_img/160.png)

#### 3.2.3 Learning

> How do we learn the latent topics?

##### Variational methods 

##### Sampling-based methods —— （e.g., Gibbs)

- Step1: Randomly assign topics to all tokens in documents

![image1](/assets/img/post_img/161.png)

- Step2: Collect **topic-word and document-topic co-occurrence statistics** based on the assignments 

  - Initialize topic-word matrix and document-topic matrix (hyper-parameter: $$\beta, \alpha$$ for each matrix)

![image1](/assets/img/post_img/162.png)

  - update these two matrix by counting

    - 对于 mouse，由于它被assign为t1有1次，所以(t1, mouse) = 0.01 + 1 = 1.01。被assign为t3有2次，所以(t3, mouse) = 0.01 + 2 = 2.01
    - 对于d1，由于它有2个tokens被assign为t1，所以(d1, t1) = 0.1 + 2 = 2.1

![image1](/assets/img/post_img/163.png)

- Step3: Go through every word token in corpus and sample a new topic (**re-assign topic**)

  - taking d1的 'mouse' for example，re-assign the topic of 'mouse' for d1

  - 首先对以上表格进行修改调整，由于此时mouse的topic是未知的，因此与它有关的count需要减掉

   ![image1](/assets/img/post_img/164.png)

  - 假设d1的mouse属于$$t_i$$，则有概率 $$P(t_i\lvert 'mouse', d_1) =P(t_i\lvert 'mouse')\times P(t_i\lvert d_1) $$

    - e.g., $$P(t_1\lvert 'mouse', d_1)= P(t_1\lvert 'mouse')\times P(t_1\lvert d_1)=\frac{0.01}{0.01+0.01+2.01}\times \frac{1.1}{1.1+1.1+2.1}$$

  - 根据[$$P(t_1\lvert 'mouse', d_1), P(t_2\lvert 'mouse', d_1), P(t_3\lvert 'mouse', d_1)$$]，randomly sample得到d1 'mouse'的topic

- Step4: 更新 **organge table**， 重新进入step2,3， 不断迭代更新，直到收敛

#### 3.2.4 When to stop

**model probability** of training set becomes stable

##### model probability

$$\log P(w_1, w_2, \dots, w_m)=\log\sum_{j=1}^TP(w_1\lvert t_j)P(t_j\lvert d_{w_1})+\dots +\log \sum_{j=1}^TP(w_m\lvert t_j)P(t_j\lvert d_{w_m})$$

**Derivation**

$$P(w_1, w_2, \dots, w_m)=\prod_{i=1}^mP(w_i)=\prod_{i=1}^m \sum_{j=1}^TP(w_i\lvert t_j)P(t_j\lvert d_{w_i})$$

$$\log P(w_1, w_2, \dots, w_m)=\sum_{i=1}^m \log\sum_{j=1}^TP(w_i\lvert t_j)P(t_j\lvert d_{w_i})$$

- 其中$$P(w_i\lvert t_j)$$是基于topic-word co-occurrence matrix (**blue table**)
- $$P(t_j\lvert d_{w_i})$$基于 document-topic co-occurrence matrix (**green table**)

### 3.3 Test : Infer Topics For New Documents

fix **topic-word matrix** (blue table), update **document-topic matrix** (green table)

- Go through all the tokens in the test document, and assign a topic by sampling from the topics

\1) Randomly assign topics to all tokens in new document (pickled peppers popped), like pickled/t1 peppers/t2 popped/t3

\2) Update document-topic matrix based on the assignments; but use the trained topic-word matrix (kept fixed) 

\3) Go through every word in the test document, i.e. (pickled peppers popped) and sample topics 

\4) Go to step 2 and repeat until convergence, i.e., model probability becomes stable

### 3.4 Hyper parameters

- The number of topic $$T$$
- $$\beta$$:  prior on the topic-word distribution
- $$\alpha$$: prior on the document-topic distribution
  - 通常希望large，(> 0.1)
- $$k$$: smoothing in LM
- initial value of co-occurrence matrix: 通常用$$\beta, \alpha$$代替

#### 3.4.1 Influence

- Higher prior values -> flatter distribution (image: a very very large value would lead to a uniform distribution)

 ![image1](/assets/img/post_img/165.png)

- Lower prior values -> peaky distribution

![image1](/assets/img/post_img/166.png)

- 因此对于$$\beta$$，我们总是希望它小一些，因为对于一个topic，我们希望它only focus on some specific words，也就是peaky distribution

- 而对于$$\alpha$$，我们希望它大一些，因为对于一个document，我们希望能够得到多个topics，也就是flatten distribution

### 3.5 Evaluation

由于这是unsupervised learning

#### 3.5.1 Intrinsic evaluation

直接根据topic model得到

##### model logprob

$$\log P(w_1, w_2, \dots, w_m)=\sum_{i=1}^m \log\sum_{j=1}^TP(w_i\lvert t_j)P(t_j\lvert d_{w_i})$$

##### perplexity

$$ppl=\exp^{\frac{-\log L}{W}}$$

- Issues:
  - More topics = better (lower) perplexity
  - Smaller vocabulary = better perplexity 
  - Does not correlate with human perception of topic quality

##### Topic Coherence

- Measure how **coherent** the generated topics

- A good topic model is one that generates more coherent topics

- How to measure coherence

  - **Word Intrusion**

    - 插入一个random word到topic中

![image1](/assets/img/post_img/167.png)

    - 人工来猜测哪个word是 **intruder word** （不合群的word)

    - 如果猜对了，说明这个topic是coherent的

    - 问题是这需要manual effort

  - **PMI**

    - High PMI for a pair of words -> words are correlated
    - If all word pairs in a topic has high PMI -> topic is coherent
    - If most topics have high PMI -> good topic model 
    - 如何获取word co-occurrence statistics 
      - A better way is to use an external corpus (e.g. Wikipedia)
    - $$PMI=\sum_{j=2}^N\sum_{i=1}^{j-1}\log \frac{P(w_i, w_j)}{P(w_i)P(w_j)}$$

    - e.g., 计算topic:  {farmers, farm, food, rice, agriculture} 的PMI

    - **Coherence** = sum PMI for all word pairs: PMI(farmers, farm) + PMI(farmers, food) +… + PMI(rice, agriculture)
    - 缺点：focus on rare word pair

  - **Variable PMI**

    - Normalised PMI: $$NPMI=\sum\sum \frac{\log\frac{P(w_i, w_j}{P(w_i)P(w_j)}}{-\log P(w_i, w_j)}$$
    - Conditional probability: $$LCP =\sum\sum\log\frac{P(w_i,w_j)}{P(w_i)}$$

#### 3.5.2 Extrinsic evaluation

根据其downstream task来evaluate

