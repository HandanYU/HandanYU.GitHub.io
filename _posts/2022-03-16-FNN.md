---
layout: post
title: 前馈神经网络在NLP中的应用
summary: 本篇文章主要讲解了前馈神经网络在NLP中的应用，包括基于前馈神经网络的主题分类(Top Classification)，基于前馈神经网络的语言模型(FFNN-LM)以及基于前馈神经网络的POS Tagging.
featured-img: deep learning
language: chinese
category: NLP
---

# 前馈神经网络 Feed-forward NN
首先我们先来简单回顾一下在「机器学习」模块中提到过的关于前馈神经网络重要知识点。

前馈神经网络又称为多层感知机 multilayer perceptrons。由一个输入层、一个输出层以及多个隐藏层组成，并且层与层之间是one-by-one的全连接(fully-connected)，因此它还有一个别名就是全连接神经网络(fully-connected neural network)。另外层与层之间都运用了一个非线性的激活函数。

使用**梯度下降**的方式进行优化训练，并且使用**Error backpropagation, backward differentiation**计算损失函数梯度。

另外关于前馈神经网络学习过程其实是一个**non-convex**的优化过程。很容易造成过拟合(overfitting)，因此常常采用**Dropout**以及**Regularisation**方法来避免。那接下来就来介绍一下Dropout和Regularisation

## Dropout

顾名思义Dropout是一种剔除操作，假设定义在某一层的dropout rate=0.1，表示将该层中10%的神经元的值令为0，相当于杀死这10%个神经元。因为当该神经元值为0后，在之后一层中经过权重相乘后还是0，因此这个神经元的信息将不会带到到后面的网络层。也正因为如此Dropout操作一般使用于隐藏层，因为对于输入层输出层的每个神经元信息都十分重要，不能随便丢弃一个。

总结Dropout的作用就是：

## Regularisation
常见的正则化有**L1-norm**和**L2-norm**

# 主题分类 Topic Classification
主题分类就是给定一个document，判断它属于哪一个topic （e.g., economy, politics, sports)。因此很明显这是一个分类问题，也就是可以采用FFNN来解决。

## 输入特征
常见的有以下几种
- bag-of-words / bag-of-unigrams
- bag-of-bigrams
- TF-IDF: weighted word counts

我们下面来看一个例子，方便理解
假设我们有3个documents，并且已知它们的bag-of-words表如下，我们的concept是判断它们分别属于economy, politics, sports中的哪一类document。

另外我们建立的前馈神经网络如下图，拥有两个隐藏层。由于一共有4类单词（也就是vocabulary的大小为4），因此输入层的大小为4。并且由于predefined的类别为economy, politics, sports，一共有3个类别，因此输出层的大小为3。
## Training

$$h_1 = \sigmoid(W_1x + b_1)$$

$$h_2 = \sigmoid(W_2h_1 + b_2)$$

$$\hat{y} = softmax(W_3h_2 + b_3)$$

- 其中$$W_i, b_i$$是randomly seleted。
- $$x$$表示一个document的bag-of-words信息。则我们选择doc1，此时x=[0,2,1,2]，假设最终$$\hat{y}$$=[0.1, 0.6, 0.3]，而真实doc1属于sports（即y=[0, 0, 1])。

则此时的损失函数为$$L = -\log(0.3)$$。由于FFNN的损失函数选择的是**cross-entropy**，$$L=\sum_i y_i \log(\hat{y_i})$$。

## Prediction

$$h_1 = \sigmoid(W_1x + b_1)$$

$$h_2 = \sigmoid(W_2h_1 + b_2)$$

$$\hat{y} = softmax(W_3h_2 + b_3)$$

- 此时的$$w_i, b_i$$是使用经过training后得到的最优$$W_i, b_i$$。
- 这里我们选择doc3作为测试样本，则x=[0, 3, 2, 1]。经过上述计算得到$$\hat{y} = [0.2, 0.1, 0.7]$$，则我们说doc3的预测所属类别为sports。

# Feed-forward NN Language Model

我们可以先回忆一下Language Model，它的concept是通过n个previous words，推测第n+1个word最有可能是vocabulary中的哪个。

- 目标函数为(以bigram为例)：$$arg\max_{w_i\in W} P(w_i|w_{i-2}, w_{i-1})$$
- 输入$$w_{i-2}, w_{i-1}$$
- 输出$$w_i$$

对比上面**Topic classification**任务，我们可以想象，能否将输入的$$w_{i-2}, w_{i-1}$$合并为一个Matrix作为输入值，然后最终输出的是所有candidate words的概率，也就是[$$P(a_1\|w_{i-2}, w_{i-1}),P(a_2\|w_{i-2}, w_{i-1}), \dots, P(a_n\|w_{i-2}, w_{i-1})$$]。然后选择概率最高的作为最终predict的$$w_i$$。

显然这是符合原始问题的需求的，但这里有一个需要解决的问题就是如何将$$w_{i-2}, w_{i-1}$$合并为一个Matrix，于是我们引入**word embedding**

## word embedding

- 将离散的word symbols转为continuous vectors。
- Pons: 能让model捕捉到word与word之间的相似性
- 每个word对应的word embedding vector一般是一个列向量
- 如何得到每个word的对应word embedding vector呢？
    - 首先我们准备一个vocabulary的pre-trained的word embedding matrix (the embeddings E)，size = d$$\times$$ ||V||。
    - 然后将每个word转为one-hot形式作为输入值X，size = ||V||$$\times$$ 1，其中index是word在这个vocabulary中的index的值为1，其余都为0。
    - 最后通过embedding layer，也就是将E与X进行矩阵相乘，得到这个word对应的embedding vector。
    - 如果是对于多个words，则将每个word单独一一进行以上三部操作得到对应的embedding vector。然后再将它们按列concat起来得到$$d\times n$$的矩阵

## Training

$$h_1 = \sigmoid(W_1X + b_1)$$

$$\hat{y} = softmax(W_2h_1 + b_2)$$

其中X就是上面提到的经过embedding layer后得到的$$d\times n$$的矩阵