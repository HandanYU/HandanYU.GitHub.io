---
layout: post
title: 文本处理 (Text Processing)
summary: 本篇文章主要讲解了NLP中文本处理的一些基本概念以及处理方法。并且详细介绍了BPE算法
featured-img: deep learning
language: chinese
category: NLP
---

# 文本处理 (Text Processing)

## 术语 (Terminology)

### Words

sequence of characters with a meaning and/or function

### Sentence

consist of one or more words

### Document

contains one or more sentences

### Corpus

a collection of documents

### Word token

each instance of a word

- e.g., sentence: 'I have a dog and he has a cat'. 
  - There are **9** word tokens in the sentence. Specifically, they are 'I' ,'have', 'a', 'dog', 'and', 'he', 'has', 'a', 'cat'.

### Word type 

unique words, *distinct* words

- e.g., sentence: 'I have a dog and he has a cat'. 
  - There are **8** word types in the sentence. Specifically, they are 'I' ,'have', 'a', 'dog', 'and', 'he', 'has', 'cat'.

## 处理步骤 (Processing Steps)

### **Remove unwanted formatting** 

e.g. HTML

### **Sentence segmentation**

- break documents into sentences

- e.g., "I am a student. I come from China." -> ["I am a student.",  "I come from China."]
- approach 1: break on **sentence punctuation**
- approach 2: use **regex**
- approach 3: use **lexicons**
- approach 4: use **machine learning**
  - Features:
    - Word shapes: Uppercase, lowercase, all capital, number, character length, etc.
    - Part-of-speech tags：（词性tag）

### **Word tokenisation**

- break sentences into words 

- e.g., "I am a student." -> ['I', 'am', 'a', 'student.']

#### Subword Tokenisation

popular algorithm: **byte-pair encoding (BPE)**

##### Advantages

- Language-independent
- work well on unkown words, via splitting them into individual letters

##### Disadvantages

- Larger vocabulary
- Creates non-sensical subwords
- Unable to handle misspellings

##### byte-pair encoding (BPE)

###### core idea

 iteratively merge **frequent pairs** of characters 

###### steps

- e.g., text: $$'low '*5 +'lower '*2+'newest '*6 +'widest '*3$$

- Get the word frequency of the text.

  - ```
    {'l o w </w>': 5,
     'l o w e r </w>': 2,
     'n e w e s t </w>': 6,
     'w i d e s t </w>': 3}
    ```

- Get the tokens' frequency of the text. 

  - ```
    {'l': 7,
    'o': 7,
    'w': 16,
    '</w>': 16,
    'e': 17,
    'r': 2,
    'n': 6,
    's': 9,
    't': 9,
    'i': 3,
    'd': 3}
    ```

- Get the frequency of the token-pairs of the text

  - ```shell
    {('l', 'o'): 7,
    ('o', 'w'): 7,
    ('w', '</w>'): 5,
    ('w', 'e'): 8,
    ('e', 'r'): 2,
    ('r', '</w>'): 2,
    ('n', 'e'): 6,
    ('e', 'w'): 6,  
    ('e', 's'): 9,  ## the most frequent 
    ('s', 't'): 9,
    ('t', '</w>'): 9,
    ('w', 'i'): 3,
    ('i', 'd'): 3,
    ('d', 'e'): 3}
    ```

- Find the most frequent token-pair

- Merge the most frequent token-pair as a single token

  - ```
    {'l o w </w>': 5,
     'l o w e r </w>': 2,
     'n e w es t </w>': 6,
     'w i d es t </w>': 3}
    ```

- Change the frequency count of the token of the text

  - add the new token's count

  - deduct the count of the characters in the most frequent token-pair 

  - ```shell
    {'l': 7,
    'o': 7,
    'w': 16,
    '</w>': 16,
    'e': 17-9=8, ## deduct
    'r': 2,
    'n': 6,
    's': 9-9=0, ## deduct
    't': 9,
    'i': 3,
    'd': 3,
    'es': 9 ## add
    }
    ```

- Generate word vocabulary using BPE

  - via iteratively finding the most frequent pair-tokens and merging them

- Use the generated word vocabulary to tokenise the sentence

  - get the latest token vocabulary of the latest word vocabulary and sorted by the frequency of the token **in descending order** firstly and then the length of token **in descending order**
  - Get the tokens' frequency of the vocabulary and get the tokens with respect to each word in the vocabulary
  - tokensize the words in the given sentence
    - as for the word in the token vocabulary
      - do nothing
    - as for the word does not occur in the token vocabulary
      - 通过用token vocabulary中的token一个一个匹配过去 （由于是按照token的长度降序排序，因此满足最大长度匹配）
      - e.g., 已知有sorted_token = ['la', 'ba', 'b','a'], 对于"banana"
        - 先取出'la'从'balabalab'中找到match的部分：显然有两次匹配，于是记录下每次匹配的start and end index，即[[2, 3], [6, 7]]。
        - 接着将'balabala'根据匹配部分进行分割
        - 将第一个匹配部分之前的部分也就是第一个'ba' slice出来，进行tokenize，然后+’la'
        - 此时剩下'balab'，再次将与'la'匹配部分之前的部分提取出来进行tokenize，然后加上+'la'
        - 此时剩下'b'，再对'b‘进行tokenize'

###### Cons

- 复杂度高，需要经过大量的merge
- rarer words 容易被分成subwords
- test data中未出现在train data中的words会被分成单独的charater/letter，因此这样永远不会产生新的未知词汇

### **Word normalisation**

transform words into canonical forms, 

#### Goal

‣ 缩小词汇表 Reduce vocabulary 

‣ 映射到相同类型 Maps words into the same type

#### Common Approaches

- 小写化 Lower casing (Australia → australia) 
- 移除形态 Removing morphology (cooking → cook) 
- 拼写纠错 Correcting spelling (definately → definitely) 
- 缩写展开 Expanding abbreviations (U.S.A → USA) 

#### **屈折形态（Inflectional Morphology）** vs. **派生形态（Derivational Morphology）**

- 屈折形态（Inflectional Morphology）
  - 同一个词的不同形态
  - 英语中，名词（nouns）、动词（verbs）和形容词（adjectives）会发生形变
    - 名词：名词的 *数量*（-s）
    - 动词：主语的 *数量*（-s）、动作的 *方面*（-ing）和 *时态*（-ed）
    - 形容词：*比较级*（-er）和 *最高级*（-est）

- 派生形态（Derivational Morphology）
  - 会创造出新的词和词性
  - 通过添加前缀或后缀，可能会改变词的意思或者词性

#### **词形还原（Lemmatisation**）

移除一个词的任何屈折形态（inflection)，将其还原为无屈折变化的形态（uninflected form/ lemma)

- e.g., speaking -> speak
- e.g., apples -> apple

#### **词干提取（Stemming）**

- 移除一个词的任何后缀，只留下主干(stem)。（注意这边一般只会移除后缀，目前对前缀进行处理的function）
- stem 通常不是一个词汇项（lexical item）
- 词汇稀疏性（lexical sparsity）比 lemmatisation 还要低，因为经过stemming后，vocabulary会进一步缩小
- 可解释性差
- 常用于**信息检索（Information Retrieval）**
- e.g., **波特词干器（Porter Stemmer）**
  - Step1: 去除 *屈折后缀* strip inflectional suffixes
  - Step2: 去除 *派生后缀* remove derivational suffixes

#### 拼写纠错 (Fixing Spelling Errors)

- 通过字符串距离 (String distance, e.g.. Levenshtein)
  - 我们可以通过计算当前字符串与vocabulary中各个字符串之间的距离，将其替换为距离最近的那个字符串
- 对错误的类型进行建模 (Modelling of error types, e.g., phonetic, typing)
- 使用基于character level的n-gram Language model
  - 这是一种数据驱动的方法

#### Lemmatisation vs. Stemming

我们会根据不同的任务选择其中的一种。Stemming 常用于信息检索（Information Retrieval，IR），但是对于大多数 NLP 任务，我们通常选择 lemmatisation。原因是在进行 stemming 后，我们会丢失大量和 tokens 相关的信息（lemmatisation 也会丢失一些信息，但是总体来说要少得多）。而对于大部分 NLP 任务，以丢失更多的单词含义为代价来换取更低的词汇稀疏性是不值得的，因为这将破坏句子原本的意思。

### Stopword removal

- delete unwanted words
  - e.g., ['I', 'am', 'a', 'student.'] -> ['I', 'student']

- 通常用于使用词袋**(bag-of-word/BOW)**表示的任务中
  - e.g., 搜索引擎。there is no order among all of words, BOW stores the counts of words excluding stopword.
- 不适用于sequence模型
  -  e.g., Language model