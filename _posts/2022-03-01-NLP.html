<!DOCTYPE html>
<html>
<head>
<title>2022-03-01-NLP.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">语言模型</h1>
<h2 id="%E5%AE%9A%E4%B9%89">定义</h2>
<p>将一段自然语言文本看作一串离散的时间序列，假设对于一段长度为 T 的文本中的词以此为$w_1, w_2, \dots, w_T$ ，则对于该离散的时间序列，$w_t$表示在 time step = t 的时候的输出或 label。则有定义，对于给定一个长度为 T 的词序列$w_1, w_2, \dots, w_T$ ，语言模型（即该序列的概率）为</p>
<p>$$P(w_1, w_2, \dots, w_T)$$</p>
<h2 id="n-gram">N-gram</h2>
<p>当取 T=4 的时候，我们可以根据联合概率的计算方法得到</p>
<p>$$P(w_1, w_2, w_3, w_4)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3)$$</p>
<p>更一般的，对于序列$w_1, w_2, \dots, w_T$ ，其概率计算可以表示为</p>
<p>$$P(w_1, w_2,\dots, w_T)=\prod_{t=1}^TP(w_t|w_1, w_2, \dots, w_{t-1})$$</p>
<h3 id="%E5%BC%95%E5%85%A5%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE">引入马尔可夫链</h3>
<p>由于当序列长度非常长的时候，计算和存储的复杂度会指数级别增长，且数据会十分稀疏从而导致概率失真。通过引入马尔可夫链来简化计算。</p>
<p>假设：一个词的出现只与其前面 n 个词相关，也就是<strong>n 阶马尔可夫链</strong></p>
<p>$$P(w_1, w_2,\dots, w_T) \approx\prod_{t=1}^TP(w_t|w_{t-n+1}, \dots, w_{t-1})$$</p>
<h3 id="n-%E7%9A%84%E5%8F%96%E5%80%BC">n 的取值</h3>
<ul>
<li>当 n 太小的时候，模型无法准确的学习到真正相关词之间的关系，欠拟合，high bias</li>
<li>当 n 太大的时候，退化成原始计算方法，导致计算存储复杂度很大，数据很稀疏概率失真。high variance</li>
<li>根据 n 的取值不同，主要有 unigram(n=1)，bigram(n=2)，trigram(n=3)
<ul>
<li>unigram：假设每个单词都是独立的</li>
<li>bigram：只依赖于前面一个词</li>
</ul>
</li>
</ul>
<h3 id="%E5%B9%B3%E6%BB%91%E6%B3%95">平滑法</h3>
<h4 id="lidstone-smoothing-alphak">Lidstone smoothing $\alpha=k$</h4>
<p>某些词可能未出现在 vocab 中或频次很小很小，则对于分母(denominator)可能出现零概率问题，因此与 Naive Bayers 中类似可以采用平滑技术</p>
<p>$$P_{smooth}(w_i|w_{i-n+1}, \dots, w_{i-1})=\frac{C(w_{i-n+1},\dots,w_i)+\alpha}{C(w_{i-n+1},\dots,w_{i-1})+\alpha|V|}$$</p>
<ul>
<li>
<p>我们将分子称为<strong>有效计数（effective counts）</strong></p>
</li>
<li>
<p>当$\alpha=1$的时候是我们常见的<strong>拉普拉斯法(Laplace Smoothing)</strong></p>
</li>
<li>
<p>当$\alpha=0.5$的时候称为**Jeffreys-Perks law **</p>
</li>
<li>
<p>我们可以发现此时对于每个可能的$w$，有效计数与真实计数之间的差值是不固定的</p>
</li>
</ul>
<h4 id="absolute-discounting-smoothing">Absolute Discounting smoothing</h4>
<ul>
<li>
<p>从每个<strong>观测</strong>到的 n-gram 计数中“借”一个**固定的概率质量（a fixed probability mass）**d</p>
</li>
<li>
<p>将其<strong>重新分配（redistribute）<strong>到</strong>未观测到</strong>的 n-gram</p>
</li>
<li>
<p>e.g.:</p>
<ul>
<li>context = 'alleged'</li>
<li>5 observed bi-grams: i.e., C('alleged'|char $\in${'impropriety', 'offense', 'damage', 'deficiencies', 'outbreak'}) &gt; 0</li>
<li>2 unobserved bi-grams: i.e., C('alleged' | 'infirmity') = 0 and C('alleged' | 'cephalopods') = 0</li>
</ul>
<img src="/Users/yuhandan/Library/Application Support/typora-user-images/image-20220307154950854.png" alt="image-20220307154950854" style="zoom:50%;" />
<ul>
<li>对于</li>
</ul>
</li>
</ul>
<p>$$
P_{ADS}(w_i|w_{i-1})=\left{
\begin{array}{l}
\frac{C(w_{i-1},w_i)-d}{C(w_{i-1})}, &amp; \text{if  } C(w_{i-1}, w_i) &gt; 0\</p>
<pre><code>       \frac{d\times C(observed\; n-grams)}{C(unobserved\; n-grams)}=\frac{\alpha(w_{i-1})}{|\{w_j:C(w_{i-1},w_j)=0\}|}, &amp; \text{otherwise}.
         \end{array}
</code></pre>
<p>\right.
$$</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_ads_prob</span><span class="hljs-params">(prev_word, word, unigram_counts, bigram_counts, d)</span>:</span>
    <span class="hljs-string">"""Compute a single word's smoothing probability using absolute discounting
    Args:
        prev_word: str, the previous word
        word: str, current word
        unigram_counts: the Counter of the whole sentences using unigram model
        bigram_counts: the Counter of the whole sentences using bigram model
        d: float, the discounting rate
    """</span>
    <span class="hljs-keyword">if</span> bigram_counts[prev_word][word] &gt; <span class="hljs-number">0</span>: <span class="hljs-comment"># observed</span>
        prob = (bigram_counts[prev_word][word] - d) / unigram_counts[prev_word]
    <span class="hljs-keyword">else</span>: <span class="hljs-comment"># unobserved</span>
        observed_prob = len([val <span class="hljs-keyword">for</span> val <span class="hljs-keyword">in</span> bigram_counts[prev_word].values() <span class="hljs-keyword">if</span> val &gt; <span class="hljs-number">0</span>])
        unobserved_prob = len([val <span class="hljs-keyword">for</span> val <span class="hljs-keyword">in</span> bigram_counts[prev_word].values() <span class="hljs-keyword">if</span> val == <span class="hljs-number">0</span>])
        prob = d * observed_prob / unobserved_prob
    <span class="hljs-keyword">return</span> prob
</div></code></pre>
<pre class="hljs"><code><div>unigram_counts = {<span class="hljs-string">'alleged'</span>: <span class="hljs-number">20</span>}
bigram_counts = Counter({<span class="hljs-string">'alleged'</span>: Counter({<span class="hljs-string">'impropriety'</span>: <span class="hljs-number">8</span>, <span class="hljs-string">'offense'</span>: <span class="hljs-number">5</span>, <span class="hljs-string">'damage'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'deficiencies'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'outbreak'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'infirmity'</span>: <span class="hljs-number">0</span>,<span class="hljs-string">'cephalopods'</span>: <span class="hljs-number">0</span>})})

</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> [<span class="hljs-string">'impropriety'</span>, <span class="hljs-string">'offense'</span>, <span class="hljs-string">'damage'</span>, <span class="hljs-string">'deficiencies'</span>, <span class="hljs-string">'outbreak'</span>, <span class="hljs-string">'infirmity'</span>, <span class="hljs-string">'cephalopods'</span>]:
    print(char + <span class="hljs-string">':'</span> + str(get_ads_prob(<span class="hljs-string">'alleged'</span>,char, unigram_counts, bigram_counts, <span class="hljs-number">0.1</span>)))
</div></code></pre>
<pre class="hljs"><code><div>impropriety:0.395
offense:0.24500000000000002
damage:0.195
deficiencies:0.095
outbreak:0.045
infirmity:0.25
cephalopods:0.25
</div></code></pre>
<h4 id="backoff-smoothing">Backoff Smoothing</h4>
<h5 id="katz-backoff">Katz backoff</h5>
<p>我们可以通过使用 Absolute Discounting Smoothing 得到的结果看出，对于 unobserved word，它们的概率是一样的。然而真实情况中并非如此，因此我们引入 Katz backoff，对每个不同的 unobserved word 的 discounting 加入权重比。</p>
<p>我们可以考虑使用降低模型阶数来避免稀疏数据的问题。</p>
<p>对于 bigram 模型，Katz backoff 的公式可以表示为如下形式：</p>
<p>$$
P_{Katz}(w_i|w_{i-1})=\left{
\begin{array}{l}
\frac{C(w_{i-1}, w_i)-d}{C(w_{i-1})}, &amp; \text{if  } C(w_{i-1}, w_i) &gt; 0\</p>
<pre><code>        \alpha(w_{i-1})\frac{P(w_i)}{\sum_{w_j:C(w_{i-1}, w_j)=0}P(w_j)}, &amp; \text{otherwise}.
         \end{array}
</code></pre>
<p>\right.
$$</p>
<ul>
<li>
<p>其中对于 observed bi-gram（也就是$C(w_{i-1}, w_i)&gt;0$）：Katz backoff 的平滑概率与 Absolute Smoothing 概率相同</p>
</li>
<li>
<p>而对于 unobserved bi-gram（也就是$C(w_{i-1}, w_i)=0$）：Katz backoff 通过退回一步（即回到 unigram)，考虑当前单词$w_i$在 unigram 中的概率$P(w_i)$，分母表示为在上下文(corpus)中与单词$w_{i-1}$没有共同出现(co-occurrence)的单词的 unigram 概率之和</p>
</li>
<li>
<p>e.g., 仍用以上例子，我们假设已知'infirmity'在文中出现的次数比'cephalopods'多，也就是$P('infirmity')&gt;P('cephalopods')$，而剩余两个部分两者保持一致【其中$\alpha(w_{i-1})=\alpha('alleged')=5\times 0.1=0.5$， 分母都是$P('infirmity')+P('cephalopods')$】</p>
</li>
<li>
<p><strong>Katz Backoff 存在的问题</strong></p>
<ul>
<li>
<p>由于我们只是单独考虑某个单词在上下文中出现的概率，而忽略了某些单词是否经常出现与不同的其他单词组合或是仅和少有不同其他单词组合</p>
</li>
<li>
<p>e.g., 在语料库(corpus)中知道$C('Francisco') &gt; C('glasses')$，又假设$C('reading'|'Francisco')=0,C('reading'|'glasses')=0$.</p>
<ul>
<li>此时当我们运用 Katz backoff 来对'reading'后的单词进行填词的时候，会偏向于选择填写'Francisco'</li>
<li>也就是说<strong>Katz Backoff Smooth 偏向于选择在低阶模型中拥有更高出现概率的单词</strong></li>
<li><strong>Katz Backoff Smooth is prone to predict the word with higher probability in lower order gram model.</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">katz_backoff_prob</span><span class="hljs-params">(prev_word, candidate_words, unigram_counts, bigram_counts, d)</span>:</span>
    <span class="hljs-string">"""Compute the smoothing probability using Katz backoff smoothing
    Args:
        prev_word: str, the previous word
        candidate_words: list, store all possible candidate words
        unigram_counts: the Counter of the whole sentences using unigram model
        bigram_counts: the Counter of the whole sentences using bigram model
        d: float, the discounting rate
    """</span>
    unobserved_words = []
    words_prob = {}
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> candidate_words:
        <span class="hljs-keyword">if</span> bigram_counts[prev_word][word] &gt; <span class="hljs-number">0</span>: <span class="hljs-comment"># observed</span>
            prob = (bigram_counts[prev_word][word] - d) / unigram_counts[prev_word]
            words_prob[word] = prob
        <span class="hljs-keyword">else</span>: <span class="hljs-comment"># store unobserved words</span>
            unobserved_words.append(word)
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> unobserved_words:
        observed_count = len(candidate_words) - len(unobserved_words)
        weighted = unigram_counts[word] / sum([unigram_counts[char] <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> unobserved_words])
        prob = d * observed_count * weighted
        words_prob[word] = prob
    <span class="hljs-keyword">return</span> words_prob
</div></code></pre>
<h5 id="kneser-ney-%E5%B9%B3%E6%BB%91">Kneser-Ney 平滑</h5>
<p>用于解决 Katz Backoff 的问题，引入<strong>多功能性（versatility）<strong>又称为</strong>延续概率（continuation probability）</strong></p>
<ul>
<li>High versatility: 与很多单独单词一起成对出现（co-occur with <strong>a lot of</strong> unique words）
<ul>
<li>e.g., glasses. (black glasses, man's glasses, ...)</li>
</ul>
</li>
<li>Low versatility：与仅有的单独单词一起出现（co-occur with <strong>few</strong> unique words）
<ul>
<li>e.g., Francisco. (San Francisco)</li>
</ul>
</li>
</ul>
<p>对于 bi-gram, Kneser-Ney Smoothing 的公式可以表示成如下：</p>
<p>$$
P_{KN}(w_i|w_{i-1})=\left{
\begin{array}{<strong>lr</strong>}
\frac{C(w_{i-1}, w_i)-d}{C(w_{i-1})}, &amp; \text{if  } C(w_{i-1}, w_i) &gt; 0\</p>
<pre><code>        \alpha(w_{i-1})P_{cont}(w_i), &amp; \text{otherwise}.
         \end{array}
</code></pre>
<p>\right.
$$</p>
<ul>
<li>
<p>对于 observed bigram, 仍然没有区别</p>
</li>
<li>
<p>对于 unobserved bigram，保留从 observed bigram 借来并进行 discount 的部分，其中$P_{cont}(w_i)$ 表示当前单词$w_i$的延续概率(continuation probability)</p>
<ul>
<li>$$P_{cont}(w_i)=\frac{|{w_{i-1}:C(w_{i-1},w_i)&gt;0}|}{\sum_{w'<em>i\in W_i}|{w'</em>{i-1}:C(w'_{i-1},w'_i)&gt;0}|}$$， 其中$W_i$表示所有候选可能的单词的集合(candidaters)（e.g., glasses &amp; Francisco 或对于上面 alleged 的例子是指那 7 个 words）</li>
<li>分子表示与当前单词$w_i$在 corpus 中一起出现过的 unique 单词数量（即 previous word 的数量），即$P_{continuation}$</li>
<li>分母表示所有与可能单词（candidaters) 在 corpus 中一起出现过的 unique 单词数量之和（也就是 candidaters 的所有 bigram 类型数量之和）</li>
<li>注意分子分母都是集合的大小，也就是对于重复出现的 co-occur 单词只能算一次</li>
</ul>
</li>
<li>
<p>此时对于 Low versatility word，分子会很小，就比如 Francisco，它的分子很有可能就是 1</p>
</li>
</ul>
<h2 id="rnn-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">RNN 语言模型</h2>
<p>$$x_m\triangleq \phi_{w_m}$$ <strong>embedding layer/lookup layer</strong></p>
<p>$$h_m=RNN(x_m, h_{m-1}) \triangleq g(\Theta h_{m-1}+x_m)$$ <strong>Elman unit</strong></p>
<p>$$p(w_{m+1}|w_1, w_2, \dots, w_m)=\frac{\exp(O_{w_{m+1}}.h_m)}{\sum_{w'\in }\exp(O_{w'}.h_m)}$$</p>
<ul>
<li>$\phi$是一个 word embeddings 矩阵，$x_m$ 表示词$w_m$对应的 embedding matrix</li>
<li>$\Theta$ 是 recurrence matrix</li>
<li>$g$是非线性激活函数(tanh)用于压缩数据到[-1,1] ,<strong>squashing function</strong></li>
<li>$O_{w_m}$表示词$w_m$对应的输出值</li>
</ul>
<h2 id="%E8%AF%84%E4%BC%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-evaluate-language-models">评估语言模型 evaluate language models</h2>
<p>focus on <strong>Instrinsic evaluation</strong></p>
<h3 id="held-out-likelihood">Held-out likelihood</h3>
<p>计算未参加训练的数据集<strong>held-out data</strong>的 log-likelihood。</p>
<p>$$l(w)=\sum_{m=1}^M\log p(w_m|w_{m-1},\dots, w_1)$$</p>
<p>$w$ 表示 sentences, documents</p>
<ul>
<li>prefer to <strong>higher</strong> likelihoods</li>
</ul>
<h3 id="perplexity">Perplexity</h3>
<p>$$Perplex(w)=n^{-\frac{l(w)}{M}}$$</p>
<p>$w$ 表示 sentences, documents，<strong>M</strong> refers to the <strong>total number of tokens in the held-out data</strong></p>
<ul>
<li>
<p>Lower perplexities -&gt; higher likelihoods, thus expect to <strong>lower</strong> perplexities</p>
</li>
<li>
<p>In <strong>unigram model</strong>, assume a uniform, where $p(w_i)=\frac{1}{V}$, then</p>
<ul>
<li>
<p>$$l(w) = \sum_{m=1}^M\log \frac{1}{V}=-M\log V$$</p>
</li>
<li>
<p>$Perplex(w)=n^{-\frac{1}{M}\times (-M\log_n V)}=n^{\log_n V}$$</p>
</li>
</ul>
<p>当 log 以 n 为底，则<strong>Perplex(w) = V</strong>，其中 V 表示 the <strong>total number of tokens in the w</strong></p>
</li>
<li>
<p>理论取值范围：[1, $\infin$]</p>
<ul>
<li>according to the range of $p(w_m|w_{m-1},\dots, w_1)$, that is from 0 to 1</li>
</ul>
</li>
<li>
<p>practical 取值范围：[1, V]</p>
</li>
<li>
<p>通常我们取 n=e</p>
</li>
</ul>
<h1 id="text-classification">Text Classification</h1>
<h2 id="tasks">Tasks</h2>
<ul>
<li>Topic classification</li>
<li>Sentiment analysis</li>
<li>Native-language identification</li>
<li>Natural language inference</li>
<li>Automatic fact-checking</li>
<li>Paraphrase</li>
</ul>
<h2 id="%E6%B5%81%E7%A8%8B">流程</h2>
<ul>
<li>Identify a task of interest</li>
<li>Collect an appropriate corpus</li>
<li>Carry out annotation</li>
<li>Select features</li>
<li>Choose a machine learning algorithm</li>
<li>Train mdel and tune hyperparameters using held-out development data</li>
<li>Repeat earlier steps as needed</li>
<li>Train final model</li>
<li>Evaluate model on held-out test data</li>
</ul>
<h2 id="algorithms">Algorithms</h2>
<h3 id="naive-bayes">Naive Bayes</h3>
<ul>
<li>Finds the class with the highest likelihood under Bayes lay</li>
</ul>
<h4 id="pros">pros</h4>
<ul>
<li>Fast to train and classify</li>
<li>robust, low-variance → good for low data situations</li>
<li>optimal classifier if independence assumption is correct</li>
<li>extremely simple to implement</li>
</ul>
<h4 id="cons">cons</h4>
<ul>
<li>Independence assumption rarely holds
<ul>
<li>e.g., the scenoria in NLP, where considering prefix.
<ul>
<li>As usual, $$P(word='unfit',prefix='un-'|y)=P(prefix='un-',word='unfit'|y)P(word='unfit'|y)=1\times P(word='unfit'|y)$$</li>
<li>if use NB, $$P(word='unfit',prefix='un-'|y)=P(word='unfit'|y)P(prefix='un-'|y)$$</li>
</ul>
</li>
</ul>
</li>
<li>low accuracy compared to similar methods in most situations</li>
<li>smoothing required for unseen class/feature combinations</li>
</ul>
<h3 id="logistic-regression">Logistic Regression</h3>
<h4 id="cons">Cons</h4>
<p>‣ 训练速度慢 Slow to train;</p>
<p>‣ 需要标准化或归一化处理 Feature scaling needed</p>
<p>‣ 需要大量的数据 Requires a lot of data to work well in practice</p>
<p>‣ 容易导致 overfitting Choosing regularisation strategy is important since overfitting is a big problem</p>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<ul>
<li>
<p>Finds hyperplane which separates the training data with maximum margin</p>
</li>
<li>
<p>be popular for NLP, mainly because it works well with huge feature sets and NLP problems often involve large feature sets</p>
</li>
</ul>
<h4 id="pros">Pros</h4>
<p>• 快且准 Fast and accurate linear classifier</p>
<p>• 能够支持非线性 kernel Can do non-linearity with kernel trick</p>
<p>• 在大特征集中表现良好 Works well with huge feature sets</p>
<h4 id="cons">Cons</h4>
<p>• 多分类的效果不佳 Multiclass classification awkward</p>
<p>• 需要特征标准化 Feature scaling needed</p>
<p>• 难处理类不平衡问题 Deals poorly with class imbalances</p>
<p>• 可解释性差 Interpretability</p>
<h3 id="k-nearest-neighbour"><em>K</em>-Nearest Neighbour</h3>
<ul>
<li>Classify based on majority class of <em>k</em>-nearest training examples in feature space</li>
</ul>
<h4 id="pros">Pros</h4>
<p>• 简单且有效 Simple but surprisingly effective</p>
<p>• <strong>不需要训练</strong> No training required</p>
<p>• 适用于多分类 Inherently multiclass</p>
<p>• 在无穷大的数据集中能够找到 optimal Optimal classifier with infinite data</p>
<h4 id="cons">Cons</h4>
<p>• 难选择 k Have to select <em>k</em></p>
<p>• 难处理类不平衡问题 Issues with imbalanced classes</p>
<p>• 速度慢 Often slow (for finding the neighbours)</p>
<p>• 特征选取难 Features must be selected carefully</p>
<h3 id="decision-tree">Decision tree</h3>
<h4 id="pros">Pros</h4>
<p>• 训练测试速度快 Fast to build and test</p>
<p>• 不需要特征归一化 Feature scaling irrelevant</p>
<p>• 在小特征集中效果好 Good for small feature sets</p>
<p>• 能处理非线性可分的情况 Handles non-linearly-separable problems</p>
<h4 id="cons">Cons</h4>
<p>• 可解释性差 In practice, not that interpretable</p>
<p>• 子树集合高度冗余 Highly redundant sub-trees</p>
<p>• 在大特征集中效果不佳 Not competitive for large feature sets</p>
<h3 id="random-forests">Random Forests</h3>
<h4 id="pros">Pros</h4>
<p>• Usually more accurate and more robust than decision trees</p>
<p>• Great classifier for medium feature sets</p>
<p>• Training easily parallelised</p>
<h4 id="cons">Cons:</h4>
<p>• Interpretability</p>
<p>• Slow with large feature sets</p>
<h3 id="neural-networks">Neural Networks</h3>
<h4 id="pros">Pros</h4>
<p>• Extremely powerful, dominant method in NLP and vision</p>
<p>• Little feature engineering</p>
<h4 id="cons">Cons</h4>
<p>• Not an off-the-shelf classifier</p>
<p>• Many hyper-parameters, difficult to optimise</p>
<p>• Slow to train</p>
<p>• Prone to overfitting</p>
<h2 id="%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98-hyper-parameter-tuning">参数调优 Hyper-parameter Tuning</h2>
<h3 id="dataset-for-tuning">Dataset for tuning</h3>
<ul>
<li>development set</li>
<li>not the training set or the test set</li>
<li>k-fold cross-validation</li>
</ul>
<h3 id="grid-search">Grid Search</h3>
<p>for multiple hyper-parameters</p>
<h1 id="part-of-speech-tagging">Part of Speech Tagging</h1>
<h2 id="%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%80%A7%E6%A0%87%E8%AE%B0%E9%9B%86">常用词性标记集</h2>
<p>Penn Treebank corpus,</p>
<p>Brown</p>
<h2 id="%E6%9E%84%E5%BB%BA%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E6%A8%A1%E5%9E%8B">构建词性标注模型</h2>
<ul>
<li>特征构建
<ul>
<li>Word itself</li>
<li>Lowercased word</li>
<li>prefixes</li>
<li>suffixes</li>
<li>capitalization</li>
<li>word shapes</li>
<li></li>
</ul>
</li>
</ul>

</body>
</html>
